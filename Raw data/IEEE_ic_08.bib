@INPROCEEDINGS{5558636, 
author={S. Andalam and P. Roop and A. Girault}, 
booktitle={Eighth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2010)}, 
title={Predictable multithreading of embedded applications using PRET-C}, 
year={2010}, 
pages={159-168}, 
abstract={We propose a new language called Precision Timed C (PRET-C), for predictable and lightweight multi-threading in C. PRET-C supports synchronous concurrency, preemption, and a high-level construct for logical time. In contrast to existing synchronous languages, PRET-C offers C-based shared memory communications between concurrent threads that is guaranteed to be thread safe. Due to the proposed synchronous semantics, the mapping of logical time to physical time can be achieved much more easily than with plain C, thanks to a Worst Case Reaction Time (WCRT) analyzer (not presented here). Associated to the PRET-C programming language, we present a dedicated target architecture, called ARPRET, which combines a hardware accelerator associated to an existing softcore processor. This allows us to improve the throughput while preserving the predictability. With extensive benchmarking, we then demonstrate that ARPRET not only achieves completely predictable execution of PRET-C programs, but also improves the throughput when compared to the pure software execution of PRET-C. The PRET-C software approach is also significantly more efficient in comparison to two other light-weight concurrent C variants (namely SC and Protothreads), as well as the well-known Esterel synchronous programming language.}, 
keywords={C language;embedded systems;multi-threading;shared memory systems;ARPRET;Esterel synchronous programming language;PRET-C programming language;embedded applications;precision timed C;predictable multithreading;shared memory communications;worst case reaction time analyzer;Assembly;Concurrent computing;Hardware;Instruction sets;Semantics;Timing}, 
doi={10.1109/MEMCOD.2010.5558636}, 
month={July},}
@INPROCEEDINGS{4400379, 
author={A. Krystosik}, 
booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
title={Model Checking in Concurrent Programming Teaching}, 
year={2007}, 
pages={2390-2396}, 
abstract={The paper presents an application of model checking in teaching a concurrent programming. The success of this approach is based on features of DT-CSM automata and EMLAN modeling language. EMLAN is a C-like, high level language for modeling and model checking of embedded systems. The language is equipped with a lot of synchronization mechanisms (semaphores, mutexes, monitors). This allows easy modeling and verification of concurrent, cooperating tasks, which is very useful for educational purposes. As an example, a typical student exercise: a producer-consumer is given.}, 
keywords={computer science education;distributed programming;embedded systems;finite state machines;program verification;specification languages;synchronisation;DT-CSM automata;EMLAN C-like high level language;EMLAN modeling language;concurrent programming teaching;embedded system model checking;synchronization mechanisms;Automata;Computer science;Concurrent computing;Education;Embedded system;Error correction;Information technology;Logic;Paper technology;Programming profession;Modeling;Software verification and validation;Teaching}, 
doi={10.1109/EURCON.2007.4400379}, 
month={Sept},}
@INPROCEEDINGS{4394196, 
author={M. Koutsoubelias and S. Lalis}, 
booktitle={2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications}, 
title={Design and Implementation of an Extensible Architecture for the Efficient Remote Access of Simple RFID-Readers}, 
year={2007}, 
pages={1-5}, 
abstract={This paper describes a software architecture for the remote monitoring of warehouses equipped with simple RFID reader devices. Its main design objective is to enable a flexible and efficient integration of resource constrained readers that may be implemented as low-cost embedded systems, while allowing higher-level middleware components to access them in a transparent way. The proposed design has been implemented in a Linux-based environment and is currently being tested in conjunction with early prototypes of simple RFID readers that are accessed over low-bandwidth.}, 
keywords={embedded systems;radiofrequency identification;software architecture;telecontrol;warehouse automation;efficient remote access;extensible architecture;higher-level middleware components;low-cost embedded systems;simple RFID-readers;software architecture;Computer architecture;Hardware;Information filtering;Information filters;Middleware;Mobile communication;Protocols;Radiofrequency identification;Remote monitoring;Testing}, 
doi={10.1109/PIMRC.2007.4394196}, 
ISSN={2166-9570}, 
month={Sept},}
@INPROCEEDINGS{6416747, 
author={R. R. Sharma and Y. Rajasekhar and R. Sass}, 
booktitle={2012 International Conference on Reconfigurable Computing and FPGAs}, 
title={Exploring hardware work queue support for lightweight threads in MPSoCs}, 
year={2012}, 
pages={1-6}, 
abstract={Fine-grain thread parallelism using task based programming models are a new trend in achieving massively parallel computations. Often, software pre-fetching and queuing mechanisms for managing these dynamic environments are inadequate, failing to keep the processor cores busy with computation. At the same time, the CPU-memory performance gap is getting worse and this puts a strain on memory subsystem to keep cores in a busy state. We describe a hardware based pre-fetching and queuing mechanism aimed at assisting the over-subscription of very lightweight threads per core. Experiments with a soft processor and a reconfigurable accelerator core are reported. The hardware demonstrates the ability to block on out-of-order memory transactions and alleviates the software bottleneck.}, 
keywords={multiprocessing systems;parallel programming;queueing theory;storage management;system-on-chip;CPU-memory performance gap;MPSoC;fine-grain thread parallelism;hardware based pre-fetching;hardware work queue support;lightweight threads;memory subsystem;multiprocessing system on chip;out-of-order memory transactions;parallel computations;queuing mechanisms;reconfigurable accelerator core;soft processor;software pre-fetching;task based programming models;Hardware;Instruction sets;Memory management;Random access memory;Registers;Switches;Throughput}, 
doi={10.1109/ReConFig.2012.6416747}, 
ISSN={2325-6532}, 
month={Dec},}
@INPROCEEDINGS{6544384, 
author={A. Asaduzzaman and F. N. Sibai and S. Aramco and H. El-Sayed}, 
booktitle={2013 9th International Conference on Innovations in Information Technology (IIT)}, 
title={Performance and power comparisons of MPI Vs Pthread implementations on multicore systems}, 
year={2013}, 
pages={1-6}, 
abstract={The advancement of multicore systems demands applications with more threads. In order to facilitate this demand, parallel programming models such as message passing interface (MPI) are developed. By using such models, the execution time and the power consumption can be reduced significantly. However, the performance of MPI programming depends on the total number of threads and the number of processing cores in the system. In this work, we experimentally study the impact of Open MPI and POSIX Thread (Pthread) implementations on performance and power consumption of multicore systems. Data dependent (like heat conduction on 2D surface) and data independent (like matrix multiplication) applications are used with high performance hardware in the experiments. Simulation results suggest that both implementations of more threads running in a system with more cores have potential to reduce the execution time with negligible or little increase in total power consumption. It is observed that the performance of MPI implementation varies (due to the dynamic communication overhead among the processing cores).}, 
keywords={application program interfaces;matrix algebra;message passing;multiprocessing systems;parallel programming;power aware computing;Open MPI;POSIX thread;Pthread implementations;data dependent;dynamic communication;matrix multiplication;message passing interface;multicore systems;parallel programming models;performance comparisons;performance hardware;power comparisons;power consumption;processing cores;pthread implementations;Instruction sets;Message systems;Multicore processing;Parallel programming;Power demand;Supercomputers;Workstations;Open MPI;Pthread;data dependency;message passing interface;multicore architecture}, 
doi={10.1109/Innovations.2013.6544384}, 
month={March},}
@INPROCEEDINGS{6910501, 
author={S. Savas and E. Gebrewahid and Z. Ul-Abdin and T. Nordstr√∂m and M. Yang}, 
booktitle={2014 IEEE 20th International Conference on Embedded and Real-Time Computing Systems and Applications}, 
title={An evaluation of code generation of dataflow languages on manycore architectures}, 
year={2014}, 
pages={1-9}, 
abstract={Today computer architectures are shifting from single core to manycores due to several reasons such as performance demands, power and heat limitations. However, shifting to manycores results in additional complexities, especially with regard to efficient development of applications. Hence there is a need to raise the abstraction level of development techniques for the manycores while exposing the inherent parallelism in the applications. One promising class of programming languages is dataflow languages and in this paper we evaluate and optimize the code generation for one such language, CAL. We have also developed a communication library to support the intercore communication. The code generation can target multiple architectures, but the results presented in this paper is focused on Adapteva's many core architecture Epiphany. We use the two-dimensional inverse discrete cosine transform (2D-IDCT) as our benchmark and compare our code generation from CAL with a hand-written implementation developed in C. Several optimizations in the code generation as well as in the communication library are described, and we have observed that the most critical optimization is reducing the number of external memory accesses. Combining all optimizations we have been able to reduce the difference in execution time between auto-generated and handwritten implementations from a factor of 4.3√ó down to a factor of only 1.3√ó.}, 
keywords={computer architecture;data flow computing;multiprocessing systems;program compilers;2D inverse discrete cosine transform;2D-IDCT;code generation;communication library;computer architectures;dataflow languages;external memory accesses;heat limitations;intercore communication;manycore architectures;programming languages;Generators;Libraries;Multicore processing;Ports (Computers);Program processors;Programming;2D-IDCT;Actor Machine;Dataflow Languages;Epiphany;Manycore;code generation;evaluation}, 
doi={10.1109/RTCSA.2014.6910501}, 
ISSN={2325-1271}, 
month={Aug},}
@INPROCEEDINGS{4542057, 
author={A. Anane and E. M. Aboulhamid and J. Vachon and Y. Savaria}, 
booktitle={2008 IEEE International Symposium on Circuits and Systems}, 
title={Modeling and simulation of complex heterogeneous systems}, 
year={2008}, 
pages={2873-2876}, 
abstract={Given the increasing heterogeneity and complexity of systems being developed, untimed modeling at a system level becomes more and more important for design space exploration and verification, due to its conciseness and speed. After showing inadequacies of SystemC, which is the predominant modeling environment in this area, we propose a paradigm shift from immediate notifications and coroutines in SystemC to Atomic Actions and true parallelism in an extension of Esys.NET. We exploit the introspection and attribute programming to extend the capabilities of the environment and to build the basis for heterogeneous cosimulation. This paper aims to show the main advantages of this paradigm shift, such as (1) the improvement of simulation time by exploiting the capabilities of multicore simulation hosts, (2) the reduction of modeling hazards related to parallelism and resource sharing, and (3) a more efficient design space exploration.}, 
keywords={programming language semantics;software engineering;temporal logic;Esys.NET;SystemC;atomic actions;complex heterogeneous systems;heterogeneous cosimulation;multicore simulation;paradigm shift;resource sharing;space exploration;Collaborative work;Costs;Hazards;Kernel;Libraries;Manufacturing processes;Multicore processing;Productivity;Resource management;Space exploration}, 
doi={10.1109/ISCAS.2008.4542057}, 
ISSN={0271-4302}, 
month={May},}
@INPROCEEDINGS{5090916, 
author={E. Vecchie and J. P. Talpin and K. Schneider}, 
booktitle={2009 Design, Automation Test in Europe Conference Exhibition}, 
title={Separate compilation and execution of imperative synchronous modules}, 
year={2009}, 
pages={1580-1583}, 
abstract={The compilation of imperative synchronous languages like Esterel has been widely studied, the separate compilation of synchronous modules has not, and remains a challenge. We propose a new compilation method inspired by traditional sequential code generation techniques to produce coroutines whose hierarchical structure reflects the control flow of the original source code. A minimalistic runtime system executes separately compiled modules.}, 
keywords={data flow analysis;program compilers;Esterel language;imperative synchronous language;imperative synchronous modules;minimalistic runtime system;module compilation;module execution;sequential code generation;source code control flow;Automata;Computational modeling;Computer languages;Domain specific languages;Embedded system;Flow graphs;Intellectual property;Real time systems;Virtual prototyping;Yarn}, 
doi={10.1109/DATE.2009.5090916}, 
ISSN={1530-1591}, 
month={April},}
@INPROCEEDINGS{5456987, 
author={R. S. Khaligh and M. Radetzki}, 
booktitle={2010 Design, Automation Test in Europe Conference Exhibition (DATE 2010)}, 
title={Modeling constructs and kernel for parallel simulation of accuracy adaptive TLMs}, 
year={2010}, 
pages={1183-1188}, 
abstract={We present a set of modeling constructs accompanied by a high performance simulation kernel for accuracy adaptive transaction level models. In contrast to traditional, fixed accuracy TLMs, accuracy of adaptive TLMs can be changed during simulation to the level which is most suitable for a given use case and scenario. Ad-hoc development of adaptive models can result in complex models, and the implementation detail of adaptivity mechanisms can obscure the actual logic of a model. To simplify and enable systematic development of adaptive models, we have identified several mechanisms which are applicable to a wide variety of models. The proposed constructs relieve the modeler from low level implementation details of those mechanisms. We have developed an efficient, light-weight simulation kernel optimized for the proposed constructs, which enables parallel simulation of large models on widely available, low-cost multi-core simulation hosts. The modeling constructs and the kernel have been evaluated using industrial benchmark applications.}, 
keywords={operating system kernels;transaction processing;accuracy adaptive transaction level model;ad-hoc development;adaptive TLM;adaptive models systematic development;adaptivity mechanisms;high performance simulation kernel;light weight simulation kernel;low cost multicore simulation hosts;parallel simulation;Adaptive systems;Computational modeling;Concurrent computing;Context modeling;Discrete event simulation;Embedded system;Kernel;Logic;Natural languages;Performance loss}, 
doi={10.1109/DATE.2010.5456987}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{6961841, 
author={Y. Bai and K. Schneider and N. Bhardwaj and B. Katti and T. Shazadi}, 
booktitle={2014 Twelfth ACM/IEEE Conference on Formal Methods and Models for Codesign (MEMOCODE)}, 
title={From clock-driven to data-driven models}, 
year={2014}, 
pages={32-41}, 
abstract={Clock/time-driven models are powerful abstractions of real-time systems, as e.g., provided by the synchronous models of computation which lend themselves well for simulation and verification. At every clock cycle, new inputs are read, computations are performed in zero-time, and results are immediately/synchronously communicated between components. However, such zero-time idealizations are not realistic since computation and communication finally takes time in implementations. For implementations, data-driven execution models have the advantage to impose no timing constraints other than arrival of input data, and thus, these models are perfectly suited for distributed or other kinds of asynchronous implementations. For this reason, modern model-based design flows consider the desynchronization of synchronous models for system synthesis which is possible for the subclass of endochronous systems only. While definitions of endochrony were considered for years, it is shown in this paper how to efficiently verify endochrony by SAT solving. Our procedure consists of two steps: In the first step, we introduce buffers to the interface of a clock-driven component, so that its inputs can arrive at different points of time. After this step, clocks of signals are viewed as `instructions' telling the component which input values have to be consumed for the current reaction.We call such components clock-scheduled. In the second step, we remove the clocks from the interface of the clock-scheduled components, so that the component may now become nondeterministic. We prove in this paper that a synchronous component is endochronous, if and only if the clock signals can be safely removed in this step without destroying determinism. Based on this result, we present a decision procedure based on symbolic system representations to check whether components are endochronous. Preliminary experimental results show the effectiveness of our method.}, 
keywords={computability;symbol manipulation;SAT solving;clock cycle;clock-driven models;clock-scheduled components;data-driven execution models;endochronous systems;model-based design flows;symbolic system representations;synchronous models;system synthesis;time-driven models;zero-time idealizations;Circuit synthesis;Clocks;Computational modeling;Distributed databases;Integrated circuit modeling;Semantics;Synchronization}, 
doi={10.1109/MEMCOD.2014.6961841}, 
month={Oct},}
@INPROCEEDINGS{6690509, 
author={A. V. Brito and A. V. Negreiros and C. Roth and O. Sander and J. Becker}, 
booktitle={2013 IEEE/ACM 17th International Symposium on Distributed Simulation and Real Time Applications}, 
title={Development and Evaluation of Distributed Simulation of Embedded Systems Using Ptolemy and HLA}, 
year={2013}, 
pages={189-196}, 
abstract={Nowadays, embedded systems have a huge amount of computational power and consequently, high complexity. It is quite usual to find different applications being executed in embedded systems. Embedded system design demands for method and tools that allow the simulation and verification in an efficient and practical way. This paper proposes the development and evaluation of a solution for embedded modeling and simulation of heterogeneous Models of Computation (MoCs) in a distributed way by the integration of Ptolemy II and the High Level Architecture (HLA), a middleware for distributed discrete event simulation, in order to create an environment with high-performance execution of large-scale heterogeneous models. Experimental results demonstrate, that the use of a non distributed simulation for some situations can be infeasible, as well as the use of distributed simulation with few machines, like one, two or three computers. It was demonstrated that a speedup of factor 4 was acquired when a model with 4,000 thousands actors were distributed in 8 different machines.}, 
keywords={discrete event simulation;embedded systems;middleware;HLA;MoC;Ptolemy II;computational power;distributed discrete event simulation;embedded modeling and simulation;embedded systems;heterogeneous models of computation;high level architecture;high-performance execution;large-scale heterogeneous models;middleware;nondistributed simulation;Computational modeling;Computer architecture;Embedded systems;Ports (Computers);Sensors;Unified modeling language;Wireless sensor networks;Distributed Simulation;Embedded Systems;Heterogeneous Simulation}, 
doi={10.1109/DS-RT.2013.28}, 
ISSN={1550-6525}, 
month={Oct},}
@INPROCEEDINGS{5302678, 
author={W. h. Wang and Y. l. Cui and T. m. Chen}, 
booktitle={2009 5th International Conference on Wireless Communications, Networking and Mobile Computing}, 
title={Identity-Based Authentication Protocol with Paring of Tate on WSN}, 
year={2009}, 
pages={1-4}, 
abstract={Identity cryptography is widely used in security authentication. This paper proposes a standard identity-based authentication scheme, and also proves the security of the scheme under passive attack. On the basis of ECC and bilinear maps, paper implements the authentication protocol in the platform of TinyOs. Finally, paper analyzes its result and proves its feasibility.}, 
keywords={cryptographic protocols;message authentication;telecommunication security;wireless sensor networks;TinyOs platform;WSN;cryptography;identity-based authentication protocols;security authentication;wireless sensor networks;Application software;Authentication;Cryptographic protocols;Electronic mail;Elliptic curve cryptography;Hardware;Identity-based encryption;Public key cryptography;Security;Wireless sensor networks}, 
doi={10.1109/WICOM.2009.5302678}, 
ISSN={2161-9646}, 
month={Sept},}
@INPROCEEDINGS{6962305, 
author={E. Gebrewahid and M. Yang and G. Cedersj√∂ and Z. U. Abdin and V. Gaspes and J. W. Janneck and B. Svensson}, 
booktitle={2014 12th IEEE International Conference on Embedded and Ubiquitous Computing}, 
title={Realizing Efficient Execution of Dataflow Actors on Manycores}, 
year={2014}, 
pages={321-328}, 
abstract={Embedded DSP computing is currently shifting towards manycore architectures in order to cope with the ever growing computational demands. Actor based dataflow languages are being considered as a programming model. In this paper we present a code generator for CAL, one such dataflow language. We propose to use a compilation tool with two intermediate representations. We start from a machine model of the actors that provides an ordering for testing of conditions and firing of actions. We then generate an Action Execution Intermediate Representation that is closer to a sequential imperative language like C and Java. We describe our two intermediate representations and show the feasibility and portability of our approach by compiling a CAL implementation of the Two-Dimensional Inverse Discrete Cosine Transform on a general purpose processor, on the Epiphany manycore architecture and on the Ambric massively parallel processor array.}, 
keywords={data flow computing;digital signal processing chips;discrete cosine transforms;embedded systems;inverse transforms;multiprocessing systems;parallel processing;program compilers;program processors;Ambric massively parallel processor array;C language;CAL;CAL implementation;Epiphany manycore architecture;Java language;action execution intermediate representation;code generator;computational demands;dataflow actor execution;dataflow languages;embedded DSP computing;general purpose processor;machine model;manycore architectures;programming model;sequential imperative language;two-dimensional inverse discrete cosine transform;Arrays;Availability;Computational modeling;Optimization;Ports (Computers);Programming;Switches;CAL;code generation;compilation framework;dataflow languages;manycore}, 
doi={10.1109/EUC.2014.55}, 
month={Aug},}
@INPROCEEDINGS{7160118, 
author={M. Schoeberl and R. B. S√∏rensen and J. Spars√∏}, 
booktitle={2015 IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops}, 
title={Models of Communication for Multicore Processors}, 
year={2015}, 
pages={9-16}, 
abstract={To efficiently use multicore processors we need to ensure that almost all data communication stays on chip, i.e., The bits moved between tasks executing on different processor cores do not leave the chip. Different forms of on-chip communication are supported by different hardware mechanism, e.g., Shared caches with cache coherency protocols, core-to-core networks-on-chip, and shared scratchpad memories. In this paper we explore the different hardware mechanism for on-chip communication and how they support or favor different models of communication. Furthermore, we discuss the usability of the different models of communication for real-time systems.}, 
keywords={data communication;multiprocessing systems;core-to-core networks-on-chip;data communication modelling;multicore processors;on-chip communication;processor cores;real-time systems;shared caches;shared scratchpad memories;Computational modeling;Hardware;Multicore processing;Program processors;Real-time systems;System-on-chip;Time division multiplexing;multicore communication;real-time systems;time-predictable systems}, 
doi={10.1109/ISORCW.2015.57}, 
month={April},}
@INPROCEEDINGS{6651023, 
author={A. Branco and A. L. d. Moura and N. Rodriguez and S. Rossetto}, 
booktitle={2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum}, 
title={Teaching Concurrent and Distributed Computing -- Initiatives in Rio de Janeiro}, 
year={2013}, 
pages={1318-1323}, 
abstract={In this paper we describe two ongoing initiatives for teaching concurrency and distribution in PUC-Rio and UFRJ. One of them is a new approach for teaching distributed systems. Conventional distributed system courses follow a syllabus in which a list of topics is discussed independently and at different levels of abstractions. In Edupar'2012, we proposed a course with a novel approach, using a wireless sensor network environment to pin all topics down to concrete applications and to maintain issues such as fault tolerance and coordination continuously present. The second initiative is a smaller one, in which we insert a new topic in a Systems Software course to allow students to have a better understanding of what is application-level multitasking and of how it can be implemented. In this paper, we report on the experience of teaching the proposed syllabus and the adjustments that were necessary. We also discuss some plans for the courses in 2013.}, 
keywords={concurrency control;distributed processing;teaching;wireless sensor networks;application-level multitasking;concurrent computing;distributed computing;systems software course;teaching;wireless sensor network environment;Education;Fault tolerance;Fault tolerant systems;Materials;Programming;Proposals;Wireless sensor networks;application-level multitasking;coroutines;cross-cutting approaches;event-based programming;fault tolerance}, 
doi={10.1109/IPDPSW.2013.33}, 
month={May},}
@INPROCEEDINGS{5665620, 
author={R. Fritzsche and C. Siemers}, 
booktitle={2010 World Automation Congress}, 
title={Scheduling of time enhanced c (TEC)}, 
year={2010}, 
pages={1-6}, 
abstract={Real-time systems mainly consist of time or event-triggered tasks that must satisfy deadline-constraints and other limitations to the execution time. Scheduling of them is a common problem especially if no operating system can be used because of limited resources like code-size and CPU power. Previous approaches deal with multi-frame models to split tasks into smaller subtask that may be arranged at compile-time in a static way to cope with given deadlines. Handling of non-periodic events and context-switching problems demand a more dynamic scheduling. This paper presents an approach of using manually given information for timing constraints in order to rearrange the code to satisfy the deadlines automatically. The presented design is still able to handle events and to force the given functions to cooperate. Supporting hardware for producing timing-events may further help the system to organize the program-flow.}, 
keywords={dynamic scheduling;multiprogramming;real-time systems;software engineering;CPU power;context switching problem;deadline constraint;dynamic scheduling;event triggered task;multiframe model;nonperiodic event;program flow;real time system;split task;time enhanced C;timing constraint;Context;Switches;application-internal scheduler;forced cooperative design;multi-frame tasks;semi-dynamic scheduling;time-enhanced language}, 
ISSN={2154-4824}, 
month={Sept},}
@INPROCEEDINGS{5751508, 
author={C. Schumacher and R. Leupers and D. Petras and A. Hoffmann}, 
booktitle={2010 IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)}, 
title={parSC: Synchronous parallel SystemC simulation on multi-core host architectures}, 
year={2010}, 
pages={241-246}, 
abstract={Time-consuming cycle-accurate MPSoC simulation is often needed for debugging and verification. Its practicability is put at risk by the growing MPSoC complexity. This work presents a conservative synchronous parallel simulation approach along with a SystemC framework to accelerate tightly-coupled MPSoC simulations on multi-core hosts. Key contribution is the implementation strategy, which utilizes techniques from the high-performance computing domain. Results show speed-ups of up to 4.4 on four host cores.}, 
keywords={computer debugging;multiprocessing systems;network interfaces;parallel architectures;system-on-chip;MPSoC complexity;high performance computing domain;multicore host architectures;multicore host core;synchronous parallel systemC simulation;tightly coupled MPSoC simulation;time consuming cycle accurate MPSoC simulation;Computational modeling;Data models;Kernel;Load modeling;Logic gates;Prefetching;Synchronization;Experimentation;Measurement;Performance}, 
doi={10.1145/1878961.1879005}, 
month={Oct},}
@INPROCEEDINGS{5314042, 
author={D. L. Clark}, 
booktitle={2009 IEEE AUTOTESTCON}, 
title={Powering intelligent instruments with Lua scripting}, 
year={2009}, 
pages={101-106}, 
abstract={As the power of the integrated processors that control today's instruments continues to climb, instrument vendors will increasingly add features that allow users to utilize the added intelligence by embedding custom applications directly onboard the instrument. For the test, measurement and automation industries, this paradigm is a complement to, among other things, the advent of synthetic instruments that can ldquobe anything you want,rdquo the frequent use of mezzanine type hardware and the rise of the LXI specification in which instrument to instrument messaging allows one instrument to control and communicate with another without the necessity of a host PC. There are various approaches the instrument vendor can take to permit users to develop embedded applications to be run on the instrument processor. Arguably the most advantageous approach, to both the vendor and customer, is to embed a high level scripting language allowing the user to easily develop scripts to perform instrument based operations. The Lua scripting language is a compact, full featured scripting language that is easily portable and seamlessly integrates into embedded designs. Written in pure ISO ANSI-C, the Lua interpreter and Lua libraries have been successfully ported to a large number of platforms, big and small, and with and without advanced operating systems such as Windows and Linux. Lua contains an API for interfacing directly to and from the instrument's embedded firmware and includes a full suite of libraries. Further, Lua is extendable. Thus, in addition to embedding the language interpreter and libraries, the vendor can implement custom libraries and various other custom utilities to increase the flexibility of the system and enhance the capabilities of the user developed scripts. This paper studies the use of Lua in intelligent instrumentation. It discusses features that provide flexibility and power to users embedding applications onboard instruments and it presents some real wo- rld applications of the technology.}, 
keywords={Linux;application program interfaces;authoring languages;automatic test equipment;embedded systems;API;ATE system;ISO ANSI-C;LXI specification;Linux;Lua scripting language;Windows;automated test equipment;automation industries;custom libraries;custom utilities;embedded applications;full featured scripting language;high level scripting language;instrument based operation;instrument to instrument messaging;integrated processor;intelligent instrumentation;language interpreter;mezzanine type hardware;onboard instruments;Automatic control;Automatic testing;Automation;Communication industry;Hardware;ISO;Industrial control;Instruments;Libraries;Process control}, 
doi={10.1109/AUTEST.2009.5314042}, 
ISSN={1088-7725}, 
month={Sept},}
@ARTICLE{6122018, 
author={J. Diaz and C. Mu√±oz-Caro and A. Ni√±o}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={A Survey of Parallel Programming Models and Tools in the Multi and Many-Core Era}, 
year={2012}, 
volume={23}, 
number={8}, 
pages={1369-1386}, 
abstract={In this work, we present a survey of the different parallel programming models and tools available today with special consideration to their suitability for high-performance computing. Thus, we review the shared and distributed memory approaches, as well as the current heterogeneous parallel programming model. In addition, we analyze how the partitioned global address space (PGAS) and hybrid parallel programming models are used to combine the advantages of shared and distributed memory systems. The work is completed by considering languages with specific parallel support and the distributed programming paradigm. In all cases, we present characteristics, strengths, and weaknesses. The study shows that the availability of multi-core CPUs has given new impulse to the shared memory parallel programming approach. In addition, we find that hybrid parallel programming is the current way of harnessing the capabilities of computer clusters with multi-core nodes. On the other hand, heterogeneous programming is found to be an increasingly popular paradigm, as a consequence of the availability of multi-core CPUs+GPUs systems. The use of open industry standards like OpenMP, MPI, or OpenCL, as opposed to proprietary solutions, seems to be the way to uniformize and extend the use of parallel programming models.}, 
keywords={distributed memory systems;parallel programming;shared memory systems;MPI;OpenCL;OpenMP;computer clusters;distributed memory approach;distributed memory systems;heterogeneous parallel programming model;high-performance computing;hybrid parallel programming model;multicore CPUs+GPUs systems;multicore nodes;open industry standards;partitioned global address space;shared memory approach;shared memory parallel programming;Computational modeling;Graphics processing unit;Instruction sets;Message systems;Multicore processing;Parallel programming;Parallelism and concurrency;distributed programming;heterogeneous (hybrid) systems.}, 
doi={10.1109/TPDS.2011.308}, 
ISSN={1045-9219}, 
month={Aug},}
@INPROCEEDINGS{7336328, 
author={D. Yunge and P. Kindt and M. Balszun and S. Chakraborty}, 
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, 
title={Hybrid Apps: Apps for the Internet of Things}, 
year={2015}, 
pages={1175-1180}, 
abstract={Smartphones have become popular mainly because of the large variety of applications they can run. In contrast, most devices in the phone's environment - e.g., household appliances or environmental sensors - are much less flexible because their functionality is hardcoded at the design time. In order to realize the vision of the Internet of Things (IoT), where all devices communicate with each other to realize joint tasks, it is necessary that these devices are able to extend and adapt their functionalities on-the-fly based on their surrounding. To realize smart functionalities for IoT devices, we propose "hybrid Apps", the concept of Smartphone "Apps" applied to small embedded systems. In contrast with current packaged "smart home" solutions, where all appliances have to be changed to their smart counterparts at the same time, hybrid Apps permit an incremental and hence feasible deployment of the IoT vision. In this paper we discuss the challenges and opportunities associated with this approach. We argue code interpretation as a candidate reprogramming method for IoT devices and analyzed its feasibility with real-world measurements of key parameters such as computational and energy overhead. While in general, code interpretation incurs a large energy-overhead, we show that for typical IoT applications executed every few seconds, it is as low as 1%.}, 
keywords={Internet of Things;embedded systems;home computing;smart phones;Internet of Things;IoT applications;IoT devices;candidate reprogramming method;code interpretation;embedded systems;energy-overhead;hybrid apps;packaged smart home solutions;smartphone apps;Hardware;Java;Middleware;Protocols;Random access memory;Temperature sensors;Apps;Code Interpreter;Reprogrammability;Virtual Machines;Wireless Sensor Networks}, 
doi={10.1109/HPCC-CSS-ICESS.2015.292}, 
month={Aug},}
@INPROCEEDINGS{6164943, 
author={S. Roloff and F. Hannig and J. Teich}, 
booktitle={17th Asia and South Pacific Design Automation Conference}, 
title={Approximate time functional simulation of resource-aware programming concepts for heterogeneous MPSoCs}, 
year={2012}, 
pages={187-192}, 
abstract={The design and the programming of heterogeneous future MPSoCs including thousands of processor cores is a hard challenge. Means are necessary to program and simulate the dynamic behavior of such systems in order to dimension the hardware design and to verify the software functionality as well as performance goals. Cycle-accurate simulation of multiple parallel applications simultaneously running on different cores of the architecture would be much too slow and is not the desired level of detail. In this paper, we therefore present a novel high-level simulation approach which tackles the complexity and the heterogeneity of such systems and enables the investigation of a new computing paradigm called invasive computing. Here, the workload and its distribution are not known at compile-time but are highly dynamic and have to be adapted to the status (load, temperature, etc.) of the underlying architecture at run-time. We propose an approach for the modeling of tiled MPSoC architectures and the simulation of resource-aware programming concepts on these. This approach delivers important timing information about the parallel execution and also is taking into account the computational properties of possibly different types of cores.}, 
keywords={Computational modeling;Computer architecture;Programming;Reduced instruction set computing;Synchronization;Tiles}, 
doi={10.1109/ASPDAC.2012.6164943}, 
ISSN={2153-6961}, 
month={Jan},}
@INPROCEEDINGS{6059016, 
author={R. Inam and J. M√§ki-Turja and M. Sj√∂din and S. M. H. Ashjaei and S. Afshar}, 
booktitle={ETFA2011}, 
title={Support for hierarchical scheduling in FreeRTOS}, 
year={2011}, 
pages={1-10}, 
abstract={This paper presents the implementation of a Hierarchical Scheduling Framework (HSF) on an open source real-time operating system (FreeRTOS) to support the temporal isolation between a number of applications, on a single processor. The goal is to achieve predictable integration and reusability of independently developed components or applications. We present the initial results of the HSF implementation by running it on an AVR 32-bit board EVK1100. The paper addresses the fixed-priority preemptive scheduling at both global and local scheduling levels. It describes the detailed design of HSF with the emphasis of doing minimal changes to the underlying FreeRTOS kernel and keeping its API intact. Finally it provides (and compares) the results for the performance measures of idling and deferrable servers with respect to the overhead of the implementation.}, 
keywords={application program interfaces;object-oriented programming;operating system kernels;public domain software;real-time systems;scheduling;software reusability;API;AVR EVK1100;FreeRTOS kernel;application reusability;fixed-priority preemptive scheduling;global scheduling levels;hierarchical scheduling framework;independently developed component reusability;local scheduling levels;open source real-time operating system;predictable integration;single processor;temporal isolation;Job shop scheduling;Kernel;Processor scheduling;Real time systems;Schedules;Servers;fixed-priority scheduling;hierarchical scheduling framework;real-time systems}, 
doi={10.1109/ETFA.2011.6059016}, 
ISSN={1946-0740}, 
month={Sept},}
@INPROCEEDINGS{7983156, 
author={U. A. Noman and B. Negash and A. M. Rahmani and P. Liljeberg and H. Tenhunen}, 
booktitle={2017 14th IEEE Annual Consumer Communications Networking Conference (CCNC)}, 
title={From threads to events: Adapting a lightweight middleware for Contiki OS}, 
year={2017}, 
pages={486-491}, 
abstract={Interoperability is one of the key requirements in the Internet of Things considering the diverse platforms, communication standards and specifications available today. Inherent resource constraints in the majority of IoT devices makes it very difficult to use existing solutions for interoperability, thus demanding new approaches. This paper presents the process of adapting a lightweight interoperability middleware for IoT, LISA, from RIOT to Contiki OS and evaluates memory and power overheads. The middleware follows a service oriented architecture and classifies devices according to available resources to assign different roles, such as Application, Service and Manager Nodes. These roles live in different tiers in a generic IoT architecture, where the Manager nodes are located in the intermediate Fog layer. To adapt to an event based kernel of Contiki, the middleware defines and handles a set of events that are used to communicate with the user application. A network of nodes is simulated to show the architecture promoted by the middleware and the results are presented.}, 
keywords={Internet of Things;middleware;open systems;operating systems (computers);service-oriented architecture;Contiki OS;IoT architecture;IoT devices;LISA;RIOT;event based kernel;intermediate fog layer;lightweight interoperability middleware;manager nodes;memory overheads;power overheads;resource constraints;Internet of Things;Interoperability;Manganese;Message systems;Middleware;Protocols;Semantics;Contiki;Internet of Things;Interoperability;LISA}, 
doi={10.1109/CCNC.2017.7983156}, 
month={Jan},}
@INPROCEEDINGS{6473629, 
author={√Ç. L. V. d. Negreiros and A. V. Brito}, 
booktitle={2012 Brazilian Symposium on Computing System Engineering}, 
title={The Development of a Methodology with a Tool Support to the Distributed Simulation of Heterogeneous and Complexes Embedded Systems}, 
year={2012}, 
pages={37-42}, 
abstract={Nowadays, embedded systems contains a big computational power and consequently a big complexity. It is very common to find different kinds of applications being executed in embedded systems. With this scenario, it is necessary some method and/or tool that allows the simulation of those systems in an efficient and practice way. The goal of this paper is to expose the integration between Ptolemy II and HLA in order to enable the elaboration of one methodology, with a tool support, to model and simulate large scale heterogeneous embedded systems.}, 
keywords={computational complexity;digital simulation;embedded systems;HLA;Ptolemy II;big complexity;complexes embedded systems;computational power;distributed simulation;large scale heterogeneous embedded systems;tool support;Computational modeling;Computer architecture;Embedded systems;Hardware;Integrated circuit modeling;Mathematical model;Unified modeling language;Distributed Simulation;HLA;Heterogeneous Systems;High Level Architecture;Ptolemy}, 
doi={10.1109/SBESC.2012.16}, 
ISSN={2324-7886}, 
month={Nov},}
@INPROCEEDINGS{7515633, 
author={D. P. B. Renaux and F. P√∂ttker and C. E. Soares and C. C. Val√©rio}, 
booktitle={2016 IEEE 19th International Symposium on Real-Time Distributed Computing (ISORC)}, 
title={A State-Based Function-Queue Software Architecture for Electric Motor Control}, 
year={2016}, 
pages={229-236}, 
abstract={Increasing demands on functional and temporal requirements for the software in electric motor controllers demand for solutions that are efficient in time and space usage while providing the required functionality. Embedded software for electric motor control must deal with the control itself, and with operation, protection, supervision, safety, and user interfaces. Concerning this need, an embedded software multitasking architecture that combines the concept of function queues and of state-based code is proposed and compared to a standard implementation based on an RTOS. In the proposed solution, the queue of function pointers is partitioned into several shorter queues each one active in a given state of the system, thus, reducing queue management overhead.}, 
keywords={control engineering computing;embedded systems;machine control;software architecture;RTOS;electric motor control;embedded software multitasking architecture;function pointer queue;queue management;state-based code;state-based function-queue software architecture;user interfaces;Computer architecture;Electric motors;Embedded software;Motor drives;Multitasking;Real-time systems;Electrical Motor Control;Embedded Software Multitasking Architecture;Real-Time Embedded Software;Task Scheduling}, 
doi={10.1109/ISORC.2016.39}, 
month={May},}
@INPROCEEDINGS{6825342, 
author={A. V. Brito and A. V. Negreiros}, 
booktitle={2013 III Brazilian Symposium on Computing Systems Engineering}, 
title={Allowing Large-Scale Systems Evaluation with Ptolemy through Distributed Simulation}, 
year={2013}, 
pages={53-58}, 
abstract={Nowadays, embedded systems have a huge amount of computational power and consequently, high complexity. It is quite usual to find different applications being executed in embedded systems. Embedded system design demands for method and tools that allow the simulation and verification in an efficient and practical way. This paper proposes the development and evaluation of a solution for embedded modeling and simulation of embedded systems in a distributed way by the integration of Ptolemy II and the High Level Architecture (HLA), in order to create an environment with high-performance execution of large-scale models. Experimental results demonstrated that the use of a non distributed simulation for some situations can be infeasible. It was demonstrated that a speedup of factor 4 was acquired when a model with 4,000 thousands actors were distributed in 8 different machines. It also presented a model of execution of each CPU core during the simulation.}, 
keywords={digital simulation;embedded systems;telecommunication computing;wireless sensor networks;CPU core;HLA;Ptolemy II;embedded modeling;embedded simulation;embedded system design;high level architecture;high-performance execution;large-scale models;large-scale systems evaluation;nondistributed simulation;wireless sensor network;Computational modeling;Computer architecture;Embedded systems;Graphics;Unified modeling language;Wireless sensor networks;Distributed Simulation;Embedded Systems;Heterogeneous Simulation}, 
doi={10.1109/SBESC.2013.19}, 
ISSN={2324-7886}, 
month={Dec},}
@INPROCEEDINGS{6575497, 
author={P. Kugler and P. Nordhus and B. Eskofier}, 
booktitle={2013 IEEE International Conference on Body Sensor Networks}, 
title={Shimmer, Cooja and Contiki: A new toolset for the simulation of on-node signal processing algorithms}, 
year={2013}, 
pages={1-6}, 
abstract={Wearable sensors are widely used for data collection in many applications. Ssensor nodes have also been applied for real-time applications, e.g. for ECG analysis or activity and fall detection. Processing of the sensor data is either done on an external device or on the node itself. While on-node processing reduces data rate and increases battery life, development and testing can be time-consuming. To allow faster implementation of such algorithms, we propose a simulation framework for the Shimmer platform using the Cooja simulator, MSPSim and the Contiki operating system. We provide the simulator and example applications compatible with the ShimmerConnect protocol, allowing streaming of raw and pre-processed sensor data to MATLAB, LabView and Android. Additionally, a simple activity and fall detection algorithm was implemented on the sensor node and evaluated using both the simulator and real hardware. In the future this will allow rapid development and testing of on-node pre-processing algorithms.}, 
keywords={Bluetooth;Hardware;Operating systems;Sensors;Testing;Wireless communication;Wireless sensor networks}, 
doi={10.1109/BSN.2013.6575497}, 
ISSN={2376-8886}, 
month={May},}
@ARTICLE{7937791, 
author={L. Rodr√≠guez-Gil and J. Garc√≠a-Zubia and P. Ordu√±a and D. L√≥pez-de-Ipi√±a}, 
journal={IEEE Access}, 
title={An Open and Scalable Web-Based Interactive Live-Streaming architecture: The WILSP Platform}, 
year={2017}, 
volume={5}, 
pages={9842-9856}, 
abstract={Interactive live-streaming applications and platforms face particular challenges: the actions of the viewer's affect the content of the stream. A minimal capture-render delay is critical. This is the case of applications, such as remote laboratories, which allow students to view specific hardware through a webcam, and interact with it remotely in close to real time. It is also the case of other applications, such as videoconferencing or remote rendering. In the latest years, several commercial live-streaming platforms have appeared. However, the most of them have two significant limitations. First, because they are oriented toward standard live-streaming, their capture-render delay tends to be too high for interactive live-streaming. Second, their architectures and sources are closed. That makes them unsuitable for many research and practical purposes, especially when customization is required. This paper presents the requirements for an interactive live-streaming platform, focusing on remote lab needs as a case study. Then, it proposes an architecture to satisfy those requirements that relies on Redis to achieve high scalability. The architecture is based on open technologies, and has been implemented and published as open source. From a client-side perspective, it is web-based and mobile-friendly. It is intended to be useful for both research and practical purposes. Finally, this paper experimentally evaluates the proposed architecture through its contributed implementation, analyzing its performance and scalability.}, 
keywords={Internet;computer aided instruction;interactive systems;laboratories;media streaming;Redis;WILSP platform;Webcam;client-side perspective;commercial live-streaming platforms;minimal capture-render delay;open Web-based interactive live-streaming architecture;open technologies;remote lab needs;scalable Web-based interactive live-streaming architecture;standard live-streaming;viewer action;Computer architecture;Delays;Remote laboratories;Robots;Scalability;Standards;Streaming media;Webcam;live streaming;live streaming platform;online learning tools;open;remote laboratories}, 
doi={10.1109/ACCESS.2017.2710328}, 
ISSN={2169-3536}, 
month={},}
@ARTICLE{6242797, 
author={C. J. Martin-Arguedas and D. Romero-Laorden and O. Martinez-Graullera and M. Perez-Lopez and L. Gomez-Ullate}, 
journal={IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control}, 
title={An ultrasonic imaging system based on a new SAFT approach and a GPU beamformer}, 
year={2012}, 
volume={59}, 
number={7}, 
pages={1402-1412}, 
abstract={The design of newer ultrasonic imaging systems attempts to obtain low-cost, small-sized devices with reduced power consumption that are capable of reaching high frame rates with high image quality. In this regard, synthetic aperture techniques have been very useful. They reduce hardware requirements and accelerate information capture. However, the beamforming process is still very slow, limiting the overall speed of the system. Recently, general-purpose computing on graphics processing unit techniques have been proposed as a way to accelerate image composition. They provide excellent computing power with which a very large volume of data can easily and quickly be processed. This paper describes a new system architecture that merges both principles. Thus, using a minimum-redundancy synthetic aperture technique to acquire the signals (2R-SAFT), and a graphics processing unit as a beamformer, we have developed a new scanner with full dynamic focusing, both on emission and reception, that attains real-time imaging with very few resources.}, 
keywords={array signal processing;computerised instrumentation;graphics processing units;signal detection;ultrasonic imaging;2R-SAFT approach;GPU beamforming processing;dynamic focusing;general-purpose computing;graphics processing unit technique;hardware requirement;image composition acceleration;image quality;information capture acceleration;minimum-redundancy synthetic aperture technique;power consumption;real-time imaging;signal acquisition;ultrasonic imaging system;Apertures;Array signal processing;Focusing;Graphics processing unit;Hardware;Ultrasonic imaging;0}, 
doi={10.1109/TUFFC.2012.2341}, 
ISSN={0885-3010}, 
month={July},}
@ARTICLE{5710575, 
author={W. Liu and J. Xu and J. K. Muppala and W. Zhang and X. Wu and Y. Ye}, 
journal={IEEE Embedded Systems Letters}, 
title={Coroutine-Based Synthesis of Efficient Embedded Software From SystemC Models}, 
year={2011}, 
volume={3}, 
number={1}, 
pages={46-49}, 
abstract={SystemC is a widely used electronic system-level (ESL) design language that can be used to model both hardware and software at different stages of system design. There has been a lot of research on behavior synthesis of hardware from SystemC, but relatively little work on synthesizing embedded software for SystemC designs. In this letter, we present an approach to automatic software synthesis from SystemC-based on coroutines instead of the traditional approaches based on real-time operating system (RTOS) threads. Performance evaluation results on some realistic applications show that our approach results in impressive reduction of runtime overheads compared to the thread-based approaches.}, 
keywords={C++ language;embedded systems;operating systems (computers);SystemC models;coroutine-based synthesis;electronic system-level design language;embedded software synthesis;real-time operating system threads;Context;Instruction sets;Kernel;Prototypes;Switches;Synchronization;Performance;SystemC;software synthesis}, 
doi={10.1109/LES.2011.2112634}, 
ISSN={1943-0663}, 
month={March},}
@INPROCEEDINGS{6064506, 
author={E. A. Lee}, 
booktitle={2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)}, 
title={Heterogeneous actor modeling}, 
year={2011}, 
pages={3-12}, 
abstract={Complex systems demand diversity in the modeling mechanisms. This ‚Äúroadmap‚Äù paper prescribes an approach to modeling based on concurrent communicating components (called actors), where a diversity of orchestration strategies govern the execution and interaction of the components. The prescribed approach has been extensively explored in the Ptolemy Project, but as yet is not widely deployed in engineering practice. The approach achieves interaction between diverse models using an abstract semantics, which is a deliberately incomplete semantics that cannot by itself define a useful modeling framework. It instead focuses on the interactions between diverse models, reducing the nature of those interactions to a minimum that achieves a well-defined composition. The actor semantics is an abstract semantics that can handle many heterogeneous models that are built today, and some that are not common today. The actor abstract semantics and many concrete semantics are implemented in Ptolemy II, an open-source software framework.}, 
keywords={large-scale systems;multiprocessing programs;public domain software;Ptolemy project;abstract semantics;actor semantics;complex systems;concurrent communicating components;heterogeneous actor modeling;modeling mechanisms;open-source software;orchestration strategies;roadmap;Adaptation models;Computational modeling;Mathematical model;Object oriented modeling;Semantics;Syntactics;Unified modeling language;Ptolemy;heterogeneity;models of computation}, 
doi={10.1145/2038642.2038646}, 
month={Oct},}
@INPROCEEDINGS{4577683, 
author={H. Schweppe and A. Zimmermann and D. Grilly}, 
booktitle={2008 International Symposium on Industrial Embedded Systems}, 
title={Flexible in-vehicle stream processing with distributed automotive control units for engineering and diagnosis}, 
year={2008}, 
pages={74-81}, 
abstract={This paper introduces a method for selectively pre-processing and recording sensor data for engineering testing purposes in vehicles. In order to condense data, methodologies from the domain of sensor networks and stream processing are applied, which results in a reduction of the quantity of data, while maintaining information quality. A situation-dependent modification of recording parameters allows for a detailed profiling of vehicle-related errors. We developed a data-flow oriented model, in which data streams are connected by processing nodes. These nodes filter and aggregate the data and can be connected in nearly any order, which permits a successive composition of the aggregation and recording strategy. The integration with an event-condition-action model provides adaptability of the processing and recording, depending on the state of the vehicle. In a proof-of-concept system, which we implemented on top of the automotive diagnostic protocols KWP and UDS, the feasibility of the approach was shown. The target platform was an embedded on-board computer that is connected to the OBD-II interface of the vehicle. As the scope of recording can be adjusted flexibly, the recording system can not only be used for diagnostic purposes, but also serves objectives in development, quality assurance, and even marketing.}, 
keywords={automobiles;automotive electronics;fault diagnosis;OBD-II interface;automotive diagnostic protocol;data-flow oriented model;diagnostic software interface;distributed automotive control unit;embedded onboard computer;engineering testing purpose;event-condition-action model;flexible in-vehicle stream processing;proof-of-concept system;sensor network;Aggregates;Automotive engineering;Computer interfaces;Data engineering;Distributed control;Filters;Maintenance engineering;Protocols;Testing;Vehicles}, 
doi={10.1109/SIES.2008.4577683}, 
ISSN={2150-3109}, 
month={June},}
@INPROCEEDINGS{6176441, 
author={A. Marongiu and P. Burgio and L. Benini}, 
booktitle={2012 Design, Automation Test in Europe Conference Exhibition (DATE)}, 
title={Fast and lightweight support for nested parallelism on cluster-based embedded many-cores}, 
year={2012}, 
pages={105-110}, 
abstract={Several recent many-core accelerators have been architected as fabrics of tightly-coupled shared memory clusters. A hierarchical interconnection system is used - with a crossbar-like medium inside each cluster and a network-on-chip (NoC) at the global level - which make memory operations non-uniform (NUMA). Nested parallelism represents a powerful programming abstraction for these architectures, where a first level of parallelism can be used to distribute coarse-grained tasks to clusters, and additional levels of fine-grained parallelism can be distributed to processors within a cluster. This paper presents a lightweight and highly optimized support for nested parallelism on cluster-based embedded many-cores. We assess the costs to enable multi-level parallelization and demonstrate that our techniques allow to extract high degrees of parallelism.}, 
keywords={embedded systems;network-on-chip;shared memory systems;NUMA;NoC;cluster based embedded manycores;fine grained parallelism;hierarchical interconnection system;manycore accelerators;memory operations nonuniform;nested parallelism;network-on-chip;programming abstraction;shared memory clusters;Arrays;Instruction sets;Parallel processing;Programming;Synchronization}, 
doi={10.1109/DATE.2012.6176441}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{4725258, 
author={M. Khezri and M. A. Sarram and F. Adibniya}, 
booktitle={2008 IEEE International Symposium on Parallel and Distributed Processing with Applications}, 
title={Simplifying Concurrent Programming of Networked Embedded Systems}, 
year={2008}, 
pages={993-998}, 
abstract={TinyOS is the current state of the art in operating systems for sensor network research. Event- based programming model of TinyOS presents concept of Task to allow postponing processing. For little processing and memory overhead and to avoid race conditions, tasks are non-preemptive. This causes executing long running task reduce system responsiveness. In general two approaches suggested for solving this problem: cooperative and multithreaded multitasking. In this paper we propose a new TinyOS task scheduler to integrate these approaches with new type of tasks. We argue that this approach improves the overall system responsiveness without concerning about data races or complicate programming for developers.}, 
keywords={multi-threading;operating systems (computers);processor scheduling;telecommunication computing;wireless sensor networks;TinyOS;concurrent programming;cooperative multitasking;event-based programming model;memory overhead;multithreaded multitasking;networked embedded systems;operating systems;postponing processing;sensor network;task scheduler;Delay;Embedded system;Job shop scheduling;Multitasking;Operating systems;Programming profession;Sensor systems;Sensor systems and applications;Wireless sensor networks;Yarn;Cooperative;Embedded System;Multitasking;Scheduler;TinyOS;Yield}, 
doi={10.1109/ISPA.2008.138}, 
ISSN={2158-9178}, 
month={Dec},}
@INPROCEEDINGS{7328218, 
author={A. Capotondi and A. Marongiu and L. Benini}, 
booktitle={2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip}, 
title={Enabling Scalable and Fine-Grained Nested Parallelism on Embedded Many-cores}, 
year={2015}, 
pages={297-304}, 
abstract={Current high-end embedded systems are designed as heterogeneous systems-on-chip (SoCs), where a general-purpose host processor is coupled to a programmable manycore accelerator (PMCA). Such PMCAs typically leverage hierarchical interconnect and distributed memory with non-uniform access (NUMA). Nested parallelism is a convenient programming abstraction for large-scale cc-NUMA systems, which allows to hierarchically (and dynamically) create multiple levels of fine-grained parallelism whenever it is available. Available implementations for cc-NUMA systems introduce large overheads for nested parallelism management, which cannot be tolerated due to the extremely fine-grained nature of embedded parallel workloads. In particular, creating a team of parallel threads has a cost that increases linearly with the number of threads, which is inherently non scalable. This work presents a software cache mechanism for frequently-used parallel team configurations to speed up parallel thread creation overheads in PMCA systems. When a configuration is found in the cache the cost for parallel team creation has a constant time, providing a scalable mechanism. We evaluated our support on the STMicroelectronics STHORM many-core. Compared to the state-of-the art, our solution shows that: i) the cost for parallel team creation is reduced by up to 67%, ii) the tangible effect on real ultra-fine-grained parallel kernels is a speedup of up to 80%.}, 
keywords={cache storage;embedded systems;multiprocessing systems;parallel programming;system-on-chip;PMCA;STMicroelectronics STHORM many-core;SoC;distributed memory with nonuniform access;embedded many-cores;embedded systems;fine-grained parallelism;heterogeneous systems-on-chip;nested parallelism;parallel team configuration;parallel thread creation overhead;programmable manycore accelerator;programming abstraction;software cache mechanism;Fabrics;Instruction sets;Message systems;Parallel processing;Programming;Recruitment;Embedded Many-Core Architectures;OpenMP;Parallel Programming Models}, 
doi={10.1109/MCSoC.2015.47}, 
month={Sept},}
@INPROCEEDINGS{6069480, 
author={B. Haetzer and M. Radetzki}, 
booktitle={FDL 2011 Proceedings}, 
title={A case study on message-based discrete event simulation for Transaction Level Modeling}, 
year={2011}, 
pages={1-8}, 
abstract={Transaction Level Modeling is a system-level design methodology for early design space exploration. The increasing complexity of systems makes it necessary to improve simulation performance. Using parallel discrete event simulation approaches seems promising to speedup the simulation runs of complex transaction level models. One of such approaches is the message-based PDES approach which is applied to TLM in this paper. Two different TLM case studies are used to evaluate and compare the execution times of sequential and message-based simulation. We show that with message-based simulation a speedup over sequential simulation can be achieved even if using only one processor core. This result lies the foundation for further speedup if running on multiple cores as no more significant synchronization overhead is expected.}, 
keywords={computational complexity;discrete event simulation;electronic engineering computing;system-on-chip;design space exploration;message based PDES;message based discrete event simulation;multiple cores;parallel discrete event simulation;processor core;sequential simulation;system level design methodology;systems complexity;transaction level modeling;Computational modeling;Context;Kernel;Switches;Time domain analysis;Time varying systems}, 
ISSN={1636-9874}, 
month={Sept},}
@ARTICLE{4397184, 
author={P. Wilson and A. Frey and T. Mihm and D. Kershaw and T. Alves}, 
journal={IEEE Design Test of Computers}, 
title={Implementing Embedded Security on Dual-Virtual-CPU Systems}, 
year={2007}, 
volume={24}, 
number={6}, 
pages={582-591}, 
abstract={In this article, we describe a low-cost, dual-virtual-CPU hardware technology for embedded-systems security. We also present a case study of a programmable software design to exploit such hardware. This design integrates a rich operating system without requiring significant changes to it, while maintaining preemptive and real-time properties, exception handling, and power management.}, 
keywords={cryptography;embedded systems;flash memories;logic partitioning;operating systems (computers);virtual storage;dual-virtual-CPU system;embedded-systems security;flash memory;logical partitioning;off-chip storage device;operating system;programmable software design;Application software;Central Processing Unit;Circuit testing;Hardware;Information security;Isolation technology;Kernel;Operating systems;Packaging;Space technology;TrustZone technology;embedded security;programmable;security software framework}, 
doi={10.1109/MDT.2007.196}, 
ISSN={0740-7475}, 
month={Nov},}
@INPROCEEDINGS{7459504, 
author={D. Cesarini and A. Marongiu and L. Benini}, 
booktitle={2016 Design, Automation Test in Europe Conference Exhibition (DATE)}, 
title={An optimized task-based runtime system for resource-constrained parallel accelerators}, 
year={2016}, 
pages={1261-1266}, 
abstract={Manycore accelerators have recently proven a promising solution for increasingly powerful and energy efficient computing systems. This raises the need for parallel programming models capable of effectively leveraging hundreds to thousands of processors. Task-based parallelism has the potential to provide such capabilities, offering flexible support to fine-grained and irregular parallelism. However, efficiently supporting this programming paradigm on resource-constrained parallel accelerators is a challenging task. In this paper, we present an optimized implementation of the OpenMP tasking model for embedded parallel accelerators, discussing the key design solution that guarantee small memory (footprint) and minimize performance overheads. We validate our design by comparing to several state-of-the-art tasking implementations, using the most representative parallelization patterns. The experimental results confirm that our solution achieves near-ideal speedups for tasks as small as 5K cycles.}, 
keywords={embedded systems;multiprocessing systems;parallel programming;performance evaluation;OpenMP tasking model;embedded parallel accelerators;energy efficIent computing systems;fine-grained parallelism;irregular parallelism;manycore accelerators;optimized task-based runtime system;parallel programming models;performance overhead minimization;resource-constrained parallel accelerators;task-based parallelism;Context;Instruction sets}, 
month={March},}
@INPROCEEDINGS{4536359, 
author={K. B. Wheeler and R. C. Murphy and D. Thain}, 
booktitle={2008 IEEE International Symposium on Parallel and Distributed Processing}, 
title={Qthreads: An API for programming with millions of lightweight threads}, 
year={2008}, 
pages={1-8}, 
abstract={Large scale hardware-supported multithreading, an attractive means of increasing computational power, benefits significantly from low per-thread costs. Hardware support for lightweight threads is a developing area of research. Each architecture with such support provides a unique interface, hindering development for them and comparisons between them. A portable abstraction that provides basic lightweight thread control and synchronization primitives is needed. Such an abstraction would assist in exploring both the architectural needs of large scale threading and the semantic power of existing languages. Managing thread resources is a problem that must be addressed if massive parallelism is to be popularized. The qthread abstraction enables development of large-scale multithreading applications on commodity architectures. This paper introduces the qthread API and its Unix implementation, discusses resource management, and presents performance results from the HPCCG benchmark.}, 
keywords={Unix;application program interfaces;multi-threading;resource allocation;API;HPCCG benchmark;Qthreads;Unix implementation;application program interfaces;large scale hardware supported multithreading;lightweight threads;qthread API;qthread abstraction;resource management;Computer architecture;Costs;Hardware;Laboratories;Large-scale systems;Multithreading;Parallel processing;Programming profession;Resource management;Yarn}, 
doi={10.1109/IPDPS.2008.4536359}, 
ISSN={1530-2075}, 
month={April},}
@INPROCEEDINGS{7363616, 
author={S. Park and H. Kim and S. Y. Kang and C. H. Koo and H. Joe}, 
booktitle={2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing}, 
title={Lua-Based Virtual Machine Platform for Spacecraft On-Board Control Software}, 
year={2015}, 
pages={44-51}, 
abstract={Mission critical embedded software for autonomous operation requires high development cost due to its long development cycle. One of the potential solutions for reducing the cost is to reuse the software developed at previous missions. Virtual machine platform such as JVM is a good example to provide code portability across various missions. Flight software in aerospace field is adopting this concept to improve reusability and eventually to reduce development cost. In this paper, we propose a Lua-based virtualization environment for spacecraft flight software. Flight software for spacecraft control consists of a few tasks that are highly autonomous. Lua is chosen as the script language for programming the control tasks. Though Lua was designed with simplicity and portability, it only supports multithreading with collaborative coroutines. To support preemptive multitasking, we implement time slicing coroutines as spacecraft control processes. New coroutine scheduler is devised and time slicing functionality is added into the scheduler. Scheduler locking and message passing with external flight software are also implemented. Instead of modifying the Lua interpreter, we have exploited the debug support APIs for our implementation. For evaluation, we have implemented the flight software virtualization environment on the flight computer. Accuracy of the time slicing scheduler is also analyzed.}, 
keywords={aerospace control;application program interfaces;authoring languages;control engineering computing;message passing;multi-threading;program debugging;scheduling;software portability;software reusability;space vehicles;spacecraft computers;virtual machines;virtualisation;JVM;Lua interpreter;Lua script language;Lua-based virtual machine platform;Lua-based virtualization environment;aerospace field;autonomous operation;code portability;collaborative coroutines;control task programming;coroutine scheduler;debug support API;development cost reduction;flight computer;flight software virtualization environment;highly autonomous task;message passing;mission critical embedded software;multithreading;preemptive multitasking;scheduler locking;software reuse;spacecraft control;spacecraft flight software;spacecraft on-board control software;time slicing coroutines;time slicing scheduler;Computers;Engines;Runtime;Software;Space vehicles;Virtual machining;Virtualization;Lua;OBCP;mission critical embedded software;reusability;spacecraft;virtual machine}, 
doi={10.1109/EUC.2015.21}, 
month={Oct},}
@INPROCEEDINGS{6913222, 
author={C. Motika and R. von Hanxleden and M. Heinold}, 
booktitle={16th IEEE International Symposium on Object/component/service-oriented Real-time distributed Computing (ISORC 2013)}, 
title={Programming deterministic reactive systems with Synchronous Java}, 
year={2013}, 
pages={1-8}, 
abstract={A key issue in the development of reliable embedded software is the proper handling of reactive control-flow, which typically involves concurrency. Java and its thread concept have only limited provisions for implementing deterministic concurrency. Thus, as has been observed in the past, it is challenging to develop concurrent Java programs without any deadlocks or race conditions. To alleviate this situation, the Synchronous Java (SJ) approach presented here adopts the key concepts that have been established in the world of synchronous programming for handling reactive control-flow. Thus SJ not only provides deterministic concurrency, but also different variants of deterministic preemption. Furthermore SJ allows concurrent threads to communicate with Esterel-style signals. As a case study for an embedded system usage, we also report on how the SJ concepts have been ported to the ARM-based Lego Mindstorms NXT system.}, 
keywords={Java;concurrency control;embedded systems;object-oriented programming;ARM-based Lego Mindstorms NXT system;Esterel-style signals;Synchronous Java;concurrent Java programs;deterministic concurrency;deterministic preemption;deterministic reactive systems programming;embedded software development;embedded system usage;reactive control-flow handling;synchronous programming;Concurrent computing;Instruction sets;Java;Monitoring;Real-time systems;Switches;Synchronization}, 
doi={10.1109/ISORC.2013.6913222}, 
ISSN={1555-0885}, 
month={June},}
@INPROCEEDINGS{7117986, 
author={M. Pavlov and A. Petrov}, 
booktitle={2015 17th Conference of Open Innovations Association (FRUCT)}, 
title={Software architecture for scalable computing systems with automatic granularity selection of executable code}, 
year={2015}, 
pages={151-156}, 
abstract={The problem of developing software architecture and its platform implementation for scalable cloud services is addressed in the paper. New scheme of distributed software developing and executing is presented with argumentation and main principles behind solution. Performance evaluation of one of the platform components (data storage) is described.}, 
keywords={cloud computing;granular computing;software architecture;automatic granularity selection;data storage;distributed software development;distributed software execution;executable code;performance evaluation;scalable cloud services;scalable computing systems;software architecture;Computer architecture;Memory;Optimization;Runtime;Software;Software architecture;Virtual machining}, 
doi={10.1109/FRUCT.2015.7117986}, 
ISSN={2305-7254}, 
month={April},}
@INPROCEEDINGS{5694285, 
author={D. W. Chang and C. D. Jenkins and P. C. Garcia and S. Z. Gilani and P. Aguilera and A. Nagarajan and M. J. Anderson and M. A. Kenny and S. M. Bauer and M. J. Schulte and K. Compton}, 
booktitle={2010 International Conference on Field Programmable Logic and Applications}, 
title={ERCBench: An Open-Source Benchmark Suite for Embedded and Reconfigurable Computing}, 
year={2010}, 
pages={408-413}, 
abstract={Researchers in embedded and reconfigurable computing are often hindered by a lack of suitable benchmarks with which to accurately evaluate their work. Without a suitable benchmark suite, researchers use either outdated, unrealistic benchmarks or spend valuable time creating their own. In this paper, we present ERCBench - a freely-available, open-source benchmark suite geared towards embedded and reconfigurable computing research. ERCBench benchmarks represent a variety of application areas, including multimedia processing, wireless communications, and cryptography. They consist of synthesizable Verilog models for hardware accelerators and hybrid hardware/software applications that combine software-based control flow with hardware-based computation tasks.}, 
keywords={benchmark testing;hardware description languages;public domain software;reconfigurable architectures;ERCBench;Verilog model;embedded computing;hardware accelerator;hardware-based computation;open source benchmark suite;reconfigurable computing;software- based control flow;benchmarks;embedded computing;open-source;reconfigurable computing}, 
doi={10.1109/FPL.2010.85}, 
ISSN={1946-147X}, 
month={Aug},}
@INPROCEEDINGS{4224663, 
author={C. Suh and J. E. Joung and Y. B. Ko}, 
booktitle={2007 IEEE Wireless Communications and Networking Conference}, 
title={New RF Models of the TinyOS Simulator for IEEE 802.15.4 Standard}, 
year={2007}, 
pages={2236-2240}, 
abstract={Recently, wireless sensor networks have gained increasing attention from the industry as well as academia. Various research issues related with sensor networks are intensively proposed, and they are evaluated by some network simulators or real sensor platforms. One of the well-known simulators for wireless sensor networks is called TOSSIM. It can simulate with TinyOS source codes on the real testbed without any significant modifications. Although TOSSIM's architecture and interfaces are well designed for wireless sensor networks based on IEEE 802.15.4 standards, its current RF model is too simple to support main features of the PHY stack of the IEEE 802.15.4. In order to enhance the accuracy of wireless simulation results and implement IEEE 802.15.4 standard, we design a new wireless propagation model and RF physical stack based on the two-ray ground path loss model and CC2420 RF transceiver. Our work contributes on the performance evaluation areas of wireless sensor networks and IEEE 802.15.4 WPAN standard using simulations.}, 
keywords={IEEE standards;personal area networks;source coding;telecommunication standards;transceivers;wireless sensor networks;CC2420 RF transceiver;IEEE 802.15.4 standard;RF models;RF physical stack;TOSSIM;TinyOS simulator;WPAN standard;source codes;wireless propagation model;wireless sensor networks;Hardware;Intelligent sensors;Operating systems;Propagation losses;Radio frequency;Sensor systems;Transceivers;Wireless application protocol;Wireless communication;Wireless sensor networks}, 
doi={10.1109/WCNC.2007.418}, 
ISSN={1525-3511}, 
month={March},}
@INPROCEEDINGS{4678868, 
author={M. Yu and S. Xiahou and X. Li}, 
booktitle={2008 4th International Conference on Wireless Communications, Networking and Mobile Computing}, 
title={A Survey of Studying on Task Scheduling Mechanism for TinyOS}, 
year={2008}, 
pages={1-4}, 
abstract={Although TinyOS has been regarded as the defacto standard for WSN (Wireless Sensor Network) applications, its simple task scheduling mechanism became a great obstacle to WSN applications. This paper, from two directions (one based on cooperative, the other based on preemptive), presented a variety of scheduling algorithms and their application in TinyOS. And their characters and advantage were discussed as well in terms of energy consuming, tasks executing efficiency. Then a new, integrated and adaptive task scheduling mechanism was pointed out for the future TinyOS task scheduling. This new scheduling mechanism was characterized with features of dynamical adaptability and context-awareness.}, 
keywords={scheduling;wireless sensor networks;TinyOS;task scheduling mechanism;wireless sensor network;Adaptive scheduling;Computer languages;Context;Dynamic scheduling;Mobile communication;Mobile computing;Processor scheduling;Scheduling algorithm;Sensor systems and applications;Wireless sensor networks}, 
doi={10.1109/WiCom.2008.960}, 
ISSN={2161-9646}, 
month={Oct},}
@INPROCEEDINGS{7383583, 
author={Z. Cheng and Y. Li and R. West}, 
booktitle={2015 IEEE Real-Time Systems Symposium}, 
title={Qduino: A Multithreaded Arduino System for Embedded Computing}, 
year={2015}, 
pages={261-272}, 
abstract={Arduino is an open source platform that offers a clear and simple environment for physical computing. It is now widely used in modern robotics and Internet of Things (IoT) applications, due in part to its low-cost, ease of programming, and rapid prototyping capabilities. Sensors and actuators can easily be connected to the analog and digital I/O pins of an Arduino device, which features an on-board microcontroller programmed using the Arduino API. The increasing complexity of physical computing applications has now led to a series of Arduino-compatible devices with faster processors, increased flash storage, larger memories and more complicated I/O architectures. The Intel Galileo, for example, is designed to support the Arduino API on top of a Linux system, code-named Clanton. However, the standard API is restricted to the capabilities found on less powerful devices, lacking support for multithreaded programs, or specification of real-time requirements. In this paper, we present Qduino, a system developed for Arduino compatible boards. Qduino provides an extended Arduino API which, while backward-compatible with the original API, supports real-time multithreaded sketches and event handling. Experiments show the performance gains of Qduino compared to Clanton Linux.}, 
keywords={Linux;application program interfaces;formal specification;multi-threading;public domain software;real-time systems;Arduino API;Arduino compatible boards;Arduino-compatible devices;Clanton;I/O architectures;Intel Galileo;Internet of Things applications;IoT applications;Linux system;Qduino;Quest real-time operating system;actuators;backward-compatibility;embedded computing;event handling;flash storage;multithreaded Arduino system;multithreaded programs;on-board microcontroller;open source platform;physical computing applications;programming;rapid prototyping capabilities;real-time multithreaded sketches;real-time requirement specification;robotics;sensors;Computer architecture;Hardware;Instruction sets;Kernel;Linux;Real-time systems;Standards;Arduino;embedded systems;multi-threading;real-time}, 
doi={10.1109/RTSS.2015.32}, 
ISSN={1052-8725}, 
month={Dec},}
@INPROCEEDINGS{5775121, 
author={R. S. Khaligh and M. Radetzki}, 
booktitle={2010 Forum on Specification Design Languages (FDL 2010)}, 
title={A dynamic load balancing method for parallel simulation of accuracy adaptive TLMs}, 
year={2010}, 
pages={1-6}, 
abstract={In this paper we present a load balancing method for parallel simulation of accuracy adaptive transaction level models. In contrast to traditional fixed accuracy TLMs, timing accuracy of adaptive TLMs changes during simulation. This makes the computation and synchronization characteristics of the models variable, and practically prohibits the use of static load balancing. To deal with this issue, we present a light-weight load balancing method which takes advantage of, and can be easily incorporated with the simulation time synchronization scheme used in parallel TLM simulation. We have developed a high performance parallel simulation kernel based on the proposed method, and our experiments using the developed kernel show the effectiveness of the proposed approach in a realistic scenario.}, 
keywords={discrete event simulation;resource allocation;accuracy adaptive TLM;dynamic load balancing method;high performance parallel simulation kernel;parallel simulation;simulation time synchronization scheme}, 
doi={10.1049/ic.2010.0141}, 
month={Sept},}
@INPROCEEDINGS{6513574, 
author={R. von Hanxleden and M. Mendler and J. Aguado and B. Duderstadt and I. Fuhrmann and C. Motika and S. Mercer and O. O'Brien}, 
booktitle={2013 Design, Automation Test in Europe Conference Exhibition (DATE)}, 
title={Sequentially constructive concurrency A conservative extension of the synchronous model of computation}, 
year={2013}, 
pages={581-586}, 
abstract={Synchronous languages ensure deterministic concurrency, but at the price of heavy restrictions on what programs are considered valid, or constructive. Meanwhile, sequential languages such as C and Java offer an intuitive, familiar programming paradigm but provide no guarantees with regard to deterministic concurrency. The sequentially constructive model of computation (SC MoC) presented here harnesses the synchronous execution model to achieve deterministic concurrency while addressing concerns that synchronous languages are unnecessarily restrictive and difficult to adopt. In essence, the SC MoC extends the classical synchronous MoC by allowing variables to be read and written in any order as long as sequentiality expressed in the program provides sufficient scheduling information to rule out race conditions. The SC MoC is a conservative extension in that programs considered constructive in the common synchronous MoC are also SC and retain the same semantics. In this paper, we identify classes of variable accesses, define sequential constructiveness based on the concept of SC-admissible scheduling, and present a priority-based scheduling algorithm for analyzing and compiling SC programs.}, 
keywords={Computational modeling;Concurrent computing;Electronic mail;Instruction sets;Java;Programming;Schedules}, 
doi={10.7873/DATE.2013.128}, 
ISSN={1530-1591}, 
month={March},}
@INPROCEEDINGS{4340471, 
author={M. Yao and X. Zhu}, 
booktitle={2007 International Conference on Wireless Communications, Networking and Mobile Computing}, 
title={Study and Transplant of Operating System for Wireless Sensor Network Node}, 
year={2007}, 
pages={2803-2807}, 
abstract={This paper studies the embedded operating system TinyOS, including its event-driven mechanism, schedule strategy mechanics, power management mechanism, and etc. Then, the component-based architecture of TinyOS is analyzed. Based on these, the transplant scheme, the design principle of the layer of hardware description and the selection principle of processor are provided. Finally, this paper gives some conclusions and foresight.}, 
keywords={embedded systems;network operating systems;power aware computing;scheduling;wireless sensor networks;TinyOS embedded operating system transplant scheme;component-based architecture;event-driven mechanism;hardware description layer design principle;power management mechanism;processor selection principle;schedule strategy mechanics;wireless sensor network node;Application software;Batteries;Communication system control;Data processing;Energy management;Hardware;Operating systems;Power supplies;Power system management;Wireless sensor networks}, 
doi={10.1109/WICOM.2007.696}, 
ISSN={2161-9646}, 
month={Sept},}
@INPROCEEDINGS{5751519, 
author={C. Brooks and E. A. Lee and S. Tripakis}, 
booktitle={2010 IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)}, 
title={Exploring models of computation with Ptolemy II}, 
year={2010}, 
pages={331-332}, 
abstract={The Ptolemy project studies modeling, simulation, and design of concurrent, real-time, embedded systems. The focus is on assembly of concurrent components. The key underlying principle in the project is the use of well-defined models of computation that govern the interaction between components. A major problem area being addressed is the use of heterogeneous mixtures of models of computation. Ptolemy II takes a component view of design, in that models are constructed as a set of interacting components. A model of computation governs the semantics of the interaction, and thus imposes an execution-time discipline. Ptolemy II has implementations of many models of computation including Synchronous Data Flow, Kahn Process Networks, Discrete Event, Continuous Time, Synchronous/Reactive and Modal Models This hands-on tutorial explores how these models of computation are implemented in Ptolemy II and how to create new models of computation such as a "non-dogmatic" Process Networks example and a left-to-right execution policy example.}, 
keywords={embedded systems;object-oriented methods;public domain software;Kahn process networks;Ptolemy II;concurrent embedded systems;continuous time models;discrete event models;execution time discipline;left-to-right execution policy;modal models;non dogmatic process networks;real time embedded systems;synchronous data flow;synchronous-reactive models;Computational modeling;Data models;Object oriented modeling;Real time systems;Semantics;Software;Tutorials;Modeling;concurrency;simulation}, 
doi={10.1145/1878961.1879020}, 
month={Oct},}
@ARTICLE{4135373, 
author={H. D. Patel and S. K. Shukla and R. A. Bergamaschi}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={Heterogeneous Behavioral Hierarchy Extensions for SystemC}, 
year={2007}, 
volume={26}, 
number={4}, 
pages={765-780}, 
abstract={System level design methodology and language support for high-level modeling enhances productivity for designing complex embedded systems. For an effective methodology, efficiency of simulation and a sound refinement-based implementation path are also necessary. Although some of the recent system level design languages (SLDLs) such as SystemC, SystemVerilog, or SpecC have features for system level abstractions, several essential ingredients are missing from these. We consider: 1) explicit support for multiple models of computation (MoCs) or heterogeneity so that distributed reactive embedded systems with hardware and software components can be easily modeled; 2) the ability to build complex behaviors by hierarchically composing simpler behaviors and the ability to distinguish between structural and heterogeneous behavioral hierarchy; and 3) hierarchical composition of behaviors that belong to distinct MoCs, as essential for successful SLDLs. One important requirement for such an SLDL should be that the simulation semantics are compositional, and hence no flattening of hierarchically composed behaviors are needed for simulation. In this paper, we show how we designed SystemC extensions to facilitates for heterogeneous behavioral hierarchy, compositional simulation semantics, and a simulation kernel that shows up to 40% more efficient than standard SystemC simulation}, 
keywords={circuit simulation;embedded systems;high level synthesis;logic design;specification languages;SystemC;behavioral decomposition;compositional simulation semantics;distributed reactive embedded systems;heterogeneous behavioral hierarchy;hierarchical finite state machine;hierarchical synchronous data flow;high-level modeling;language support;multiple models of computation;simulation kernel;structural modeling;system level abstractions;system level design;Circuit simulation;Computational modeling;Design methodology;Embedded computing;Embedded software;Embedded system;Hardware design languages;Productivity;Software systems;System-level design;Behavioral decomposition;SystemC;behavioral modeling;embedded system design;heterogeneous behavioral hierarchy;hierarchical finite state machine (HFSM);hierarchical synchronous data flow (SDF);models of computation (MoCs);simulation efficiency;structural modeling;system level designs}, 
doi={10.1109/TCAD.2006.884859}, 
ISSN={0278-0070}, 
month={April},}
@INPROCEEDINGS{7336309, 
author={W. B. Gardner and A. Gumtie and J. D. Carter}, 
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, 
title={Supporting Selective Formalism in CSP++ with Process-Specific Storage}, 
year={2015}, 
pages={1057-1065}, 
abstract={Communicating Sequential Processes (CSP) is a formal language whose primary purpose is to model and verify concurrent systems. The CSP++ toolset was created to realize the concept of selective formalism by making machine-readable CSPm specifications both executable (through automatic C++ code generation) and extensible (by allowing integration of C++ user-coded functions, UCFs). However, UCFs were limited by their inability to share data with each other, thus their application was constrained to solving simple problems in isolation. We extend CSP++ by providing UCFs in the same CSP process with safe access to a shared storage area, similar in concept and API to Pthreads' thread-local storage, enabling cooperation between them and granting them the ability to undertake more complex tasks without breaking the formalism of the underlying specification. Process-specific storage is demonstrated with a line-following robot case study, applying CSP++ in a soft real-time system. Also described is the Eclipse plug-in that supports the CSPm design flow.}, 
keywords={C++ language;application program interfaces;communicating sequential processes;concurrency (computers);control engineering computing;formal languages;formal specification;formal verification;program compilers;real-time systems;robots;storage management;API;C++ user-coded function;CSP++;CSPm design flow;Eclipse plug-in;Pthread thread-local storage;UCF;automatic C++ code generation;concurrent system modelling;concurrent system verification;formal language;line-following robot case study;machine-readable CSPm specification;process-specific storage;selective formalism;soft real-time system;Libraries;Real-time systems;Robot sensing systems;Switches;System recovery;Writing;C++;CSPm;Eclipse;Timed CSP;code generation;embedded systems;formal methods;model-based design;selective formalism;soft real-time;software synthesis}, 
doi={10.1109/HPCC-CSS-ICESS.2015.265}, 
month={Aug},}
@ARTICLE{6463378, 
author={S. Andalam and P. S. Roop and A. Girault and C. Traulsen}, 
journal={IEEE Transactions on Computers}, 
title={A Predictable Framework for Safety-Critical Embedded Systems}, 
year={2014}, 
volume={63}, 
number={7}, 
pages={1600-1612}, 
abstract={Safety-critical embedded systems, commonly found in automotive, space, and health-care, are highly reactive and concurrent. Their most important characteristics are that they require both functional and timing correctness. C has been the language of choice for programming such systems. However, C lacks many features that can make the design process of such systems seamless while also maintaining predictability. This paper addresses the need for a C-based design framework for achieving time predictability. To this end, we propose the PRET-C language and the ARPRET architecture. PRET-C offers a small set of extensions to a subset of C to facilitate effective concurrent programming. We present a new synchronous semantics for PRET-C. It guarantees that all PRET-C programs are deterministic, reactive, and provides thread-safe communication via shared memory access. This simplifies considerably the design of safety-critical systems. We also present the architecture of a precision timed machine (PRET) called ARPRET. It offers the ability to design time predictable architectures through simple customizations of soft-core processors. We have designed ARPRET particularly for efficient and predictable execution of PRET-C. We demonstrate through extensive benchmarking that PRET-C based system design excels in comparison to existing C-based paradigms. We also qualitatively compare our approach to the Berkeley-Columbia PRET approach. We have demonstrated that the proposed approach provides an ideal framework for designing and validating safety-critical embedded systems.}, 
keywords={C language;embedded systems;multi-threading;programming language semantics;safety-critical software;shared memory systems;ARPRET;ARPRET architecture;Berkeley-Columbia PRET approach;C-based design framework;PRET-C language;PRET-C programs;Precision Timed C language;architecture of a precision timed machine;concurrent programming;deterministic programs;lightweight multithreaded language;reactive programs;safety-critical embedded systems;shared memory access;soft-core processors;synchronous semantics;thread-safe communication;time predictable architectures;Computer architecture;Instruction sets;Programming;Real-time systems;Semantics;Timing;PRET;PRET-C;Safety-critical systems;WCET;WCRT;synchronous languages;time predictability}, 
doi={10.1109/TC.2013.28}, 
ISSN={0018-9340}, 
month={July},}
@INPROCEEDINGS{4570792, 
author={P. K. Huang and M. Hashemi and S. Ghiasi}, 
booktitle={2008 Symposium on Application Specific Processors}, 
title={System-Level Performance Estimation for Application-Specific MPSoC Interconnect Synthesis}, 
year={2008}, 
pages={95-100}, 
abstract={We present a framework for development of streaming applications as concurrent software modules running on multi-processors system-on-chips (MPSoC). We propose an iterative design space exploration mechanism to customize MPSoC architecture for given applications. Central to the exploration engine is our system-level performance estimation methodology, that both quickly and accurately determine quality of candidate architectures. We implemented a number of streaming applications on candidate architectures that were emulated on an FPGA. Hardware measurements show that our system-level performance estimation method incurs only 15% error in predicting application throughput. More importantly, it always correctly guides design space exploration by achieving 100% fidelity in quality-ranking candidate architectures. Compared to behavioral simulation of compiled code, our system-level estimator runs more than 12 times faster, and requires 7 times less memory.}, 
keywords={field programmable gate arrays;multiprocessing systems;parallel architectures;system-on-chip;FPGA;MPSoC interconnect synthesis;design space exploration;system-level performance estimation;Application software;Computational modeling;Computer architecture;Field programmable gate arrays;Hardware;Network synthesis;Software performance;Space exploration;System-on-a-chip;Throughput}, 
doi={10.1109/SASP.2008.4570792}, 
month={June},}
@ARTICLE{4100760, 
author={S. Pasricha and N. D. Dutt}, 
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
title={A Framework for Cosynthesis of Memory and Communication Architectures for MPSoC}, 
year={2007}, 
volume={26}, 
number={3}, 
pages={408-420}, 
abstract={Memory and communication architectures have a significant impact on the cost, performance, and time-to-market of complex multiprocessor system-on-chip (MPSoC) designs. The memory architecture dictates most of the data traffic flow in a design, which in turn influences the design of the communication architecture. Thus, there is a need to cosynthesize the memory and communication architectures to avoid making suboptimal design decisions. This is in contrast to traditional platform-based design approaches where memory and communication architectures are synthesized separately. In this paper, the authors propose an automated application-specific cosynthesis framework for memory and communication architecture (COSMECA) in MPSoC designs. The primary objective is to design a communication architecture having the least number of buses, which satisfies performance and memory-area constraints, while the secondary objective is to reduce the memory-area cost. Results of applying COSMECA to several industrial strength MPSoC applications from the networking domain indicate a saving of as much as 40% in number of buses and 29% in memory area compared to the traditional approach}, 
keywords={high level synthesis;integrated circuit design;integrated memory circuits;memory architecture;multiprocessing systems;system buses;system-on-chip;COSMECA;automated application-specific cosynthesis framework;communication architectures;complex multiprocessor system-on-chip designs;data traffic flow;memory architectures;Bandwidth;Costs;Digital systems;High level synthesis;Libraries;Memory architecture;Multiprocessing systems;Network synthesis;System performance;Time to market;Communication system performance;digital systems;high-level synthesis;memory architecture}, 
doi={10.1109/TCAD.2006.884487}, 
ISSN={0278-0070}, 
month={March},}
@INPROCEEDINGS{5210958, 
author={F. Oldewurtel and J. Riihijarvi and K. Rerkrai and P. Mahonen}, 
booktitle={2009 Third International Conference on Sensor Technologies and Applications}, 
title={The RUNES Architecture for Reconfigurable Embedded and Sensor Networks}, 
year={2009}, 
pages={109-116}, 
abstract={We present the RUNES architecture for reconfigurable embedded networked systems and wireless sensor networks. It is the first systems-level architecture for such networks to explicitly deal with heterogeneity in hardware platforms, link-layer technologies and networking protocols while offering a simple programming language independent set of APIs together with a component-oriented middleware for the application developers to work on. The solutions developed are particularly appropriate for use in various emergency response scenarios, in which reconfigurability is often a key requirement. We also report on an example realisation of our architecture in a prototypical demonstration environment in a particular emergency scenario. The evaluation of architectural aspects such as reconfigurability shows that great programming flexibility can be achieved at low implementation overhead. The experience gained from RUNES modular architecture are very promising both in academic and industry projects context.}, 
keywords={embedded systems;intelligent sensors;wireless sensor networks;RUNES architecture;reconfigurable embedded networked systems;wireless sensor networks;Actuators;Computer architecture;Hardware;Middleware;Operating systems;Protocols;Sensor systems;Sensor systems and applications;System testing;Wireless sensor networks;architecture;programming model;prototype;sensor networks;software platform}, 
doi={10.1109/SENSORCOMM.2009.26}, 
month={June},}
@INPROCEEDINGS{7774446, 
author={J. O. Ooi and F. A. B. Hussin and N. Zakaria}, 
booktitle={2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)}, 
title={Dual-Engine Cross-ISA DBTO Technique Utilising MultiThreaded Support for Multicore Processor System}, 
year={2016}, 
pages={257-264}, 
abstract={The emergence of new era of Internet of Things or IoT have encouraged intensive if not extensive usage of modern mobile apps, thus multi-ISA equipped multicore processor gain great potential to be used for more efficient instruction binary processing in near future. In order to support this ISA diversity of computing platforms, mix modes of statically and dynamically Binary Translation and Optimization system, popularly consists of QEMU and LLVM or similar system, is the default technique used. However this complex system exhibits heavy slowdown (60x slowdown as compare to generic QEMU) [21] which impede its performance especially for short running application codes, typically used in IoT based apps applications. This research introduce a dual binary code translation engines to support apps based and kernel based application codes, through utilising multithreaded supported apart of original single thread supported binary translation processing in run-time. The dual engine consists of TCG generator from QEMU, and LLVM which include rich optimisations library. The evaluation through PARSEC-3.0 Benchmark shows our Hybrid DBTO system achieved performance improvement approaching 2.0x for apps based programs and 1.25x for kernel based programs, for x86 to X86-64 emulation. This technique possess great potential and serve as research based platform for future binary translation technique development, including adaptive method.}, 
keywords={Internet of Things;binary codes;mobile computing;multi-threading;multiprocessing systems;Internet of Things;IoT;LLVM;PARSEC-3.0 benchmark;QEMU;TCG generator;X86-64 emulation;apps based application codes;dual binary code translation engines;dual-engine cross-ISA DBTO technique;dynamically binary translation and optimization system;hybrid DBTO system;instruction binary processing;kernel based application codes;mobile apps;multiISA equipped multicore processor;multicore processor system;multithreaded support;optimisations library;short running application codes;statically binary translation and optimization system;Emulation;Instruction sets;Kernel;Multicore processing;Optimization;Registers;Virtual machining;Binary Optimization;Binary Translation;Multi-ISA processor;Multicores;Multitheraded}, 
doi={10.1109/MCSoC.2016.36}, 
month={Sept},}
@INPROCEEDINGS{5678449, 
author={T. Riedel and N. Fantana and A. Genaid and D. Yordanov and H. R. Schmidtke and M. Beigl}, 
booktitle={2010 Internet of Things (IOT)}, 
title={Using web service gateways and code generation for sustainable IoT system development}, 
year={2010}, 
pages={1-8}, 
abstract={Wireless Sensing and Radio Identification systems have undergone many innovations during the past years. This has led to short product lifetimes for both software and hardware compared to classical industries. However, especially industries dealing with long-term support of products, e.g. of industrial machinery, and product lifetime of 40+ years may especially profit from an Internet of Things. Motivated by a practical industrial servicing use case this paper shows how we hope to make equally sustainable IoT solutions by employing a model driven software development approach based on code generation for multi-protocol web service gateways.}, 
keywords={Web services;internetworking;program compilers;protocols;software engineering;sustainable development;Internet of things;IoT system;Web service gateways;code generation;industrial servicing;multi-protocol;radio identification systems;software development approach;sustainable development;Automata;Logic gates;Radiofrequency identification;Semantics;Unified modeling language;Web services;XML}, 
doi={10.1109/IOT.2010.5678449}, 
month={Nov},}
@INPROCEEDINGS{7460722, 
author={M. P. Andersen and G. Fierro and D. E. Culler}, 
booktitle={2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)}, 
title={System Design for a Synergistic, Low Power Mote/BLE Embedded Platform}, 
year={2016}, 
pages={1-12}, 
abstract={Modern IoT prototyping platforms fall short in terms of energy efficiency, connectivity and software programming practices. We present the design of a new hardware and software platform that addresses these shortcomings by bringing together Mobile, Wearable, Maker and Wireless Sensor Network technologies to enable rapid prototyping with a high degree of synergy and energy efficiency. This is achieved in part by leveraging the Memory Protection Unit on modern microcontrollers along with a novel syscall interface to provide kernel / user isolation and a clean concurrency model. Such a design allows a wide range of languages to be used for application development without significant adaptation. We demonstrate how careful choice of application language allows the naturally asynchronous nature of embedded programming to be expressed cleanly and powerfully. Finally we evaluate the platform in several integrated use cases, providing examples of the capabilities introduced by Synergy.}, 
keywords={concurrency (computers);embedded systems;microcontrollers;clean concurrency model;embedded programming;low power mote-BLE embedded platform;memory protection unit;microcontrollers;mobile network technologies;rapid prototyping;syscall interface;wearable technologies;wireless sensor network technologies;Hardware;IEEE 802.15 Standard;Microcontrollers;Software;Storms;Technological innovation;Wireless sensor networks}, 
doi={10.1109/IPSN.2016.7460722}, 
month={April},}
@INPROCEEDINGS{7818327, 
author={G. Cedersj√∂ and J. W. Janneck}, 
booktitle={2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)}, 
title={Processes and actors: Translating Kahn processes to dataflow with firing}, 
year={2016}, 
pages={21-30}, 
abstract={Dataflow programming is a paradigm for describing stream processing algorithms in a manner that naturally exposes their concurrency and makes the resulting programs readily implementable on highly parallel architectures. Dataflow programs are graph structured, with nodes representing computational kernels that process the data flowing over the edges. There are two major families of languages for the kernels: process languages and languages for dataflow with firing. While processes tend to be easier to write, the additional structure provided by the dataflow-with-firing style increases the analyzability of dataflow programs and supports more efficient implementation techniques. This paper seeks to combine these benefits in a principled manner by constructing a family of translations from a process language to dataflow with firing. In order to formally relate these descriptions, we first introduce a notion of firing to the semantics of Kahn processes, which allows us to give a precise definition of equivalence between programs written in these different styles. Then we introduce a family of translations between them and and show that they retain meaning of a program. The presented language and its translation has been implemented in a compiler for the dataflow programming language CAL.}, 
keywords={concurrency control;data flow computing;parallel architectures;program compilers;CAL;Kahn process translation;compiler;computational kernels;dataflow programming language;dataflow-with-firing style;parallel architectures;process languages;stream processing algorithms;Computational modeling;Computer languages;Grammar;Ports (Computers);Programming;Semantics;Structural rings}, 
doi={10.1109/SAMOS.2016.7818327}, 
month={July},}
@INPROCEEDINGS{4228186, 
author={M. Cohen and T. Ponte and S. Rossetto and N. Rodriguez}, 
booktitle={2007 IEEE International Parallel and Distributed Processing Symposium}, 
title={Using Coroutines for RPC in Sensor Networks}, 
year={2007}, 
pages={1-8}, 
abstract={This paper proposes a concurrency model which integrates the asynchronous and event-driven nature of wireless sensor networks with higher-level abstractions that provide a more familiar programming style for the developer. As a basis for this proposal, we designed and implemented a cooperative multitasking scheduler, based on coroutines, for the TinyOS operating system. We then used this scheduler to implement RPC-like interfaces that capture different communication patterns common in wireless sensor networks. This allows the programmer to work, when appropriate, with a synchronous style, while maintaining an asynchronous model at the message exchange level.}, 
keywords={concurrency control;network operating systems;remote procedure calls;scheduling;wireless sensor networks;RPC coroutine;RPC-like interface;TinyOS operating system;concurrency model;cooperative multitasking scheduler;event-driven wireless sensor network;Computer languages;Concurrent computing;Embedded system;Multitasking;Operating systems;Programming profession;Proposals;Sensor phenomena and characterization;Testing;Wireless sensor networks}, 
doi={10.1109/IPDPS.2007.370458}, 
ISSN={1530-2075}, 
month={March},}
@ARTICLE{5492692, 
author={A. Bergel and W. Harrison and V. Cahill and S. Clarke}, 
journal={IEEE Transactions on Software Engineering}, 
title={FlowTalk: Language Support for Long-Latency Operations in Embedded Devices}, 
year={2011}, 
volume={37}, 
number={4}, 
pages={526-543}, 
abstract={Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.}, 
keywords={C language;embedded systems;intelligent sensors;object-oriented languages;object-oriented programming;software engineering;wireless sensor networks;C language;FlowTalk;TinyOS;asynchronous long-latency operations;embedded sensor devices;language support;light-weight continuation mechanism;memory consumption;memory size 4 KByte;object-oriented programming language;programming language;sensor sampling;wireless sensor networks;Application software;Automotive engineering;Biosensors;Computer languages;Embedded software;Java;Object oriented modeling;Object oriented programming;Sampling methods;Wireless sensor networks;Embedded systems;object-based programming.}, 
doi={10.1109/TSE.2010.66}, 
ISSN={0098-5589}, 
month={July},}
@INPROCEEDINGS{4624024, 
author={D. G. Kim and S. M. Lee and D. R. Shin}, 
booktitle={2008 Fourth International Conference on Networked Computing and Advanced Information Management}, 
title={Design of the Operating System Virtualization on L4 Microkernel}, 
year={2008}, 
volume={1}, 
pages={307-310}, 
abstract={The importance of the virtualization in embedded computing area is currently emerging. The virtualization can enhance system flexibility by enabling the concurrent execution of an application OS and a real-time OS (RTOS) on the same processor. L4 microkernel can be used as an efficient hypervisor which provides environment for operating systems virtualization. In order to run the application OSes on L4 microkernel, the application OSes should be adapted. The source code of Linux kernel can be readily accessed and modified. Hence, the Linux kernel is chosen as virtualized operating systems. In this paper, the architecture for virtualization of Linux kernel which is based on L4 microkernel is proposed.}, 
keywords={Linux;operating system kernels;virtual machines;L4 microkernel;Linux kernel virtualization;concurrent execution;embedded computing;hypervisor;operating system virtualization;real-time operating system;system flexibility;Application software;Application virtualization;Embedded computing;Kernel;Linux;Operating systems;Platform virtualization;Real time systems;Virtual machine monitors;Yarn}, 
doi={10.1109/NCM.2008.165}, 
month={Sept},}
@ARTICLE{6393004, 
journal={IEEE Solid-State Circuits Magazine}, 
title={CEDA Currents [IEEE News]}, 
year={2012}, 
volume={4}, 
number={4}, 
pages={62-63}, 
abstract={}, 
doi={10.1109/MSSC.2012.2214295}, 
ISSN={1943-0582}, 
month={Dec},}
@INPROCEEDINGS{6270634, 
author={Q. Wu and C. Yang and F. Wang and J. Xue}, 
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum}, 
title={A Fast Parallel Implementation of Molecular Dynamics with the Morse Potential on a Heterogeneous Petascale Supercomputer}, 
year={2012}, 
pages={140-149}, 
abstract={Molecular Dynamics (MD) simulations have been widely used in the study of macromolecules. To ensure an acceptable level of statistical accuracy relatively large number of particles are needed, which calls for high performance implementations of MD. These days heterogeneous systems, with their high performance potential, low power consumption, and high price-performance ratio, offer a viable alternative for running MD simulations. In this paper we introduce a fast parallel implementation of MD simulation with the Morse potential on Tianhe-1A, a petascale heterogeneous supercomputer. Our code achieves a speedup of 3.6√ó on one NVIDIA Tesla M2050 GPU (containing 14 Streaming Multiprocessors) compared to a 2.93GHz six-core Intel Xeon X5670 CPU. In addition, our code runs faster on 1024 compute nodes (with two CPUs and one GPU inside a node) than on 4096 GPU-excluded nodes, effectively rendering one GPU more efficient than six six-core CPUs. Our work shows that large-scale MD simulations can benefit enormously from GPU acceleration in petascale supercomputing platforms. Our performance results are achieved by using (1) a patch-cell design to exploit parallelism across the simulation domain, (2) a new GPU kernel developed by taking advantage of Newton's Third Law to reduce redundant force computation on GPUs, (3) two optimization methods including a dynamic load balancing strategy that adjusts the workload, and a communication overlapping method to overlap the communications between CPUs and GPUs.}, 
keywords={Morse potential;chemistry computing;graphics processing units;macromolecules;molecular dynamics method;parallel machines;resource allocation;statistical analysis;GPU acceleration;GPU kernel;GPU-excluded node;MD simulation;Morse potential;NVIDIA Tesla M2050 GPU;Newton Third Law;Tianhe-1A;communication overlapping method;dynamic load balancing strategy;fast parallel implementation;heterogeneous petascale supercomputer;heterogeneous systems;macromolecules;molecular dynamics simulation;optimization method;parallelism;patch-cell design;petascale supercomputing platform;redundant force computation;six-core Intel Xeon X5670 CPU;statistical accuracy;streaming multiprocessors;workload adjustment;Computational modeling;Force;Graphics processing unit;Indexes;Instruction sets;Kernel;Mathematical model;GPU computing;Molecular Dynamics;heterogeneous computing;petascale supercomputer}, 
doi={10.1109/IPDPSW.2012.13}, 
month={May},}
@INPROCEEDINGS{6269627, 
author={J. F. Ferreira and G. He and S. Qin}, 
booktitle={2012 Sixth International Symposium on Theoretical Aspects of Software Engineering}, 
title={Automated Verification of the FreeRTOS Scheduler in HIP/SLEEK}, 
year={2012}, 
pages={51-58}, 
abstract={Automated verification of operating system kernels is a challenging problem, partly due to the use of shared mutable data structures. In this paper, we show how we can automatically verify memory safety and functional correctness of the task scheduler component of the FreeRTOS kernel using the verification system HIP/SLEEK. We show how some of HIP/SLEEK features like user-defined predicates and lemmas make the specifications highly expressive and the verification process viable. To the best of our knowledge, this is the first code-level verification of memory safety and functional correctness properties of the FreeRTOS scheduler. The outcome of our experiment confirms that HIP/SLEEK can indeed be used to verify code that is used in production. Moreover, since the properties that we verify are quite general, we envisage that the same approach can be adopted to verify the scheduler of other operating systems.}, 
keywords={data structures;formal verification;operating system kernels;scheduling;shared memory systems;FreeRTOS kernel;FreeRTOS scheduler;HIP-SLEEK verification system;automated operating system kernel verification;first code-level verification;functional correctness verification;memory safety verification;real-time operating systems;shared mutable data structures;task scheduler component;user-defined lemmas;user-defined predicates;Context;Data structures;Hip;Kernel;Safety;Shape;FreeRTOS;HIP/SLEEK;automated verification;embedded systems;operating systems;separation logic;task scheduler}, 
doi={10.1109/TASE.2012.45}, 
month={July},}
@INPROCEEDINGS{7001431, 
author={M. Tan and B. Liu and S. Dai and Z. Zhang}, 
booktitle={2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
title={Multithreaded pipeline synthesis for data-parallel kernels}, 
year={2014}, 
pages={718-725}, 
abstract={Pipelining is an important technique in high-level synthesis, which overlaps the execution of successive loop iterations or threads to achieve high throughput for loop/function kernels. Since existing pipelining techniques typically enforce in-order thread execution, a variable-latency operation in one thread would block all subsequent threads, resulting in considerable performance degradation. In this paper, we propose a multithreaded pipelining approach that enables context switching to allow out-of-order thread execution for data-parallel kernels. To ensure that the synthesized pipeline is complexity effective, we further propose efficient scheduling algorithms for minimizing the hardware overhead associated with context management. Experimental results show that our proposed techniques can significantly improve the effective pipeline throughput over conventional approaches while conserving hardware resources.}, 
keywords={multi-threading;pipeline processing;processor scheduling;context management;context switching;data-parallel kernels;hardware overhead;hardware resources;high-level synthesis;in-order thread execution;loop iterations;loop/function kernels;multithreaded pipeline synthesis;out-of-order thread execution;pipeline throughput;pipelining techniques;scheduling algorithms;variable-latency operation;Context;Instruction sets;Kernel;Pipeline processing;Schedules;Switches;Throughput}, 
doi={10.1109/ICCAD.2014.7001431}, 
ISSN={1092-3152}, 
month={Nov},}
@INPROCEEDINGS{6108279, 
author={C. Chise and I. Jurca}, 
booktitle={2011 10th International Symposium on Parallel and Distributed Computing}, 
title={Hybrid Analytical-Simulation Model Used to Evaluate and Improve System Performance}, 
year={2011}, 
pages={240-246}, 
abstract={Performance prediction has been intensively studied in the last decade, alongside the accelerated development of distributed systems. This paper focuses on a hybrid approach regarding model solving, combining two popular prediction techniques applied separately so far, analytical and simulation modeling, in order to benefit from the strengths of both. The input UML model with MARTE (Modeling and Analysis of Real-time and Embedded systems) annotations is transformed into a hierarchically decomposed performance model, and performance results for simulated sub models are used by an analytical solver. The validation of the proposed method is to be performed with a tool called PHYMSS (Performance Hybrid Model Solver and Simulator) developed by the authors that implements both the hybrid solver and a multithreaded simulator.}, 
keywords={Unified Modeling Language;distributed processing;performance evaluation;MARTE;PHYMSS;distributed systems;hybrid analytical-simulation model;input UML model;system performance;Analytical models;Computational modeling;Mathematical model;Predictive models;Servers;Throughput;Unified modeling language;LQN;hybrid model;simulation}, 
doi={10.1109/ISPDC.2011.42}, 
ISSN={2379-5352}, 
month={July},}
@ARTICLE{7387670, 
author={M. Amjad and M. Sharif and M. K. Afzal and S. W. Kim}, 
journal={IEEE Sensors Journal}, 
title={TinyOS-New Trends, Comparative Views, and Supported Sensing Applications: A Review}, 
year={2016}, 
volume={16}, 
number={9}, 
pages={2865-2889}, 
abstract={The wireless sensor network (WSN) is an interesting area for modern day research groups. Tiny sensor nodes are deployed in a diversity of environments but with limited resources. Scarce resources compel researchers to employ an operating system that requires limited memory and minimum power. Tiny operating system (TinyOS) is a widely used operating system for sensor nodes, which provides concurrency and flexibility while adhering to the constraints of scarce resources. Comparatively, TinyOS is considered to be the most robust, innovative, energy-efficient, and widely used operating system in sensor networks. This paper looks at the state-of-the-art TinyOS and the different dimensions of its design paradigm, programming model, execution model, scheduling algorithms, concurrency, memory management, hardware support platforms, and other features. The addition of different features in TinyOS makes it the operating system of choice for WSNs. Sensing nodes with TinyOS seem to show more flexibility in supporting diverse types of sensing applications.}, 
keywords={systems software;wireless sensor networks;Tiny operating system;concurrency;design paradigm;execution model;hardware support platforms;memory management;programming model;scheduling algorithms;sensor nodes;wireless sensor network;Adaptation models;Concurrent computing;Instruction sets;Operating systems;Programming;Sensors;Wireless sensor networks;Wireless sensor networks;energy efficiency;operating system;sensor nodes},
doi={10.1109/JSEN.2016.2519924}, 
ISSN={1530-437X}, 
month={May},}
@INPROCEEDINGS{7173939, 
author={M. H√∂lzl and T. Gabor}, 
booktitle={2015 IEEE/ACM 1st International Workshop on Software Engineering for Smart Cyber-Physical Systems}, 
title={Continuous Collaboration: A Case Study on the Development of an Adaptive Cyber-physical System}, 
year={2015}, 
pages={19-25}, 
abstract={The need to interact with complex environments that are often not well understood at design time makes the development of smart cyber-physical systems (sCPS) a challenging endeavor. We propose a set of practices and tools that support the design and implementation of sCPS using continuous collaboration -- a development lifecycle and architecture to continuously incorporate data gained from the operation of the sCPS into the system. Continuous collaboration attempts to harmonize three interlocking feedback cycles: refinement of the system design by the developers, autonomous evolution of agents in the sCPS, and feedback from the evolving system to the developers. To support the process we introduce tools and techniques that we have found helpful to realize continuous collaboration: The HADES/Hexameter platform, extended behavior trees and the teacher/student learning pattern.}, 
keywords={software architecture;HADES-Hexameter platform;adaptive cyber-physical system;complex environments;continuous collaboration;extended behavior trees;interlocking feedback cycles;sCPS;software architecture;software development lifecycle approach;system design;teacher-student learning pattern;Collaboration;Cyber-physical systems;Hardware;Navigation;Rescue robots;Software;Cyber-Physical Systems;Embodied Evolution;Evolutionary Algorithms;Reinforcement Learning;Software Engineering}, 
doi={10.1109/SEsCPS.2015.12}, 
month={May},}
@ARTICLE{6733370, 
author={W. B. Langdon and M. Harman}, 
journal={IEEE Transactions on Evolutionary Computation}, 
title={Optimizing Existing Software With Genetic Programming}, 
year={2015}, 
volume={19}, 
number={1}, 
pages={118-135}, 
abstract={We show that the genetic improvement of programs (GIP) can scale by evolving increased performance in a widely-used and highly complex 50000 line system. Genetic improvement of software for multiple objective exploration (GISMOE) found code that is 70 times faster (on average) and yet is at least as good functionally. Indeed, it even gives a small semantic gain.}, 
keywords={genetic algorithms;software engineering;GIP;GISMOE;genetic improvement of programs;genetic improvement of software for multiple objective exploration;genetic programming;software optimization;Complexity theory;DNA;Genetic programming;Grammar;Semantics;Software;${\rm Bowtie2}^{GP}$;Automatic software reengineering;genetic programming (GP);multiple objective exploration;search based software engineering (SBSE)}, 
doi={10.1109/TEVC.2013.2281544}, 
ISSN={1089-778X}, 
month={Feb},}
@INPROCEEDINGS{4292873, 
author={M. Karpinski and V. Cahill}, 
booktitle={2007 4th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks}, 
title={High-Level Application Development is Realistic for Wireless Sensor Networks}, 
year={2007}, 
pages={610-619}, 
abstract={Programming wireless sensor network (WSN) applications is known to be a difficult task. Part of the problem is that the resource limitations of typical WSN nodes force programmers to use relatively low-level techniques to deal with the logical concurrency and asynchronous event handling inherent in these applications. In addition, existing general-purpose, node-level programming tools only support the networked nature of WSN applications in a limited way and result in application code that is hardly portable across different software platforms. All of this makes programming a single device a tedious and error-prone task. To address these issues we propose a high-level programming model that allows programmers to express applications as hierarchical state machines and to handle events and application concurrency in a way similar to imperative synchronous languages. Our program execution model is based on static scheduling what allows for standalone application analysis and testing. For deployment, the resulting programs are translated into efficient sequential C code. A prototype compiler for TinyOS has been implemented and its evaluation in described in this paper.}, 
keywords={C++ language;wireless sensor networks;TinyOS;asynchronous event handling;hierarchical state machines;high-level application development;high-level programming model;logical concurrency;node-level programming tools;nodes force programmers;resource limitations;sequential C code;software platforms;synchronous languages;wireless sensor networks;Application software;Computer languages;Computer science;Concurrent computing;Hardware;Operating systems;Peer to peer computing;Programming profession;Prototypes;Wireless sensor networks}, 
doi={10.1109/SAHCN.2007.4292873}, 
ISSN={2155-5486}, 
month={June},}
@ARTICLE{6403642, 
author={M. Hosseinabady and J. l. Nunez-Yanez}, 
journal={IET Computers Digital Techniques}, 
title={Fast and low overhead architectural transaction level modelling for large-scale network-on-chip simulation}, 
year={2012}, 
volume={6}, 
number={6}, 
pages={384-395}, 
abstract={Early system modelling is an essential tool to accelerate software development, architectural analysis and hardware verification in complex many-core system-on-chips (SoCs). Transaction level modelling (TLM) offers a higher level of abstraction than register transfer level (RTL) and can be used for early system modelling. Maintaining simulation speed with the right accuracy is a major challenge and this paper proposes SystemC-based architectural modelling techniques that extend TLM to deliver faster simulation models for many-core system. The proposed approach considers a micro-scheduler for large modules (in the sense of SystemC modules) to locally manage all events in the module. Exploiting this micro-scheduler along with function object and coroutine concepts, the authors propose a lightweight thread process that significantly reduces the context switching overhead among the different processes. Additionally the micro-scheduler allows some processes to be run ahead of simulation time. The proposed techniques are applied to the model of a very large networks-on-chip (NoC) formed by thousands of cores stressing the simulation capabilities of the host computer and operating system. The experimental results demonstrate that the model can run successfully and exhibits up to 93% improvement in simulation speed compared to traditional SystemC-based modelling.}, 
keywords={electronic engineering computing;integrated circuit modelling;network-on-chip;operating systems (computers);software engineering;system-on-chip;transaction processing;RTL;SystemC-based architectural modelling techniques;architectural analysis;coroutine concepts;function object concepts;hardware verification;host computer;large-scale NoC;large-scale network-on-chip simulation;low overhead architectural transaction level modelling;many-core SoC;many-core system-on-chips;microscheduler;operating system;register transfer level;software development}, 
doi={10.1049/iet-cdt.2012.0001}, 
ISSN={1751-8601}, 
month={November},}
@INPROCEEDINGS{6651056, 
author={J. D. Leidel and J. Bolding and G. Rogers}, 
booktitle={2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum}, 
title={Toward a Scalable Heterogeneous Runtime System for the Convey MX Architecture}, 
year={2013}, 
pages={1597-1606}, 
abstract={Given the recent advent of the multicore era [1], research efforts in the area of high performance, low latency runtime systems have increased significantly. This research has given birth to new techniques in low-overhead scheduling techniques, small-memory footprint parallel execution units and kernel-free contextual environments. This paper presents a framework and runtime system for a truly heterogeneous approach to low-latency, high performance runtime techniques on the Convey MX-100 platform and CHOMP micro-architecture [14]. This framework, deemed the Convey Lightweight Runtime [CLR], is designed to provide high performance, programming-model agnostic parallel library support to the massively parallel CHOMP infrastructure. This work explores the fundamental design requirements and implementation details behind constructing the CLR system as a truly heterogeneous low-level runtime system for a wide array of parallel programming model targets.}, 
keywords={multiprocessing systems;parallel programming;scheduling;CHOMP micro-architecture;CLR system;convey MX architecture;convey lightweight runtime;kernel-free contextual environments;low-overhead scheduling techniques;multicore era;parallel CHOMP infrastructure;parallel programming model;scalable heterogeneous runtime system;small-memory footprint parallel execution units;Computer architecture;Coprocessors;Hardware;Instruction sets;Programming;Registers;Runtime;architecture;heterogeneous;instruction set architecture;multithreading;runtime;scheduling;tasking;work stealing}, 
doi={10.1109/IPDPSW.2013.18}, 
month={May},}