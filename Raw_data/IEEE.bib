% Encoding: UTF-8

@Article{7387670,
  author   = {M. Amjad and M. Sharif and M. K. Afzal and S. W. Kim},
  title    = {TinyOS-New Trends, Comparative Views, and Supported Sensing Applications: A Review},
  journal  = {IEEE Sensors Journal},
  year     = {2016},
  volume   = {16},
  number   = {9},
  pages    = {2865-2889},
  month    = {May},
  issn     = {1530-437X},
  abstract = {The wireless sensor network (WSN) is an interesting area for modern day research groups. Tiny sensor nodes are deployed in a diversity of environments but with limited resources. Scarce resources compel researchers to employ an operating system that requires limited memory and minimum power. Tiny operating system (TinyOS) is a widely used operating system for sensor nodes, which provides concurrency and flexibility while adhering to the constraints of scarce resources. Comparatively, TinyOS is considered to be the most robust, innovative, energy-efficient, and widely used operating system in sensor networks. This paper looks at the state-of-the-art TinyOS and the different dimensions of its design paradigm, programming model, execution model, scheduling algorithms, concurrency, memory management, hardware support platforms, and other features. The addition of different features in TinyOS makes it the operating system of choice for WSNs. Sensing nodes with TinyOS seem to show more flexibility in supporting diverse types of sensing applications.},
  db       = {IEEE},
  doi      = {10.1109/JSEN.2016.2519924},
  keywords = {systems software;wireless sensor networks;Tiny operating system;concurrency;design paradigm;execution model;hardware support platforms;memory management;programming model;scheduling algorithms;sensor nodes;wireless sensor network;Adaptation models;Concurrent computing;Instruction sets;Operating systems;Programming;Sensors;Wireless sensor networks;Wireless sensor networks;energy efficiency;operating system;sensor nodes},
}

@InProceedings{7460722,
  author    = {M. P. Andersen and G. Fierro and D. E. Culler},
  title     = {System Design for a Synergistic, Low Power Mote/BLE Embedded Platform},
  booktitle = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
  year      = {2016},
  pages     = {1-12},
  month     = {April},
  abstract  = {Modern IoT prototyping platforms fall short in terms of energy efficiency, connectivity and software programming practices. We present the design of a new hardware and software platform that addresses these shortcomings by bringing together Mobile, Wearable, Maker and Wireless Sensor Network technologies to enable rapid prototyping with a high degree of synergy and energy efficiency. This is achieved in part by leveraging the Memory Protection Unit on modern microcontrollers along with a novel syscall interface to provide kernel / user isolation and a clean concurrency model. Such a design allows a wide range of languages to be used for application development without significant adaptation. We demonstrate how careful choice of application language allows the naturally asynchronous nature of embedded programming to be expressed cleanly and powerfully. Finally we evaluate the platform in several integrated use cases, providing examples of the capabilities introduced by Synergy.},
  db        = {IEEE},
  doi       = {10.1109/IPSN.2016.7460722},
  keywords  = {concurrency (computers);embedded systems;microcontrollers;clean concurrency model;embedded programming;low power mote-BLE embedded platform;memory protection unit;microcontrollers;mobile network technologies;rapid prototyping;syscall interface;wearable technologies;wireless sensor network technologies;Hardware;IEEE 802.15 Standard;Microcontrollers;Software;Storms;Technological innovation;Wireless sensor networks},
}

@InProceedings{7471346,
  author    = {M. P. Andersen and G. Fierro and D. E. Culler},
  title     = {Enabling Synergy in IoT: Platform to Service and Beyond},
  booktitle = {2016 IEEE First International Conference on Internet-of-Things Design and Implementation (IoTDI)},
  year      = {2016},
  pages     = {1-12},
  month     = {April},
  abstract  = {To enable a prosperous Internet of Things, devices and services must be extensible and adapt to changes in the environment or user interaction patterns. These requirements manifest as a set of design principles for each of the layers in an IoT ecosystem, from hardware to cloud services. This paper gives concrete guidelines learned from building a full-stack Synergistic IoT platform.},
  db        = {IEEE},
  doi       = {10.1109/IoTDI.2015.45},
  keywords  = {Internet of Things;wireless sensor networks;Internet of Things;IoT synergy;synergistic IoT platform;user interaction pattern;Complexity theory;Context;Hardware;IEEE 802.15 Standard;Metadata;Protocols;Software;Internet of Things;Sensor motes;Wireless sensor networks},
}

@InProceedings{7383583,
  author    = {Z. Cheng and Y. Li and R. West},
  title     = {Qduino: A Multithreaded Arduino System for Embedded Computing},
  booktitle = {2015 IEEE Real-Time Systems Symposium},
  year      = {2015},
  pages     = {261-272},
  month     = {Dec},
  abstract  = {Arduino is an open source platform that offers a clear and simple environment for physical computing. It is now widely used in modern robotics and Internet of Things (IoT) applications, due in part to its low-cost, ease of programming, and rapid prototyping capabilities. Sensors and actuators can easily be connected to the analog and digital I/O pins of an Arduino device, which features an on-board microcontroller programmed using the Arduino API. The increasing complexity of physical computing applications has now led to a series of Arduino-compatible devices with faster processors, increased flash storage, larger memories and more complicated I/O architectures. The Intel Galileo, for example, is designed to support the Arduino API on top of a Linux system, code-named Clanton. However, the standard API is restricted to the capabilities found on less powerful devices, lacking support for multithreaded programs, or specification of real-time requirements. In this paper, we present Qduino, a system developed for Arduino compatible boards. Qduino provides an extended Arduino API which, while backward-compatible with the original API, supports real-time multithreaded sketches and event handling. Experiments show the performance gains of Qduino compared to Clanton Linux.},
  db        = {IEEE},
  doi       = {10.1109/RTSS.2015.32},
  issn      = {1052-8725},
  keywords  = {Linux;application program interfaces;formal specification;multi-threading;public domain software;real-time systems;Arduino API;Arduino compatible boards;Arduino-compatible devices;Clanton;I/O architectures;Intel Galileo;Internet of Things applications;IoT applications;Linux system;Qduino;Quest real-time operating system;actuators;backward-compatibility;embedded computing;event handling;flash storage;multithreaded Arduino system;multithreaded programs;on-board microcontroller;open source platform;physical computing applications;programming;rapid prototyping capabilities;real-time multithreaded sketches;real-time requirement specification;robotics;sensors;Computer architecture;Hardware;Instruction sets;Kernel;Linux;Real-time systems;Standards;Arduino;embedded systems;multi-threading;real-time},
}

@InProceedings{8016226,
  author    = {A. Elsts and G. Oikonomou and X. Fafoutis and R. Piechocki},
  title     = {Internet of Things for smart homes: Lessons learned from the SPHERE case study},
  booktitle = {2017 Global Internet of Things Summit (GIoTS)},
  year      = {2017},
  pages     = {1-6},
  month     = {June},
  abstract  = {Building large-scale low-power Internet of Things (IoT) systems remains a challenge, as these systems have to meet the requirements of reliability, robustness, and energy-efficiency while running on resource-restricted microcontrollers without memory protection. In this paper we present the case study of IoT in SPHERE (Sensor Platform for HEalthcare in a Residential Environment), a project with the objective to develop a multipurpose, multi-modal sensor platform for monitoring people's health inside their homes. Atypically for academic projects, in 2017 the SPHERE software is going to be deployed in a 100-home study in volunteer homes, therefore it has to satisfy many real-world requirements. We discuss the requirements for IoT networking in this project, the IoT architecture (built on top of Contiki OS), software engineering challenges and lessons learned, as well as some of the general aspects that still make embedded low-power IoT software development difficult.},
  db        = {IEEE},
  doi       = {10.1109/GIOTS.2017.8016226},
  keywords  = {Internet of Things;assisted living;power aware computing;sensors;software engineering;IoT;SPHERE case study;embedded low-power IoT software development;energy-efficiency;large-scale low-power Internet of Things systems;multipurpose multimodal sensor platform;real-world requirements;resource-restricted microcontrollers;sensor platform for healthcare in a residential environment;smart homes;software engineering challenges;volunteer homes;Hardware;IEEE 802.15 Standard;Logic gates;Protocols;Reliability;Servers;Software},
}

@InProceedings{6926617,
  author    = {Ye Jihua and W. Wang},
  title     = {Research and design of solar photovoltaic power generation monitoring system based on TinyOS},
  booktitle = {2014 9th International Conference on Computer Science Education},
  year      = {2014},
  pages     = {1020-1023},
  month     = {Aug},
  abstract  = {In this paper, in order to solve management problems and field maintenance difficult issues existing in the process of PV power generation, we have designed a remote intelligent monitoring system based on TinyOS for monitoring and management. This system had implemented remote monitoring and reverse control by host computer, ARM gateways, wireless sensor networks and other components.},
  db        = {IEEE},
  doi       = {10.1109/ICCSE.2014.6926617},
  keywords  = {microcontrollers;photovoltaic power systems;power system measurement;wireless sensor networks;ARM gateways;PV power generation;TinyOS;field maintenance;host computer;management problems;remote intelligent monitoring system;remote monitoring;reverse control;wireless sensor networks;Computers;Data analysis;Logic gates;Monitoring;Routing;Wireless communication;Wireless sensor networks;IOT;TinyOS;node;remote monitoring},
}

@Article{6472115,
  author   = {M. T. Lazarescu},
  title    = {Design of a WSN Platform for Long-Term Environmental Monitoring for IoT Applications},
  journal  = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  year     = {2013},
  volume   = {3},
  number   = {1},
  pages    = {45-54},
  month    = {March},
  issn     = {2156-3357},
  abstract = {The Internet of Things (IoT) provides a virtual view, via the Internet Protocol, to a huge variety of real life objects, ranging from a car, to a teacup, to a building, to trees in a forest. Its appeal is the ubiquitous generalized access to the status and location of any “thing” we may be interested in. Wireless sensor networks (WSN) are well suited for long-term environmental data acquisition for IoT representation. This paper presents the functional design and implementation of a complete WSN platform that can be used for a range of long-term environmental monitoring IoT applications. The application requirements for low cost, high number of sensors, fast deployment, long lifetime, low maintenance, and high quality of service are considered in the specification and design of the platform and of all its components. Low-effort platform reuse is also considered starting from the specifications and at all design levels for a wide array of related monitoring applications.},
  db       = {IEEE},
  doi      = {10.1109/JETCAS.2013.2243032},
  keywords = {IP networks;Internet of Things;data acquisition;environmental monitoring (geophysics);environmental science computing;quality of service;sensor placement;telecommunication network reliability;wireless sensor networks;Internet Protocol;Internet-of-Things;IoT application;IoT representation;WSN platform;functional design;long-term environmental data acquisition;long-term environmental monitoring;network lifetime;quality of service;sensor deployment;ubiquitous generalized access;wireless sensor networks;Internet of Things (IoT);IoT applications;WSN optimized design;WSN platform;WSN protocol;long term environmental monitoring applications;wireless sensor networks (WSN)},
}

@InProceedings{7983156,
  author    = {U. A. Noman and B. Negash and A. M. Rahmani and P. Liljeberg and H. Tenhunen},
  title     = {From threads to events: Adapting a lightweight middleware for Contiki OS},
  booktitle = {2017 14th IEEE Annual Consumer Communications Networking Conference (CCNC)},
  year      = {2017},
  pages     = {486-491},
  month     = {Jan},
  abstract  = {Interoperability is one of the key requirements in the Internet of Things considering the diverse platforms, communication standards and specifications available today. Inherent resource constraints in the majority of IoT devices makes it very difficult to use existing solutions for interoperability, thus demanding new approaches. This paper presents the process of adapting a lightweight interoperability middleware for IoT, LISA, from RIOT to Contiki OS and evaluates memory and power overheads. The middleware follows a service oriented architecture and classifies devices according to available resources to assign different roles, such as Application, Service and Manager Nodes. These roles live in different tiers in a generic IoT architecture, where the Manager nodes are located in the intermediate Fog layer. To adapt to an event based kernel of Contiki, the middleware defines and handles a set of events that are used to communicate with the user application. A network of nodes is simulated to show the architecture promoted by the middleware and the results are presented.},
  db        = {IEEE},
  doi       = {10.1109/CCNC.2017.7983156},
  keywords  = {Internet of Things;middleware;open systems;operating systems (computers);service-oriented architecture;Contiki OS;IoT architecture;IoT devices;LISA;RIOT;event based kernel;intermediate fog layer;lightweight interoperability middleware;manager nodes;memory overheads;power overheads;resource constraints;Internet of Things;Interoperability;Manganese;Message systems;Middleware;Protocols;Semantics;Contiki;Internet of Things;Interoperability;LISA},
}

@InProceedings{7774446,
  author    = {J. O. Ooi and F. A. B. Hussin and N. Zakaria},
  title     = {Dual-Engine Cross-ISA DBTO Technique Utilising MultiThreaded Support for Multicore Processor System},
  booktitle = {2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)},
  year      = {2016},
  pages     = {257-264},
  month     = {Sept},
  abstract  = {The emergence of new era of Internet of Things or IoT have encouraged intensive if not extensive usage of modern mobile apps, thus multi-ISA equipped multicore processor gain great potential to be used for more efficient instruction binary processing in near future. In order to support this ISA diversity of computing platforms, mix modes of statically and dynamically Binary Translation and Optimization system, popularly consists of QEMU and LLVM or similar system, is the default technique used. However this complex system exhibits heavy slowdown (60x slowdown as compare to generic QEMU) [21] which impede its performance especially for short running application codes, typically used in IoT based apps applications. This research introduce a dual binary code translation engines to support apps based and kernel based application codes, through utilising multithreaded supported apart of original single thread supported binary translation processing in run-time. The dual engine consists of TCG generator from QEMU, and LLVM which include rich optimisations library. The evaluation through PARSEC-3.0 Benchmark shows our Hybrid DBTO system achieved performance improvement approaching 2.0x for apps based programs and 1.25x for kernel based programs, for x86 to X86-64 emulation. This technique possess great potential and serve as research based platform for future binary translation technique development, including adaptive method.},
  db        = {IEEE},
  doi       = {10.1109/MCSoC.2016.36},
  keywords  = {Internet of Things;binary codes;mobile computing;multi-threading;multiprocessing systems;Internet of Things;IoT;LLVM;PARSEC-3.0 benchmark;QEMU;TCG generator;X86-64 emulation;apps based application codes;dual binary code translation engines;dual-engine cross-ISA DBTO technique;dynamically binary translation and optimization system;hybrid DBTO system;instruction binary processing;kernel based application codes;mobile apps;multiISA equipped multicore processor;multicore processor system;multithreaded support;optimisations library;short running application codes;statically binary translation and optimization system;Emulation;Instruction sets;Kernel;Multicore processing;Optimization;Registers;Virtual machining;Binary Optimization;Binary Translation;Multi-ISA processor;Multicores;Multitheraded},
}

@InProceedings{7117986,
  author    = {M. Pavlov and A. Petrov},
  title     = {Software architecture for scalable computing systems with automatic granularity selection of executable code},
  booktitle = {2015 17th Conference of Open Innovations Association (FRUCT)},
  year      = {2015},
  pages     = {151-156},
  month     = {April},
  abstract  = {The problem of developing software architecture and its platform implementation for scalable cloud services is addressed in the paper. New scheme of distributed software developing and executing is presented with argumentation and main principles behind solution. Performance evaluation of one of the platform components (data storage) is described.},
  db        = {IEEE},
  doi       = {10.1109/FRUCT.2015.7117986},
  issn      = {2305-7254},
  keywords  = {cloud computing;granular computing;software architecture;automatic granularity selection;data storage;distributed software development;distributed software execution;executable code;performance evaluation;scalable cloud services;scalable computing systems;software architecture;Computer architecture;Memory;Optimization;Runtime;Software;Software architecture;Virtual machining},
}

@InProceedings{7972165,
  author    = {W. Pipatsakulroj and V. Visoottiviseth and R. Takano},
  title     = {muMQ: A lightweight and scalable MQTT broker},
  booktitle = {2017 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN)},
  year      = {2017},
  pages     = {1-6},
  month     = {June},
  abstract  = {A message broker is an imperative component in IoT systems, and it works as a gateway between IoT devices and application platforms. With the growth of IoT devices today, these systems can easily overwhelm message brokers unless the software can fully utilize hardware resources such as multi-core facility. This paper presents muMQ, a high-performance MQTT broker running on Commercial-Off-The-Shelf hardware. It tackles the challenge to improve the performance of message brokering on a single machine by efficiently utilizing multi-core CPUs. First, muMQ exploits an event-driven I/O mechanism for multi-core scalability. Each CPU core equally handles dispatched TCP connections and locally processes MQTT logic. Second, muMQ adopts a user-level TCP/IP stack, mTCP with DPDK, to avoid the overhead of the in-kernel TCP/IP stack, including system call overhead and resource contention. We evaluate the effectiveness of our approach through experiments. The results show that muMQ can handle 512K or greater long-lived subscribers with no message loss; muMQ achieves a publish messaging rate at 930K messages per second, which is 5.38 times faster than an existing MQTT broker. We also confirm mTCP accelerates the performance by 1.8 times compared with muMQ using the in-kernel TCP/IP stack.},
  db        = {IEEE},
  doi       = {10.1109/LANMAN.2017.7972165},
  keywords  = {Internet of Things;multiprocessing systems;transport protocols;CPU core;Commercial-Off-The-Shelf hardware;DPDK;IoT devices;IoT systems;MQTT logic;application platforms;dispatched TCP connections;event-driven I/O mechanism;hardware resources;high-performance MQTT broker;in-kernel TCP/IP stack;mTCP;message brokering;muMQ;multicore CPU;multicore facility;multicore scalability;publish messaging rate;resource contention;scalable MQTT broker;single machine;user-level TCP/IP stack;Hardware;Kernel;Message systems;Multicore processing;Scalability;Sockets;TCPIP;MQTT;high performance computing;message broker;multi-core system;user-level TCP/IP stack},
}

@InProceedings{5678449,
  author    = {T. Riedel and N. Fantana and A. Genaid and D. Yordanov and H. R. Schmidtke and M. Beigl},
  title     = {Using web service gateways and code generation for sustainable IoT system development},
  booktitle = {2010 Internet of Things (IOT)},
  year      = {2010},
  pages     = {1-8},
  month     = {Nov},
  abstract  = {Wireless Sensing and Radio Identification systems have undergone many innovations during the past years. This has led to short product lifetimes for both software and hardware compared to classical industries. However, especially industries dealing with long-term support of products, e.g. of industrial machinery, and product lifetime of 40+ years may especially profit from an Internet of Things. Motivated by a practical industrial servicing use case this paper shows how we hope to make equally sustainable IoT solutions by employing a model driven software development approach based on code generation for multi-protocol web service gateways.},
  db        = {IEEE},
  doi       = {10.1109/IOT.2010.5678449},
  keywords  = {Web services;internetworking;program compilers;protocols;software engineering;sustainable development;Internet of things;IoT system;Web service gateways;code generation;industrial servicing;multi-protocol;radio identification systems;software development approach;sustainable development;Automata;Logic gates;Radiofrequency identification;Semantics;Unified modeling language;Web services;XML},
}

@InProceedings{7562102,
  author    = {S. Thombre and R. Ul Islam and K. Andersson and M. S. Hossain},
  title     = {Performance analysis of an IP based protocol stack for WSNs},
  booktitle = {2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
  year      = {2016},
  pages     = {360-365},
  month     = {April},
  abstract  = {Wireless sensor networks (WSNs) are the key enablers of the Internet of Things (IoT) paradigm. Traditionally, sensor network research has been to be unlike the internet, motivated by power and device constraints. The IETF 6LoWPAN draft standard changes this, defining how IPv6 packets can be efficiently transmitted over IEEE 802.15.4 radio links. Due to this 6LoWPAN technology, low power, low cost microcontrollers can be connected to the internet forming what is known as the Wireless embedded Internet. Another IETF recommendation, CoAP allows these devices to communicate interactively over the Internet. The integration of such tiny, ubiquitous electronic devices to the Internet enables interesting real-time applications. We evaluate the performance of a stack consisting of CoAP and 6LoWPAN over the IEEE 802.15.4 radio link using the Contiki OS and Cooja simulator, along with the CoAP framework Californium (Cf).},
  db        = {IEEE},
  doi       = {10.1109/INFCOMW.2016.7562102},
  keywords  = {IP networks;Internet of Things;Zigbee;access protocols;packet radio networks;wireless sensor networks;CoAP framework californium;Contiki OS;Cooja simulator;IEEE 802.15.4 radio links;IETF 6LoWPAN draft standard;IP based protocol stack;IPv6 packets;Internet of Things;IoT;WSN;performance analysis;ubiquitous electronic devices;wireless embedded Internet;wireless sensor networks;IEEE 802.15 Standard;IP networks;Internet of things;Protocols;Wireless sensor networks;6LoWPAN;Californium (Cf);CoAP;Contiki;Cooja},
}

@InProceedings{7336328,
  author    = {D. Yunge and P. Kindt and M. Balszun and S. Chakraborty},
  title     = {Hybrid Apps: Apps for the Internet of Things},
  booktitle = {2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
  year      = {2015},
  pages     = {1175-1180},
  month     = {Aug},
  abstract  = {Smartphones have become popular mainly because of the large variety of applications they can run. In contrast, most devices in the phone's environment - e.g., household appliances or environmental sensors - are much less flexible because their functionality is hardcoded at the design time. In order to realize the vision of the Internet of Things (IoT), where all devices communicate with each other to realize joint tasks, it is necessary that these devices are able to extend and adapt their functionalities on-the-fly based on their surrounding. To realize smart functionalities for IoT devices, we propose "hybrid Apps", the concept of Smartphone "Apps" applied to small embedded systems. In contrast with current packaged "smart home" solutions, where all appliances have to be changed to their smart counterparts at the same time, hybrid Apps permit an incremental and hence feasible deployment of the IoT vision. In this paper we discuss the challenges and opportunities associated with this approach. We argue code interpretation as a candidate reprogramming method for IoT devices and analyzed its feasibility with real-world measurements of key parameters such as computational and energy overhead. While in general, code interpretation incurs a large energy-overhead, we show that for typical IoT applications executed every few seconds, it is as low as 1%.},
  db        = {IEEE},
  doi       = {10.1109/HPCC-CSS-ICESS.2015.292},
  keywords  = {Internet of Things;embedded systems;home computing;smart phones;Internet of Things;IoT applications;IoT devices;candidate reprogramming method;code interpretation;embedded systems;energy-overhead;hybrid apps;packaged smart home solutions;smartphone apps;Hardware;Java;Middleware;Protocols;Random access memory;Temperature sensors;Apps;Code Interpreter;Reprogrammability;Virtual Machines;Wireless Sensor Networks},
}

@InProceedings{7883359,
  author    = {T. Ball and J. Protzenko and J. Bishop and M. Moskal and J. de Halleux and M. Braun and S. Hodges and C. Riley},
  title     = {Microsoft Touch Develop and the BBC micro:bit},
  booktitle = {2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)},
  year      = {2016},
  pages     = {637-640},
  month     = {May},
  abstract  = {The chance to influence the lives of a million children does not come often. Through a partnership between the BBC and several technology companies, a small instructional computing device called the BBC micro:bit will be given to a million children in the UK in 2016. Moreover, using the micro:bit will be part of the CS curriculum. We describe how Microsoft's Touch Develop programming platform works with the BBC micro:bit. We describe the design and architecture of the micro:bit and the software engineering hurdles that had to be overcome to ensure it was as accessible as possible to children and teachers. The combined hardware/software platform is evaluated and early anecdotal evidence is presented.},
  db        = {IEEE},
  keywords  = {hardware-software codesign;BBC micro:bit;Microsoft touch develop programming platform;combined hardware-software platform;instructional computing device;software engineering;C++ languages;Computers;Encoding;Hardware;Libraries;Programming;Software;BBC micro:bit;K-12 education;Touch Develop;cloud;devices},
}

@Article{7927932,
  author   = {J. Lin},
  title    = {In Defense of MapReduce},
  journal  = {IEEE Internet Computing},
  year     = {2017},
  volume   = {21},
  number   = {3},
  pages    = {94-98},
  month    = {May},
  issn     = {1089-7801},
  abstract = {Don't throw the MapReduce baby out with the bath water! MapReduce represents a specific instance of a general class of data-parallel dataflow languages, in which computations are conceptualized as directed graphs, where vertices represent operations on records that flow along the directed edges. From this perspective, MAP and REDUCE are the two operators that MapReduce provides, which define particular configurations of the edges that flow into and out of vertices and specify the computations that occur at the vertices themselves.},
  db       = {IEEE},
  doi      = {10.1109/MIC.2017.53},
  keywords = {data flow computing;data handling;MapReduce;Big Data;Computational modeling;Internet;Internet and web services;Optimization;Performance evaluation;Internet/Web technologies;MapReduce;Spark;big data},
}

@Article{7937791,
  author   = {L. Rodríguez-Gil and J. García-Zubia and P. Orduña and D. López-de-Ipiña},
  title    = {An Open and Scalable Web-Based Interactive Live-Streaming architecture: The WILSP Platform},
  journal  = {IEEE Access},
  year     = {2017},
  volume   = {5},
  pages    = {9842-9856},
  issn     = {2169-3536},
  abstract = {Interactive live-streaming applications and platforms face particular challenges: the actions of the viewer's affect the content of the stream. A minimal capture-render delay is critical. This is the case of applications, such as remote laboratories, which allow students to view specific hardware through a webcam, and interact with it remotely in close to real time. It is also the case of other applications, such as videoconferencing or remote rendering. In the latest years, several commercial live-streaming platforms have appeared. However, the most of them have two significant limitations. First, because they are oriented toward standard live-streaming, their capture-render delay tends to be too high for interactive live-streaming. Second, their architectures and sources are closed. That makes them unsuitable for many research and practical purposes, especially when customization is required. This paper presents the requirements for an interactive live-streaming platform, focusing on remote lab needs as a case study. Then, it proposes an architecture to satisfy those requirements that relies on Redis to achieve high scalability. The architecture is based on open technologies, and has been implemented and published as open source. From a client-side perspective, it is web-based and mobile-friendly. It is intended to be useful for both research and practical purposes. Finally, this paper experimentally evaluates the proposed architecture through its contributed implementation, analyzing its performance and scalability.},
  db       = {IEEE},
  doi      = {10.1109/ACCESS.2017.2710328},
  keywords = {Internet;computer aided instruction;interactive systems;laboratories;media streaming;Redis;WILSP platform;Webcam;client-side perspective;commercial live-streaming platforms;minimal capture-render delay;open Web-based interactive live-streaming architecture;open technologies;remote lab needs;scalable Web-based interactive live-streaming architecture;standard live-streaming;viewer action;Computer architecture;Delays;Remote laboratories;Robots;Scalability;Standards;Streaming media;Webcam;live streaming;live streaming platform;online learning tools;open;remote laboratories},
}

@InProceedings{7473021,
  author    = {M. U. Yaseen and M. S. Zafar and A. Anjum and R. Hill},
  title     = {High Performance Video Processing in Cloud Data Centres},
  booktitle = {2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)},
  year      = {2016},
  pages     = {152-161},
  month     = {March},
  abstract  = {Mobile phones and affordable cameras are generating large amounts of video data. This data holds information regarding several activities and incidents. Video analytics systems have been introduced to extract valuable information from this data. However, most of these systems are expensive, require human supervision and are time consuming. The probability of extracting inaccurate information is also high due to human involvement. We have addressed these challenges by proposing a cloud based high performance video analytics platform. This platform attempts to minimize human intervention, reduce computation time and enables the processing of a large number of video streams. It achieves high performance by optimizing the occupancy of GPU resources in cloud and minimizing the data transfer by concurrently processing a large number of video streams. The proposed video processing platform is evaluated in three stages. The first evaluation was performed at the cloud level in order to evaluate the scalability of the platform. This evaluation includes fetching and distributing video streams and efficiently utilizing available resources within the cloud. The second valuation was performed at the individual cloud nodes. This evaluation includes measuring the occupancy level, effect of data transfer and the extent of concurrency achieved at each node. The third evaluation was performed at the frame level in order to determine the performance of object recognition algorithms. To measure this, compute intensive tasks of the Local Binary Pattern (LBP) algorithm have been ported on to the GPU resources. The platform proved to be very scalable with high throughput and performance when tested on a large number of video streams with increasing number of nodes.},
  db        = {IEEE},
  doi       = {10.1109/SOSE.2016.56},
  keywords  = {cloud computing;computer centres;object recognition;video signal processing;video streaming;GPU resources;LBP algorithm;cameras;cloud based high performance video analytics;cloud data centres;cloud nodes;cloud resources;data transfer;high performance video processing;information extraction;local binary pattern;mobile phones;object recognition algorithms;occupancy level;video data;video stream processing;Cloud computing;Decoding;Graphics processing units;Object detection;Object recognition;Streaming media;Throughput;Cloud Computing;High Performance and GPUs;Video Processing;Video analytics},
}

@InProceedings{6825342,
  author    = {A. V. Brito and A. V. Negreiros},
  title     = {Allowing Large-Scale Systems Evaluation with Ptolemy through Distributed Simulation},
  booktitle = {2013 III Brazilian Symposium on Computing Systems Engineering},
  year      = {2013},
  pages     = {53-58},
  month     = {Dec},
  abstract  = {Nowadays, embedded systems have a huge amount of computational power and consequently, high complexity. It is quite usual to find different applications being executed in embedded systems. Embedded system design demands for method and tools that allow the simulation and verification in an efficient and practical way. This paper proposes the development and evaluation of a solution for embedded modeling and simulation of embedded systems in a distributed way by the integration of Ptolemy II and the High Level Architecture (HLA), in order to create an environment with high-performance execution of large-scale models. Experimental results demonstrated that the use of a non distributed simulation for some situations can be infeasible. It was demonstrated that a speedup of factor 4 was acquired when a model with 4,000 thousands actors were distributed in 8 different machines. It also presented a model of execution of each CPU core during the simulation.},
  db        = {IEEE},
  doi       = {10.1109/SBESC.2013.19},
  issn      = {2324-7886},
  keywords  = {digital simulation;embedded systems;telecommunication computing;wireless sensor networks;CPU core;HLA;Ptolemy II;embedded modeling;embedded simulation;embedded system design;high level architecture;high-performance execution;large-scale models;large-scale systems evaluation;nondistributed simulation;wireless sensor network;Computational modeling;Computer architecture;Embedded systems;Graphics;Unified modeling language;Wireless sensor networks;Distributed Simulation;Embedded Systems;Heterogeneous Simulation},
}

@InProceedings{6690509,
  author    = {A. V. Brito and A. V. Negreiros and C. Roth and O. Sander and J. Becker},
  title     = {Development and Evaluation of Distributed Simulation of Embedded Systems Using Ptolemy and HLA},
  booktitle = {2013 IEEE/ACM 17th International Symposium on Distributed Simulation and Real Time Applications},
  year      = {2013},
  pages     = {189-196},
  month     = {Oct},
  abstract  = {Nowadays, embedded systems have a huge amount of computational power and consequently, high complexity. It is quite usual to find different applications being executed in embedded systems. Embedded system design demands for method and tools that allow the simulation and verification in an efficient and practical way. This paper proposes the development and evaluation of a solution for embedded modeling and simulation of heterogeneous Models of Computation (MoCs) in a distributed way by the integration of Ptolemy II and the High Level Architecture (HLA), a middleware for distributed discrete event simulation, in order to create an environment with high-performance execution of large-scale heterogeneous models. Experimental results demonstrate, that the use of a non distributed simulation for some situations can be infeasible, as well as the use of distributed simulation with few machines, like one, two or three computers. It was demonstrated that a speedup of factor 4 was acquired when a model with 4,000 thousands actors were distributed in 8 different machines.},
  db        = {IEEE},
  doi       = {10.1109/DS-RT.2013.28},
  issn      = {1550-6525},
  keywords  = {discrete event simulation;embedded systems;middleware;HLA;MoC;Ptolemy II;computational power;distributed discrete event simulation;embedded modeling and simulation;embedded systems;heterogeneous models of computation;high level architecture;high-performance execution;large-scale heterogeneous models;middleware;nondistributed simulation;Computational modeling;Computer architecture;Embedded systems;Ports (Computers);Sensors;Unified modeling language;Wireless sensor networks;Distributed Simulation;Embedded Systems;Heterogeneous Simulation},
}

@InProceedings{5751519,
  author    = {C. Brooks and E. A. Lee and S. Tripakis},
  title     = {Exploring models of computation with Ptolemy II},
  booktitle = {2010 IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
  year      = {2010},
  pages     = {331-332},
  month     = {Oct},
  abstract  = {The Ptolemy project studies modeling, simulation, and design of concurrent, real-time, embedded systems. The focus is on assembly of concurrent components. The key underlying principle in the project is the use of well-defined models of computation that govern the interaction between components. A major problem area being addressed is the use of heterogeneous mixtures of models of computation. Ptolemy II takes a component view of design, in that models are constructed as a set of interacting components. A model of computation governs the semantics of the interaction, and thus imposes an execution-time discipline. Ptolemy II has implementations of many models of computation including Synchronous Data Flow, Kahn Process Networks, Discrete Event, Continuous Time, Synchronous/Reactive and Modal Models This hands-on tutorial explores how these models of computation are implemented in Ptolemy II and how to create new models of computation such as a "non-dogmatic" Process Networks example and a left-to-right execution policy example.},
  db        = {IEEE},
  doi       = {10.1145/1878961.1879020},
  keywords  = {embedded systems;object-oriented methods;public domain software;Kahn process networks;Ptolemy II;concurrent embedded systems;continuous time models;discrete event models;execution time discipline;left-to-right execution policy;modal models;non dogmatic process networks;real time embedded systems;synchronous data flow;synchronous-reactive models;Computational modeling;Data models;Object oriented modeling;Real time systems;Semantics;Software;Tutorials;Modeling;concurrency;simulation},
}

@InProceedings{7173939,
  author    = {M. Hölzl and T. Gabor},
  title     = {Continuous Collaboration: A Case Study on the Development of an Adaptive Cyber-physical System},
  booktitle = {2015 IEEE/ACM 1st International Workshop on Software Engineering for Smart Cyber-Physical Systems},
  year      = {2015},
  pages     = {19-25},
  month     = {May},
  abstract  = {The need to interact with complex environments that are often not well understood at design time makes the development of smart cyber-physical systems (sCPS) a challenging endeavor. We propose a set of practices and tools that support the design and implementation of sCPS using continuous collaboration -- a development lifecycle and architecture to continuously incorporate data gained from the operation of the sCPS into the system. Continuous collaboration attempts to harmonize three interlocking feedback cycles: refinement of the system design by the developers, autonomous evolution of agents in the sCPS, and feedback from the evolving system to the developers. To support the process we introduce tools and techniques that we have found helpful to realize continuous collaboration: The HADES/Hexameter platform, extended behavior trees and the teacher/student learning pattern.},
  db        = {IEEE},
  doi       = {10.1109/SEsCPS.2015.12},
  keywords  = {software architecture;HADES-Hexameter platform;adaptive cyber-physical system;complex environments;continuous collaboration;extended behavior trees;interlocking feedback cycles;sCPS;software architecture;software development lifecycle approach;system design;teacher-student learning pattern;Collaboration;Cyber-physical systems;Hardware;Navigation;Rescue robots;Software;Cyber-Physical Systems;Embodied Evolution;Evolutionary Algorithms;Reinforcement Learning;Software Engineering},
}

@InProceedings{6064506,
  author    = {E. A. Lee},
  title     = {Heterogeneous actor modeling},
  booktitle = {2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)},
  year      = {2011},
  pages     = {3-12},
  month     = {Oct},
  abstract  = {Complex systems demand diversity in the modeling mechanisms. This “roadmap” paper prescribes an approach to modeling based on concurrent communicating components (called actors), where a diversity of orchestration strategies govern the execution and interaction of the components. The prescribed approach has been extensively explored in the Ptolemy Project, but as yet is not widely deployed in engineering practice. The approach achieves interaction between diverse models using an abstract semantics, which is a deliberately incomplete semantics that cannot by itself define a useful modeling framework. It instead focuses on the interactions between diverse models, reducing the nature of those interactions to a minimum that achieves a well-defined composition. The actor semantics is an abstract semantics that can handle many heterogeneous models that are built today, and some that are not common today. The actor abstract semantics and many concrete semantics are implemented in Ptolemy II, an open-source software framework.},
  db        = {IEEE},
  doi       = {10.1145/2038642.2038646},
  keywords  = {large-scale systems;multiprocessing programs;public domain software;Ptolemy project;abstract semantics;actor semantics;complex systems;concurrent communicating components;heterogeneous actor modeling;modeling mechanisms;open-source software;orchestration strategies;roadmap;Adaptation models;Computational modeling;Mathematical model;Object oriented modeling;Semantics;Syntactics;Unified modeling language;Ptolemy;heterogeneity;models of computation},
}

@Article{7513207,
  author   = {L. Li and D. Li and Z. Su and L. Jin and G. Huang},
  title    = {Performance analysis and framework optimization of open source cloud storage system},
  journal  = {China Communications},
  year     = {2016},
  volume   = {13},
  number   = {6},
  pages    = {110-122},
  month    = {June},
  issn     = {1673-5447},
  abstract = {More and more embedded devices, such as mobile phones, tablet PCs and laptops, are used in every field, so huge files need to be stored or backed up into cloud storage. Optimizing the performance of cloud storage is very important for Internet development. This paper presents the performance evaluation of the open source distributed storage system, a highly available, distributed, eventually consistent object/blob store from OpenStack cloud computing components. This paper mainly focuses on the mechanism of cloud storage as well as the optimization methods to process different sized files. This work provides two major contributions through comprehensive performance evaluations. First, it provides different configurations for OpenStack Swift system and an analysis of how every component affects the performance. Second, it presents the detailed optimization methods to improve the performance in processing different sized files. The experimental results show that our method improves the performance and the structure. We give the methods to optimize the object-based cloud storage system to deploy the readily available storage system.},
  db       = {IEEE},
  doi      = {10.1109/CC.2016.7513207},
  keywords = {cloud computing;public domain software;software performance evaluation;storage management;Internet development;OpenStack Swift system;OpenStack cloud computing components;comprehensive performance evaluations;embedded devices;framework optimization;object-based cloud storage system;open source cloud storage system;open source distributed storage system;optimization methods;performance analysis;Cloud computing;File systems;Hard disks;Optimization;Performance evaluation;Servers;Cloud Computing;Distribute System;Object Storage;OpenStack Swift;Storage Service Optimization},
}

@InProceedings{6843715,
  author    = {R. Mancuso and O. D. Dantsker and M. Caccamo and M. S. Selig},
  title     = {A low-power architecture for high frequency sensor acquisition in many-DOF UAVs},
  booktitle = {2014 ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS)},
  year      = {2014},
  pages     = {103-114},
  month     = {April},
  abstract  = {Unmanned Aerial Vehicles (UAVs) are becoming increasingly popular thanks to an increase in the accessibility of components with high reliability and reduced cost, making them suitable for civil, military and research purposes. Vehicles classified as UAVs can have largely different properties in terms of physical design, size, power, capabilities, as well as associated production and operational cost. In this work, we target UAVs that feature a high number of degrees of freedom (DOF) and that are instrumented with a large number of sensors. For such platforms, we propose an architecture to perform data acquisition from on-board instrumentation at a frequency (100 Hz) that is twice as fast as existing products. Our architecture is capable of performing acquisition with strict timing constraints, thus, the produced data stream is suitable for performing real-time sensor fusion. Furthermore, our architecture can be implemented using embedded, commercial hardware, resulting in a low-cost solution. Finally, the resulting data acquisition unit features a low-power consumption, allowing it to operate for two to three hours with a miniature battery.},
  db        = {IEEE},
  doi       = {10.1109/ICCPS.2014.6843715},
  keywords  = {autonomous aerial vehicles;data acquisition;mobile robots;sensor fusion;data stream;degrees-of-freedom;frequency 100 Hz;high frequency sensor acquisition;low-power architecture;manyDOF UAV;miniature battery;sensor fusion;timing constraints;unmanned aerial vehicles;Data acquisition;Hardware;Magnetic separation;Magnetometers;Magnetosphere;Monitoring;Pulse width modulation},
}

@InProceedings{6473629,
  author    = {Â. L. V. d. Negreiros and A. V. Brito},
  title     = {The Development of a Methodology with a Tool Support to the Distributed Simulation of Heterogeneous and Complexes Embedded Systems},
  booktitle = {2012 Brazilian Symposium on Computing System Engineering},
  year      = {2012},
  pages     = {37-42},
  month     = {Nov},
  abstract  = {Nowadays, embedded systems contains a big computational power and consequently a big complexity. It is very common to find different kinds of applications being executed in embedded systems. With this scenario, it is necessary some method and/or tool that allows the simulation of those systems in an efficient and practice way. The goal of this paper is to expose the integration between Ptolemy II and HLA in order to enable the elaboration of one methodology, with a tool support, to model and simulate large scale heterogeneous embedded systems.},
  db        = {IEEE},
  doi       = {10.1109/SBESC.2012.16},
  issn      = {2324-7886},
  keywords  = {computational complexity;digital simulation;embedded systems;HLA;Ptolemy II;big complexity;complexes embedded systems;computational power;distributed simulation;large scale heterogeneous embedded systems;tool support;Computational modeling;Computer architecture;Embedded systems;Hardware;Integrated circuit modeling;Mathematical model;Unified modeling language;Distributed Simulation;HLA;Heterogeneous Systems;High Level Architecture;Ptolemy},
}

@InProceedings{6549920,
  author    = {E. Pereira and C. Potiron and C. M. Kirsch and R. Sengupta},
  title     = {Modeling and controlling the structure of heterogeneous mobile robotic systems: A bigactor approach},
  booktitle = {2013 IEEE International Systems Conference (SysCon)},
  year      = {2013},
  pages     = {442-447},
  month     = {April},
  abstract  = {In this paper we address the problem of modelling and controlling heterogeneous mobile robotic systems at a structure-level abstraction. We consider a system of mobile robotic entities that are able to observe, control, compute, and communicate. They operate upon an abstraction of the structure of the world that entails location and connectivity as first-class concepts. Our approach is to model mobile robotic entities as bigActors [18], a model of computation that combines bigraphs with the actor model for modeling structure-aware computation. As case study, we model a mission of heterogeneous unmanned vehicles performing an environmental monitoring mission.},
  db        = {IEEE},
  doi       = {10.1109/SysCon.2013.6549920},
  keywords  = {mobile robots;BigActor approach;heterogeneous mobile robotic system;heterogeneous unmanned vehicle;structure-aware computation;structure-level abstraction;Computational modeling;Mathematical model;Mobile communication;Robots;Semantics;Vehicles},
}

@InProceedings{7345624,
  author    = {E. Rakadjiev and T. Shimosawa and H. Mine and S. Oshima},
  title     = {Parallel SMT Solving and Concurrent Symbolic Execution},
  booktitle = {2015 IEEE Trustcom/BigDataSE/ISPA},
  year      = {2015},
  volume    = {3},
  pages     = {17-26},
  month     = {Aug},
  abstract  = {Satisfiability Modulo Theories (SMT) solving is a fundamental tool in numerous areas of computer science, where problems are expressed as logical formulas whose satisfiability has to be decided. State-of-the-art solvers can handle many real-world problems efficiently, however, SMT solving is an NP-hard problem, and the strong reliance on the solvers typically makes them the dominating performance hot spot of the systems utilizing them. Symbolic execution is a software analysis method used for automated high-coverage test generation, among others. It relies heavily on SMT solving and spends substantial amount of its run time, commonly more than 90%, in solver activities. In this paper, we investigate how symbolic execution can benefit from the use of general-purpose, parallel SMT solving. We present design, prototypical implementation, and evaluation of a linearly scalable SMT solver cluster and an extension of the KLEE symbolic execution engine, offering concurrent execution and asynchronous constraint solving. We show that, depending on the characteristics of the program being analyzed, KLEE's performance is improved by up to 7.6x with the help of our approach.},
  db        = {IEEE},
  doi       = {10.1109/Trustcom.2015.608},
  keywords  = {computability;computational complexity;concurrency control;constraint handling;parallel processing;program diagnostics;KLEE symbolic execution engine;NP-hard problem;asynchronous constraint solving;automated high-coverage test generation;concurrent symbolic execution;linearly scalable SMT solver cluster;logical formulas;parallel SMT solving;satisfiability modulo theories;software analysis method;Computer science;Concurrent computing;Data structures;Explosions;Scalability;Search problems;Software;Asynchronous SMT Solving;Concurrent Symbolic Execution;Distributed SMT Solving;Parallel SMT Solving;SMT Solving;Symbolic Execution},
}

@InProceedings{7925596,
  author    = {C. Shen and S. Chen},
  title     = {A Cyber-Physical Design for Indoor Temperature Monitoring Using Wireless Sensor Networks},
  booktitle = {2017 IEEE Wireless Communications and Networking Conference (WCNC)},
  year      = {2017},
  pages     = {1-6},
  month     = {March},
  abstract  = {Indoor temperature monitoring using wireless sensor networks is critical in many applications. For example, in data centers, it is important to monitor if the indoor temperature is within certain range so that the computers can function with near-optimal performance. In contrast to existing research that often separates the sensor network design and the measured temperature, this paper proposes a cyber-physical design approach to monitor the indoor temperature using wireless sensor networks. The source sensor wakes up and senses the temperature periodically using sleep#x002F;wake duty cycles and sends the data to the destination via multi-hop relaying nodes in an anycast way. Moreover, the period of sleep#x002F;wake duty cycle is dynamically adjusted based on the sensed temperature: when the measured temperature is normal, the sensor nodes wake up infrequently for better energy efficiency; as the sensed temperature approaches a pre-determined threshold, the sensor nodes wake up more frequently to avoid any delayed alarm trigger. The proposed design is implemented using TelosB with TinyOS, and experiments confirm that the cyber-physical system reports the alarm with a very small delay while achieving high long-term energy efficiency.},
  db        = {IEEE},
  doi       = {10.1109/WCNC.2017.7925596},
  keywords  = {computerised monitoring;cyber-physical systems;temperature sensors;wireless sensor networks;cyber-physical design;energy efficiency;indoor temperature monitoring;multihop relaying nodes;source sensor;wireless sensor networks;Monitoring;Protocols;Relays;Temperature distribution;Temperature measurement;Temperature sensors;Wireless sensor networks},
}

@InProceedings{4542057,
  author    = {A. Anane and E. M. Aboulhamid and J. Vachon and Y. Savaria},
  title     = {Modeling and simulation of complex heterogeneous systems},
  booktitle = {2008 IEEE International Symposium on Circuits and Systems},
  year      = {2008},
  pages     = {2873-2876},
  month     = {May},
  abstract  = {Given the increasing heterogeneity and complexity of systems being developed, untimed modeling at a system level becomes more and more important for design space exploration and verification, due to its conciseness and speed. After showing inadequacies of SystemC, which is the predominant modeling environment in this area, we propose a paradigm shift from immediate notifications and coroutines in SystemC to Atomic Actions and true parallelism in an extension of Esys.NET. We exploit the introspection and attribute programming to extend the capabilities of the environment and to build the basis for heterogeneous cosimulation. This paper aims to show the main advantages of this paradigm shift, such as (1) the improvement of simulation time by exploiting the capabilities of multicore simulation hosts, (2) the reduction of modeling hazards related to parallelism and resource sharing, and (3) a more efficient design space exploration.},
  db        = {IEEE},
  doi       = {10.1109/ISCAS.2008.4542057},
  issn      = {0271-4302},
  keywords  = {programming language semantics;software engineering;temporal logic;Esys.NET;SystemC;atomic actions;complex heterogeneous systems;heterogeneous cosimulation;multicore simulation;paradigm shift;resource sharing;space exploration;Collaborative work;Costs;Hazards;Kernel;Libraries;Manufacturing processes;Multicore processing;Productivity;Resource management;Space exploration},
}

@InProceedings{5558636,
  author    = {S. Andalam and P. Roop and A. Girault},
  title     = {Predictable multithreading of embedded applications using PRET-C},
  booktitle = {Eighth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2010)},
  year      = {2010},
  pages     = {159-168},
  month     = {July},
  abstract  = {We propose a new language called Precision Timed C (PRET-C), for predictable and lightweight multi-threading in C. PRET-C supports synchronous concurrency, preemption, and a high-level construct for logical time. In contrast to existing synchronous languages, PRET-C offers C-based shared memory communications between concurrent threads that is guaranteed to be thread safe. Due to the proposed synchronous semantics, the mapping of logical time to physical time can be achieved much more easily than with plain C, thanks to a Worst Case Reaction Time (WCRT) analyzer (not presented here). Associated to the PRET-C programming language, we present a dedicated target architecture, called ARPRET, which combines a hardware accelerator associated to an existing softcore processor. This allows us to improve the throughput while preserving the predictability. With extensive benchmarking, we then demonstrate that ARPRET not only achieves completely predictable execution of PRET-C programs, but also improves the throughput when compared to the pure software execution of PRET-C. The PRET-C software approach is also significantly more efficient in comparison to two other light-weight concurrent C variants (namely SC and Protothreads), as well as the well-known Esterel synchronous programming language.},
  db        = {IEEE},
  doi       = {10.1109/MEMCOD.2010.5558636},
  keywords  = {C language;embedded systems;multi-threading;shared memory systems;ARPRET;Esterel synchronous programming language;PRET-C programming language;embedded applications;precision timed C;predictable multithreading;shared memory communications;worst case reaction time analyzer;Assembly;Concurrent computing;Hardware;Instruction sets;Semantics;Timing},
}

@Article{6463378,
  author   = {S. Andalam and P. S. Roop and A. Girault and C. Traulsen},
  title    = {A Predictable Framework for Safety-Critical Embedded Systems},
  journal  = {IEEE Transactions on Computers},
  year     = {2014},
  volume   = {63},
  number   = {7},
  pages    = {1600-1612},
  month    = {July},
  issn     = {0018-9340},
  abstract = {Safety-critical embedded systems, commonly found in automotive, space, and health-care, are highly reactive and concurrent. Their most important characteristics are that they require both functional and timing correctness. C has been the language of choice for programming such systems. However, C lacks many features that can make the design process of such systems seamless while also maintaining predictability. This paper addresses the need for a C-based design framework for achieving time predictability. To this end, we propose the PRET-C language and the ARPRET architecture. PRET-C offers a small set of extensions to a subset of C to facilitate effective concurrent programming. We present a new synchronous semantics for PRET-C. It guarantees that all PRET-C programs are deterministic, reactive, and provides thread-safe communication via shared memory access. This simplifies considerably the design of safety-critical systems. We also present the architecture of a precision timed machine (PRET) called ARPRET. It offers the ability to design time predictable architectures through simple customizations of soft-core processors. We have designed ARPRET particularly for efficient and predictable execution of PRET-C. We demonstrate through extensive benchmarking that PRET-C based system design excels in comparison to existing C-based paradigms. We also qualitatively compare our approach to the Berkeley-Columbia PRET approach. We have demonstrated that the proposed approach provides an ideal framework for designing and validating safety-critical embedded systems.},
  db       = {IEEE},
  doi      = {10.1109/TC.2013.28},
  keywords = {C language;embedded systems;multi-threading;programming language semantics;safety-critical software;shared memory systems;ARPRET;ARPRET architecture;Berkeley-Columbia PRET approach;C-based design framework;PRET-C language;PRET-C programs;Precision Timed C language;architecture of a precision timed machine;concurrent programming;deterministic programs;lightweight multithreaded language;reactive programs;safety-critical embedded systems;shared memory access;soft-core processors;synchronous semantics;thread-safe communication;time predictable architectures;Computer architecture;Instruction sets;Programming;Real-time systems;Semantics;Timing;PRET;PRET-C;Safety-critical systems;WCET;WCRT;synchronous languages;time predictability},
}

@InProceedings{6007780,
  author    = {S. P. Crago and D. I. Kang and M. Kang and R. Kost and K. Singh and J. Suh and J. P. Walters},
  title     = {Programming Models and Development Software for a Space-Based Many-Core Processor},
  booktitle = {2011 IEEE Fourth International Conference on Space Mission Challenges for Information Technology},
  year      = {2011},
  pages     = {95-102},
  month     = {Aug},
  abstract  = {The Maestro processor is a 49-core many-core processor for space based on the TILE64 architecture and implemented in rad-hard-by-design technology by Boeing. In this paper we discuss the programming models for Maestro, the implications of the programming model on fault tolerance and flight software, and the software development tools that have been developed for Maestro. The software described here is experimental development software that allows application and algorithm evaluation on the architecture, but we believe this software can be used as the basis for flight software. The software includes libraries, performance analysis and optimization tools, and compilers. While this work was done on the Maestro chip, the principles discussed can be applied to any multi-core or many-core processor.},
  db        = {IEEE},
  doi       = {10.1109/SMC-IT.2011.29},
  keywords  = {aerospace computing;computer architecture;microprocessor chips;multiprocessing systems;program compilers;program processors;software architecture;software fault tolerance;software libraries;software performance evaluation;49-core manycore processor;Maestro chip;Maestro processor;TILE64 architecture;fault tolerance;flight software;multicore processor;optimization tool;programming model;rad-hard-by-design technology;software compiler;software development tool;software library;space-based manycore processor;Computer architecture;Libraries;Linux;Message passing;Programming;Real time systems;Software;Multi-core programming;parallel software;space-based processing},
}

@InProceedings{5665620,
  author    = {R. Fritzsche and C. Siemers},
  title     = {Scheduling of time enhanced c (TEC)},
  booktitle = {2010 World Automation Congress},
  year      = {2010},
  pages     = {1-6},
  month     = {Sept},
  abstract  = {Real-time systems mainly consist of time or event-triggered tasks that must satisfy deadline-constraints and other limitations to the execution time. Scheduling of them is a common problem especially if no operating system can be used because of limited resources like code-size and CPU power. Previous approaches deal with multi-frame models to split tasks into smaller subtask that may be arranged at compile-time in a static way to cope with given deadlines. Handling of non-periodic events and context-switching problems demand a more dynamic scheduling. This paper presents an approach of using manually given information for timing constraints in order to rearrange the code to satisfy the deadlines automatically. The presented design is still able to handle events and to force the given functions to cooperate. Supporting hardware for producing timing-events may further help the system to organize the program-flow.},
  db        = {IEEE},
  issn      = {2154-4824},
  keywords  = {dynamic scheduling;multiprogramming;real-time systems;software engineering;CPU power;context switching problem;deadline constraint;dynamic scheduling;event triggered task;multiframe model;nonperiodic event;program flow;real time system;split task;time enhanced C;timing constraint;Context;Switches;application-internal scheduler;forced cooperative design;multi-frame tasks;semi-dynamic scheduling;time-enhanced language},
}

@InProceedings{6059016,
  author    = {R. Inam and J. Mäki-Turja and M. Sjödin and S. M. H. Ashjaei and S. Afshar},
  title     = {Support for hierarchical scheduling in FreeRTOS},
  booktitle = {ETFA2011},
  year      = {2011},
  pages     = {1-10},
  month     = {Sept},
  abstract  = {This paper presents the implementation of a Hierarchical Scheduling Framework (HSF) on an open source real-time operating system (FreeRTOS) to support the temporal isolation between a number of applications, on a single processor. The goal is to achieve predictable integration and reusability of independently developed components or applications. We present the initial results of the HSF implementation by running it on an AVR 32-bit board EVK1100. The paper addresses the fixed-priority preemptive scheduling at both global and local scheduling levels. It describes the detailed design of HSF with the emphasis of doing minimal changes to the underlying FreeRTOS kernel and keeping its API intact. Finally it provides (and compares) the results for the performance measures of idling and deferrable servers with respect to the overhead of the implementation.},
  db        = {IEEE},
  doi       = {10.1109/ETFA.2011.6059016},
  issn      = {1946-0740},
  keywords  = {application program interfaces;object-oriented programming;operating system kernels;public domain software;real-time systems;scheduling;software reusability;API;AVR EVK1100;FreeRTOS kernel;application reusability;fixed-priority preemptive scheduling;global scheduling levels;hierarchical scheduling framework;independently developed component reusability;local scheduling levels;open source real-time operating system;predictable integration;single processor;temporal isolation;Job shop scheduling;Kernel;Processor scheduling;Real time systems;Schedules;Servers;fixed-priority scheduling;hierarchical scheduling framework;real-time systems},
}

@InProceedings{5456987,
  author    = {R. S. Khaligh and M. Radetzki},
  title     = {Modeling constructs and kernel for parallel simulation of accuracy adaptive TLMs},
  booktitle = {2010 Design, Automation Test in Europe Conference Exhibition (DATE 2010)},
  year      = {2010},
  pages     = {1183-1188},
  month     = {March},
  abstract  = {We present a set of modeling constructs accompanied by a high performance simulation kernel for accuracy adaptive transaction level models. In contrast to traditional, fixed accuracy TLMs, accuracy of adaptive TLMs can be changed during simulation to the level which is most suitable for a given use case and scenario. Ad-hoc development of adaptive models can result in complex models, and the implementation detail of adaptivity mechanisms can obscure the actual logic of a model. To simplify and enable systematic development of adaptive models, we have identified several mechanisms which are applicable to a wide variety of models. The proposed constructs relieve the modeler from low level implementation details of those mechanisms. We have developed an efficient, light-weight simulation kernel optimized for the proposed constructs, which enables parallel simulation of large models on widely available, low-cost multi-core simulation hosts. The modeling constructs and the kernel have been evaluated using industrial benchmark applications.},
  db        = {IEEE},
  doi       = {10.1109/DATE.2010.5456987},
  issn      = {1530-1591},
  keywords  = {operating system kernels;transaction processing;accuracy adaptive transaction level model;ad-hoc development;adaptive TLM;adaptive models systematic development;adaptivity mechanisms;high performance simulation kernel;light weight simulation kernel;low cost multicore simulation hosts;parallel simulation;Adaptive systems;Computational modeling;Concurrent computing;Context modeling;Discrete event simulation;Embedded system;Kernel;Logic;Natural languages;Performance loss},
}

@InProceedings{4624024,
  author    = {D. G. Kim and S. M. Lee and D. R. Shin},
  title     = {Design of the Operating System Virtualization on L4 Microkernel},
  booktitle = {2008 Fourth International Conference on Networked Computing and Advanced Information Management},
  year      = {2008},
  volume    = {1},
  pages     = {307-310},
  month     = {Sept},
  abstract  = {The importance of the virtualization in embedded computing area is currently emerging. The virtualization can enhance system flexibility by enabling the concurrent execution of an application OS and a real-time OS (RTOS) on the same processor. L4 microkernel can be used as an efficient hypervisor which provides environment for operating systems virtualization. In order to run the application OSes on L4 microkernel, the application OSes should be adapted. The source code of Linux kernel can be readily accessed and modified. Hence, the Linux kernel is chosen as virtualized operating systems. In this paper, the architecture for virtualization of Linux kernel which is based on L4 microkernel is proposed.},
  db        = {IEEE},
  doi       = {10.1109/NCM.2008.165},
  keywords  = {Linux;operating system kernels;virtual machines;L4 microkernel;Linux kernel virtualization;concurrent execution;embedded computing;hypervisor;operating system virtualization;real-time operating system;system flexibility;Application software;Application virtualization;Embedded computing;Kernel;Linux;Operating systems;Platform virtualization;Real time systems;Virtual machine monitors;Yarn},
}

@Article{5710575,
  author   = {W. Liu and J. Xu and J. K. Muppala and W. Zhang and X. Wu and Y. Ye},
  title    = {Coroutine-Based Synthesis of Efficient Embedded Software From SystemC Models},
  journal  = {IEEE Embedded Systems Letters},
  year     = {2011},
  volume   = {3},
  number   = {1},
  pages    = {46-49},
  month    = {March},
  issn     = {1943-0663},
  abstract = {SystemC is a widely used electronic system-level (ESL) design language that can be used to model both hardware and software at different stages of system design. There has been a lot of research on behavior synthesis of hardware from SystemC, but relatively little work on synthesizing embedded software for SystemC designs. In this letter, we present an approach to automatic software synthesis from SystemC-based on coroutines instead of the traditional approaches based on real-time operating system (RTOS) threads. Performance evaluation results on some realistic applications show that our approach results in impressive reduction of runtime overheads compared to the thread-based approaches.},
  db       = {IEEE},
  doi      = {10.1109/LES.2011.2112634},
  keywords = {C++ language;embedded systems;operating systems (computers);SystemC models;coroutine-based synthesis;electronic system-level design language;embedded software synthesis;real-time operating system threads;Context;Instruction sets;Kernel;Prototypes;Switches;Synchronization;Performance;SystemC;software synthesis},
}

@InProceedings{5272418,
  author    = {E. Lubbers and M. Platzner},
  title     = {Cooperative multithreading in dynamically reconfigurable systems},
  booktitle = {2009 International Conference on Field Programmable Logic and Applications},
  year      = {2009},
  pages     = {551-554},
  month     = {Aug},
  abstract  = {Preemptive multitasking, a popular technique for timesharing of computational resources in software-based systems, faces considerable difficulties when applied to partially reconfigurable hardware. In this paper, we propose a cooperative scheduling technique for reconfigurable hardware threads as a feasible compromise between computational efficiency and implementation complexity. We have implemented this mechanism for the multithreaded reconfigurable operating system ReconOS and evaluated its overheads and performance on a prototype.},
  db        = {IEEE},
  doi       = {10.1109/FPL.2009.5272418},
  issn      = {1946-147X},
  keywords  = {multi-threading;multiprogramming;operating systems (computers);reconfigurable architectures;scheduling;ReconOS;cooperative multithreading;cooperative scheduling technique;dynamically reconfigurable system;multithreaded reconfigurable operating system;preemptive multitasking;Delay;Field programmable gate arrays;Hardware;Multitasking;Multithreading;Operating systems;Processor scheduling;Prototypes;Reconfigurable logic;Yarn},
}

@InProceedings{7363616,
  author    = {S. Park and H. Kim and S. Y. Kang and C. H. Koo and H. Joe},
  title     = {Lua-Based Virtual Machine Platform for Spacecraft On-Board Control Software},
  booktitle = {2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing},
  year      = {2015},
  pages     = {44-51},
  month     = {Oct},
  abstract  = {Mission critical embedded software for autonomous operation requires high development cost due to its long development cycle. One of the potential solutions for reducing the cost is to reuse the software developed at previous missions. Virtual machine platform such as JVM is a good example to provide code portability across various missions. Flight software in aerospace field is adopting this concept to improve reusability and eventually to reduce development cost. In this paper, we propose a Lua-based virtualization environment for spacecraft flight software. Flight software for spacecraft control consists of a few tasks that are highly autonomous. Lua is chosen as the script language for programming the control tasks. Though Lua was designed with simplicity and portability, it only supports multithreading with collaborative coroutines. To support preemptive multitasking, we implement time slicing coroutines as spacecraft control processes. New coroutine scheduler is devised and time slicing functionality is added into the scheduler. Scheduler locking and message passing with external flight software are also implemented. Instead of modifying the Lua interpreter, we have exploited the debug support APIs for our implementation. For evaluation, we have implemented the flight software virtualization environment on the flight computer. Accuracy of the time slicing scheduler is also analyzed.},
  db        = {IEEE},
  doi       = {10.1109/EUC.2015.21},
  keywords  = {aerospace control;application program interfaces;authoring languages;control engineering computing;message passing;multi-threading;program debugging;scheduling;software portability;software reusability;space vehicles;spacecraft computers;virtual machines;virtualisation;JVM;Lua interpreter;Lua script language;Lua-based virtual machine platform;Lua-based virtualization environment;aerospace field;autonomous operation;code portability;collaborative coroutines;control task programming;coroutine scheduler;debug support API;development cost reduction;flight computer;flight software virtualization environment;highly autonomous task;message passing;mission critical embedded software;multithreading;preemptive multitasking;scheduler locking;software reuse;spacecraft control;spacecraft flight software;spacecraft on-board control software;time slicing coroutines;time slicing scheduler;Computers;Engines;Runtime;Software;Space vehicles;Virtual machining;Virtualization;Lua;OBCP;mission critical embedded software;reusability;spacecraft;virtual machine},
}

@InProceedings{7515633,
  author    = {D. P. B. Renaux and F. Pöttker and C. E. Soares and C. C. Valério},
  title     = {A State-Based Function-Queue Software Architecture for Electric Motor Control},
  booktitle = {2016 IEEE 19th International Symposium on Real-Time Distributed Computing (ISORC)},
  year      = {2016},
  pages     = {229-236},
  month     = {May},
  abstract  = {Increasing demands on functional and temporal requirements for the software in electric motor controllers demand for solutions that are efficient in time and space usage while providing the required functionality. Embedded software for electric motor control must deal with the control itself, and with operation, protection, supervision, safety, and user interfaces. Concerning this need, an embedded software multitasking architecture that combines the concept of function queues and of state-based code is proposed and compared to a standard implementation based on an RTOS. In the proposed solution, the queue of function pointers is partitioned into several shorter queues each one active in a given state of the system, thus, reducing queue management overhead.},
  db        = {IEEE},
  doi       = {10.1109/ISORC.2016.39},
  keywords  = {control engineering computing;embedded systems;machine control;software architecture;RTOS;electric motor control;embedded software multitasking architecture;function pointer queue;queue management;state-based code;state-based function-queue software architecture;user interfaces;Computer architecture;Electric motors;Embedded software;Motor drives;Multitasking;Real-time systems;Electrical Motor Control;Embedded Software Multitasking Architecture;Real-Time Embedded Software;Task Scheduling},
}

@InProceedings{4417219,
  author    = {Jiri Zdenek},
  title     = {Efficient scheduler-dispatcher software architecture of the spacepower facility distributed control computer},
  booktitle = {2007 European Conference on Power Electronics and Applications},
  year      = {2007},
  pages     = {1-10},
  month     = {Sept},
  abstract  = {The system software architecture of the distributed control computer (computer network) of the mechatronic scientific facility (crystallizer) for automatic high temperature material processing in a orbital space station in micro-gravitation environment is presented in this paper. The scientific facility consists of the multi-zone high temperature furnace with heating system, PWM controlled heating converters, the precise extra low speed vibration-less electric drives to make possible to manipulate the processed material samples during experiments, very precise temperature measurement module, telemetric channel, crew interface computer, free programmable central controller and several further units. Facility computer network nodes have many user tasks (processes) divided into many threads running in real time environment. Using preemptive real time operating system tends to have unacceptable high overhead therefore the system of table driven coroutines with low system resource requirement (overhead, stack space) was designed. Emphasis is given on the design of efficient, reliable and self documented scheduler-dispatcher of the user tasks with minimized overhead and easily extensible descriptors of table driven user finite state automata. Presented scheduler architecture is used in the distributed network control computer of newly designed facility (Advanced TITUS) intended to be placed in the ISS space station. It is advanced version of the proved software utilized in the distributed control computer of the TITUS scientific equipment which was successfully operated several years in the MIR orbital station especially during ESA missions EUROMIR.},
  db        = {IEEE},
  doi       = {10.1109/EPE.2007.4417219},
  keywords  = {aerospace computing;aerospace control;aerospace instrumentation;computer networks;crystallisers;distributed control;electric furnaces;finite state machines;mechatronics;PWM controlled heating converters;TITUS scientific equipment;automatic high temperature material processing;crew interface computer;crystallizer;distributed control computer;distributed network control computer;facility computer network nodes;finite state automata;free programmable central controller;heating system;high temperature furnace;mechatronic scientific facility;microgravitation environment;orbital space station;real time operating system;scheduler-dispatcher software architecture;space power facility;telemetric channel;temperature measurement module;vibrationless electric drives;Automatic control;Centralized control;Computer architecture;Computer networks;Distributed computing;Distributed control;Processor scheduling;Resistance heating;Software architecture;Space stations;Measurement;Mechatronics;Real time processing;Software;Space},
}

@InProceedings{6269627,
  author    = {J. F. Ferreira and G. He and S. Qin},
  title     = {Automated Verification of the FreeRTOS Scheduler in HIP/SLEEK},
  booktitle = {2012 Sixth International Symposium on Theoretical Aspects of Software Engineering},
  year      = {2012},
  pages     = {51-58},
  month     = {July},
  abstract  = {Automated verification of operating system kernels is a challenging problem, partly due to the use of shared mutable data structures. In this paper, we show how we can automatically verify memory safety and functional correctness of the task scheduler component of the FreeRTOS kernel using the verification system HIP/SLEEK. We show how some of HIP/SLEEK features like user-defined predicates and lemmas make the specifications highly expressive and the verification process viable. To the best of our knowledge, this is the first code-level verification of memory safety and functional correctness properties of the FreeRTOS scheduler. The outcome of our experiment confirms that HIP/SLEEK can indeed be used to verify code that is used in production. Moreover, since the properties that we verify are quite general, we envisage that the same approach can be adopted to verify the scheduler of other operating systems.},
  db        = {IEEE},
  doi       = {10.1109/TASE.2012.45},
  keywords  = {data structures;formal verification;operating system kernels;scheduling;shared memory systems;FreeRTOS kernel;FreeRTOS scheduler;HIP-SLEEK verification system;automated operating system kernel verification;first code-level verification;functional correctness verification;memory safety verification;real-time operating systems;shared mutable data structures;task scheduler component;user-defined lemmas;user-defined predicates;Context;Data structures;Hip;Kernel;Safety;Shape;FreeRTOS;HIP/SLEEK;automated verification;embedded systems;operating systems;separation logic;task scheduler},
}

@Article{7374832,
  author   = {J. Herbert and S. Wilson and A. D. Rakic and T. Taimre},
  title    = {FPGA implementation of a high-speed, real-time, windowed standard deviation filter},
  journal  = {Electronics Letters},
  year     = {2016},
  volume   = {52},
  number   = {1},
  pages    = {22-23},
  issn     = {0013-5194},
  abstract = {Characterisation of the standard deviation of a time-series signal has uncommon, yet widespread applications. The usual requirement for a representation of signal standard deviation in real-time implies a high computation speed. A method based on a field programmable gate array (FPGA) implementation is presented. The technique is benchmarked against conventional computational approaches and shows a single windowed standard deviation update calculation of a 16 bit sample can be achieved in 11 ns on a modern CPU. The FPGA implementation is found to be superior to all other approaches examined with an operation time of below 10 ns, and thus provides a useful tool for the real-time measurement of the standard deviation of signals above 100 MHz.},
  db       = {IEEE},
  doi      = {10.1049/el.2015.2407},
  keywords = {field programmable gate arrays;filters;time series;FPGA;field programmable gate array;modern CPU;time-series signal;windowed standard deviation filter;word length 16 bit},
}

@InProceedings{5210958,
  author    = {F. Oldewurtel and J. Riihijarvi and K. Rerkrai and P. Mahonen},
  title     = {The RUNES Architecture for Reconfigurable Embedded and Sensor Networks},
  booktitle = {2009 Third International Conference on Sensor Technologies and Applications},
  year      = {2009},
  pages     = {109-116},
  month     = {June},
  abstract  = {We present the RUNES architecture for reconfigurable embedded networked systems and wireless sensor networks. It is the first systems-level architecture for such networks to explicitly deal with heterogeneity in hardware platforms, link-layer technologies and networking protocols while offering a simple programming language independent set of APIs together with a component-oriented middleware for the application developers to work on. The solutions developed are particularly appropriate for use in various emergency response scenarios, in which reconfigurability is often a key requirement. We also report on an example realisation of our architecture in a prototypical demonstration environment in a particular emergency scenario. The evaluation of architectural aspects such as reconfigurability shows that great programming flexibility can be achieved at low implementation overhead. The experience gained from RUNES modular architecture are very promising both in academic and industry projects context.},
  db        = {IEEE},
  doi       = {10.1109/SENSORCOMM.2009.26},
  keywords  = {embedded systems;intelligent sensors;wireless sensor networks;RUNES architecture;reconfigurable embedded networked systems;wireless sensor networks;Actuators;Computer architecture;Hardware;Middleware;Operating systems;Protocols;Sensor systems;Sensor systems and applications;System testing;Wireless sensor networks;architecture;programming model;prototype;sensor networks;software platform},
}

@InProceedings{6544384,
  author    = {A. Asaduzzaman and F. N. Sibai and S. Aramco and H. El-Sayed},
  title     = {Performance and power comparisons of MPI Vs Pthread implementations on multicore systems},
  booktitle = {2013 9th International Conference on Innovations in Information Technology (IIT)},
  year      = {2013},
  pages     = {1-6},
  month     = {March},
  abstract  = {The advancement of multicore systems demands applications with more threads. In order to facilitate this demand, parallel programming models such as message passing interface (MPI) are developed. By using such models, the execution time and the power consumption can be reduced significantly. However, the performance of MPI programming depends on the total number of threads and the number of processing cores in the system. In this work, we experimentally study the impact of Open MPI and POSIX Thread (Pthread) implementations on performance and power consumption of multicore systems. Data dependent (like heat conduction on 2D surface) and data independent (like matrix multiplication) applications are used with high performance hardware in the experiments. Simulation results suggest that both implementations of more threads running in a system with more cores have potential to reduce the execution time with negligible or little increase in total power consumption. It is observed that the performance of MPI implementation varies (due to the dynamic communication overhead among the processing cores).},
  db        = {IEEE},
  doi       = {10.1109/Innovations.2013.6544384},
  keywords  = {application program interfaces;matrix algebra;message passing;multiprocessing systems;parallel programming;power aware computing;Open MPI;POSIX thread;Pthread implementations;data dependent;dynamic communication;matrix multiplication;message passing interface;multicore systems;parallel programming models;performance comparisons;performance hardware;power comparisons;power consumption;processing cores;pthread implementations;Instruction sets;Message systems;Multicore processing;Parallel programming;Power demand;Supercomputers;Workstations;Open MPI;Pthread;data dependency;message passing interface;multicore architecture},
}

@InProceedings{6961841,
  author    = {Y. Bai and K. Schneider and N. Bhardwaj and B. Katti and T. Shazadi},
  title     = {From clock-driven to data-driven models},
  booktitle = {2014 Twelfth ACM/IEEE Conference on Formal Methods and Models for Codesign (MEMOCODE)},
  year      = {2014},
  pages     = {32-41},
  month     = {Oct},
  abstract  = {Clock/time-driven models are powerful abstractions of real-time systems, as e.g., provided by the synchronous models of computation which lend themselves well for simulation and verification. At every clock cycle, new inputs are read, computations are performed in zero-time, and results are immediately/synchronously communicated between components. However, such zero-time idealizations are not realistic since computation and communication finally takes time in implementations. For implementations, data-driven execution models have the advantage to impose no timing constraints other than arrival of input data, and thus, these models are perfectly suited for distributed or other kinds of asynchronous implementations. For this reason, modern model-based design flows consider the desynchronization of synchronous models for system synthesis which is possible for the subclass of endochronous systems only. While definitions of endochrony were considered for years, it is shown in this paper how to efficiently verify endochrony by SAT solving. Our procedure consists of two steps: In the first step, we introduce buffers to the interface of a clock-driven component, so that its inputs can arrive at different points of time. After this step, clocks of signals are viewed as `instructions' telling the component which input values have to be consumed for the current reaction.We call such components clock-scheduled. In the second step, we remove the clocks from the interface of the clock-scheduled components, so that the component may now become nondeterministic. We prove in this paper that a synchronous component is endochronous, if and only if the clock signals can be safely removed in this step without destroying determinism. Based on this result, we present a decision procedure based on symbolic system representations to check whether components are endochronous. Preliminary experimental results show the effectiveness of our method.},
  db        = {IEEE},
  doi       = {10.1109/MEMCOD.2014.6961841},
  keywords  = {computability;symbol manipulation;SAT solving;clock cycle;clock-driven models;clock-scheduled components;data-driven execution models;endochronous systems;model-based design flows;symbolic system representations;synchronous models;system synthesis;time-driven models;zero-time idealizations;Circuit synthesis;Clocks;Computational modeling;Distributed databases;Integrated circuit modeling;Semantics;Synchronization},
}

@Article{5492692,
  author   = {A. Bergel and W. Harrison and V. Cahill and S. Clarke},
  title    = {FlowTalk: Language Support for Long-Latency Operations in Embedded Devices},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2011},
  volume   = {37},
  number   = {4},
  pages    = {526-543},
  month    = {July},
  issn     = {0098-5589},
  abstract = {Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.},
  db       = {IEEE},
  doi      = {10.1109/TSE.2010.66},
  keywords = {C language;embedded systems;intelligent sensors;object-oriented languages;object-oriented programming;software engineering;wireless sensor networks;C language;FlowTalk;TinyOS;asynchronous long-latency operations;embedded sensor devices;language support;light-weight continuation mechanism;memory consumption;memory size 4 KByte;object-oriented programming language;programming language;sensor sampling;wireless sensor networks;Application software;Automotive engineering;Biosensors;Computer languages;Embedded software;Java;Object oriented modeling;Object oriented programming;Sampling methods;Wireless sensor networks;Embedded systems;object-based programming.},
}

@InProceedings{6651023,
  author    = {A. Branco and A. L. d. Moura and N. Rodriguez and S. Rossetto},
  title     = {Teaching Concurrent and Distributed Computing -- Initiatives in Rio de Janeiro},
  booktitle = {2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum},
  year      = {2013},
  pages     = {1318-1323},
  month     = {May},
  abstract  = {In this paper we describe two ongoing initiatives for teaching concurrency and distribution in PUC-Rio and UFRJ. One of them is a new approach for teaching distributed systems. Conventional distributed system courses follow a syllabus in which a list of topics is discussed independently and at different levels of abstractions. In Edupar'2012, we proposed a course with a novel approach, using a wireless sensor network environment to pin all topics down to concrete applications and to maintain issues such as fault tolerance and coordination continuously present. The second initiative is a smaller one, in which we insert a new topic in a Systems Software course to allow students to have a better understanding of what is application-level multitasking and of how it can be implemented. In this paper, we report on the experience of teaching the proposed syllabus and the adjustments that were necessary. We also discuss some plans for the courses in 2013.},
  db        = {IEEE},
  doi       = {10.1109/IPDPSW.2013.33},
  keywords  = {concurrency control;distributed processing;teaching;wireless sensor networks;application-level multitasking;concurrent computing;distributed computing;systems software course;teaching;wireless sensor network environment;Education;Fault tolerance;Fault tolerant systems;Materials;Programming;Proposals;Wireless sensor networks;application-level multitasking;coroutines;cross-cutting approaches;event-based programming;fault tolerance},
}

@InProceedings{7328218,
  author    = {A. Capotondi and A. Marongiu and L. Benini},
  title     = {Enabling Scalable and Fine-Grained Nested Parallelism on Embedded Many-cores},
  booktitle = {2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip},
  year      = {2015},
  pages     = {297-304},
  month     = {Sept},
  abstract  = {Current high-end embedded systems are designed as heterogeneous systems-on-chip (SoCs), where a general-purpose host processor is coupled to a programmable manycore accelerator (PMCA). Such PMCAs typically leverage hierarchical interconnect and distributed memory with non-uniform access (NUMA). Nested parallelism is a convenient programming abstraction for large-scale cc-NUMA systems, which allows to hierarchically (and dynamically) create multiple levels of fine-grained parallelism whenever it is available. Available implementations for cc-NUMA systems introduce large overheads for nested parallelism management, which cannot be tolerated due to the extremely fine-grained nature of embedded parallel workloads. In particular, creating a team of parallel threads has a cost that increases linearly with the number of threads, which is inherently non scalable. This work presents a software cache mechanism for frequently-used parallel team configurations to speed up parallel thread creation overheads in PMCA systems. When a configuration is found in the cache the cost for parallel team creation has a constant time, providing a scalable mechanism. We evaluated our support on the STMicroelectronics STHORM many-core. Compared to the state-of-the art, our solution shows that: i) the cost for parallel team creation is reduced by up to 67%, ii) the tangible effect on real ultra-fine-grained parallel kernels is a speedup of up to 80%.},
  db        = {IEEE},
  doi       = {10.1109/MCSoC.2015.47},
  keywords  = {cache storage;embedded systems;multiprocessing systems;parallel programming;system-on-chip;PMCA;STMicroelectronics STHORM many-core;SoC;distributed memory with nonuniform access;embedded many-cores;embedded systems;fine-grained parallelism;heterogeneous systems-on-chip;nested parallelism;parallel team configuration;parallel thread creation overhead;programmable manycore accelerator;programming abstraction;software cache mechanism;Fabrics;Instruction sets;Message systems;Parallel processing;Programming;Recruitment;Embedded Many-Core Architectures;OpenMP;Parallel Programming Models},
}

@InProceedings{7818327,
  author    = {G. Cedersjö and J. W. Janneck},
  title     = {Processes and actors: Translating Kahn processes to dataflow with firing},
  booktitle = {2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)},
  year      = {2016},
  pages     = {21-30},
  month     = {July},
  abstract  = {Dataflow programming is a paradigm for describing stream processing algorithms in a manner that naturally exposes their concurrency and makes the resulting programs readily implementable on highly parallel architectures. Dataflow programs are graph structured, with nodes representing computational kernels that process the data flowing over the edges. There are two major families of languages for the kernels: process languages and languages for dataflow with firing. While processes tend to be easier to write, the additional structure provided by the dataflow-with-firing style increases the analyzability of dataflow programs and supports more efficient implementation techniques. This paper seeks to combine these benefits in a principled manner by constructing a family of translations from a process language to dataflow with firing. In order to formally relate these descriptions, we first introduce a notion of firing to the semantics of Kahn processes, which allows us to give a precise definition of equivalence between programs written in these different styles. Then we introduce a family of translations between them and and show that they retain meaning of a program. The presented language and its translation has been implemented in a compiler for the dataflow programming language CAL.},
  db        = {IEEE},
  doi       = {10.1109/SAMOS.2016.7818327},
  keywords  = {concurrency control;data flow computing;parallel architectures;program compilers;CAL;Kahn process translation;compiler;computational kernels;dataflow programming language;dataflow-with-firing style;parallel architectures;process languages;stream processing algorithms;Computational modeling;Computer languages;Grammar;Ports (Computers);Programming;Semantics;Structural rings},
}

@InProceedings{7459504,
  author    = {D. Cesarini and A. Marongiu and L. Benini},
  title     = {An optimized task-based runtime system for resource-constrained parallel accelerators},
  booktitle = {2016 Design, Automation Test in Europe Conference Exhibition (DATE)},
  year      = {2016},
  pages     = {1261-1266},
  month     = {March},
  abstract  = {Manycore accelerators have recently proven a promising solution for increasingly powerful and energy efficient computing systems. This raises the need for parallel programming models capable of effectively leveraging hundreds to thousands of processors. Task-based parallelism has the potential to provide such capabilities, offering flexible support to fine-grained and irregular parallelism. However, efficiently supporting this programming paradigm on resource-constrained parallel accelerators is a challenging task. In this paper, we present an optimized implementation of the OpenMP tasking model for embedded parallel accelerators, discussing the key design solution that guarantee small memory (footprint) and minimize performance overheads. We validate our design by comparing to several state-of-the-art tasking implementations, using the most representative parallelization patterns. The experimental results confirm that our solution achieves near-ideal speedups for tasks as small as 5K cycles.},
  db        = {IEEE},
  keywords  = {embedded systems;multiprocessing systems;parallel programming;performance evaluation;OpenMP tasking model;embedded parallel accelerators;energy efficIent computing systems;fine-grained parallelism;irregular parallelism;manycore accelerators;optimized task-based runtime system;parallel programming models;performance overhead minimization;resource-constrained parallel accelerators;task-based parallelism;Context;Instruction sets},
}

@InProceedings{5694285,
  author    = {D. W. Chang and C. D. Jenkins and P. C. Garcia and S. Z. Gilani and P. Aguilera and A. Nagarajan and M. J. Anderson and M. A. Kenny and S. M. Bauer and M. J. Schulte and K. Compton},
  title     = {ERCBench: An Open-Source Benchmark Suite for Embedded and Reconfigurable Computing},
  booktitle = {2010 International Conference on Field Programmable Logic and Applications},
  year      = {2010},
  pages     = {408-413},
  month     = {Aug},
  abstract  = {Researchers in embedded and reconfigurable computing are often hindered by a lack of suitable benchmarks with which to accurately evaluate their work. Without a suitable benchmark suite, researchers use either outdated, unrealistic benchmarks or spend valuable time creating their own. In this paper, we present ERCBench - a freely-available, open-source benchmark suite geared towards embedded and reconfigurable computing research. ERCBench benchmarks represent a variety of application areas, including multimedia processing, wireless communications, and cryptography. They consist of synthesizable Verilog models for hardware accelerators and hybrid hardware/software applications that combine software-based control flow with hardware-based computation tasks.},
  db        = {IEEE},
  doi       = {10.1109/FPL.2010.85},
  issn      = {1946-147X},
  keywords  = {benchmark testing;hardware description languages;public domain software;reconfigurable architectures;ERCBench;Verilog model;embedded computing;hardware accelerator;hardware-based computation;open source benchmark suite;reconfigurable computing;software- based control flow;benchmarks;embedded computing;open-source;reconfigurable computing},
}

@InProceedings{6108279,
  author    = {C. Chise and I. Jurca},
  title     = {Hybrid Analytical-Simulation Model Used to Evaluate and Improve System Performance},
  booktitle = {2011 10th International Symposium on Parallel and Distributed Computing},
  year      = {2011},
  pages     = {240-246},
  month     = {July},
  abstract  = {Performance prediction has been intensively studied in the last decade, alongside the accelerated development of distributed systems. This paper focuses on a hybrid approach regarding model solving, combining two popular prediction techniques applied separately so far, analytical and simulation modeling, in order to benefit from the strengths of both. The input UML model with MARTE (Modeling and Analysis of Real-time and Embedded systems) annotations is transformed into a hierarchically decomposed performance model, and performance results for simulated sub models are used by an analytical solver. The validation of the proposed method is to be performed with a tool called PHYMSS (Performance Hybrid Model Solver and Simulator) developed by the authors that implements both the hybrid solver and a multithreaded simulator.},
  db        = {IEEE},
  doi       = {10.1109/ISPDC.2011.42},
  issn      = {2379-5352},
  keywords  = {Unified Modeling Language;distributed processing;performance evaluation;MARTE;PHYMSS;distributed systems;hybrid analytical-simulation model;input UML model;system performance;Analytical models;Computational modeling;Mathematical model;Predictive models;Servers;Throughput;Unified modeling language;LQN;hybrid model;simulation},
}

@InProceedings{5314042,
  author    = {D. L. Clark},
  title     = {Powering intelligent instruments with Lua scripting},
  booktitle = {2009 IEEE AUTOTESTCON},
  year      = {2009},
  pages     = {101-106},
  month     = {Sept},
  abstract  = {As the power of the integrated processors that control today's instruments continues to climb, instrument vendors will increasingly add features that allow users to utilize the added intelligence by embedding custom applications directly onboard the instrument. For the test, measurement and automation industries, this paradigm is a complement to, among other things, the advent of synthetic instruments that can ldquobe anything you want,rdquo the frequent use of mezzanine type hardware and the rise of the LXI specification in which instrument to instrument messaging allows one instrument to control and communicate with another without the necessity of a host PC. There are various approaches the instrument vendor can take to permit users to develop embedded applications to be run on the instrument processor. Arguably the most advantageous approach, to both the vendor and customer, is to embed a high level scripting language allowing the user to easily develop scripts to perform instrument based operations. The Lua scripting language is a compact, full featured scripting language that is easily portable and seamlessly integrates into embedded designs. Written in pure ISO ANSI-C, the Lua interpreter and Lua libraries have been successfully ported to a large number of platforms, big and small, and with and without advanced operating systems such as Windows and Linux. Lua contains an API for interfacing directly to and from the instrument's embedded firmware and includes a full suite of libraries. Further, Lua is extendable. Thus, in addition to embedding the language interpreter and libraries, the vendor can implement custom libraries and various other custom utilities to increase the flexibility of the system and enhance the capabilities of the user developed scripts. This paper studies the use of Lua in intelligent instrumentation. It discusses features that provide flexibility and power to users embedding applications onboard instruments and it presents some real wo- rld applications of the technology.},
  db        = {IEEE},
  doi       = {10.1109/AUTEST.2009.5314042},
  issn      = {1088-7725},
  keywords  = {Linux;application program interfaces;authoring languages;automatic test equipment;embedded systems;API;ATE system;ISO ANSI-C;LXI specification;Linux;Lua scripting language;Windows;automated test equipment;automation industries;custom libraries;custom utilities;embedded applications;full featured scripting language;high level scripting language;instrument based operation;instrument to instrument messaging;integrated processor;intelligent instrumentation;language interpreter;mezzanine type hardware;onboard instruments;Automatic control;Automatic testing;Automation;Communication industry;Hardware;ISO;Industrial control;Instruments;Libraries;Process control},
}

@InProceedings{4228186,
  author    = {M. Cohen and T. Ponte and S. Rossetto and N. Rodriguez},
  title     = {Using Coroutines for RPC in Sensor Networks},
  booktitle = {2007 IEEE International Parallel and Distributed Processing Symposium},
  year      = {2007},
  pages     = {1-8},
  month     = {March},
  abstract  = {This paper proposes a concurrency model which integrates the asynchronous and event-driven nature of wireless sensor networks with higher-level abstractions that provide a more familiar programming style for the developer. As a basis for this proposal, we designed and implemented a cooperative multitasking scheduler, based on coroutines, for the TinyOS operating system. We then used this scheduler to implement RPC-like interfaces that capture different communication patterns common in wireless sensor networks. This allows the programmer to work, when appropriate, with a synchronous style, while maintaining an asynchronous model at the message exchange level.},
  db        = {IEEE},
  doi       = {10.1109/IPDPS.2007.370458},
  issn      = {1530-2075},
  keywords  = {concurrency control;network operating systems;remote procedure calls;scheduling;wireless sensor networks;RPC coroutine;RPC-like interface;TinyOS operating system;concurrency model;cooperative multitasking scheduler;event-driven wireless sensor network;Computer languages;Concurrent computing;Embedded system;Multitasking;Operating systems;Programming profession;Proposals;Sensor phenomena and characterization;Testing;Wireless sensor networks},
}

@Article{6122018,
  author   = {J. Diaz and C. Muñoz-Caro and A. Niño},
  title    = {A Survey of Parallel Programming Models and Tools in the Multi and Many-Core Era},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2012},
  volume   = {23},
  number   = {8},
  pages    = {1369-1386},
  month    = {Aug},
  issn     = {1045-9219},
  abstract = {In this work, we present a survey of the different parallel programming models and tools available today with special consideration to their suitability for high-performance computing. Thus, we review the shared and distributed memory approaches, as well as the current heterogeneous parallel programming model. In addition, we analyze how the partitioned global address space (PGAS) and hybrid parallel programming models are used to combine the advantages of shared and distributed memory systems. The work is completed by considering languages with specific parallel support and the distributed programming paradigm. In all cases, we present characteristics, strengths, and weaknesses. The study shows that the availability of multi-core CPUs has given new impulse to the shared memory parallel programming approach. In addition, we find that hybrid parallel programming is the current way of harnessing the capabilities of computer clusters with multi-core nodes. On the other hand, heterogeneous programming is found to be an increasingly popular paradigm, as a consequence of the availability of multi-core CPUs+GPUs systems. The use of open industry standards like OpenMP, MPI, or OpenCL, as opposed to proprietary solutions, seems to be the way to uniformize and extend the use of parallel programming models.},
  db       = {IEEE},
  doi      = {10.1109/TPDS.2011.308},
  keywords = {distributed memory systems;parallel programming;shared memory systems;MPI;OpenCL;OpenMP;computer clusters;distributed memory approach;distributed memory systems;heterogeneous parallel programming model;high-performance computing;hybrid parallel programming model;multicore CPUs+GPUs systems;multicore nodes;open industry standards;partitioned global address space;shared memory approach;shared memory parallel programming;Computational modeling;Graphics processing unit;Instruction sets;Message systems;Multicore processing;Parallel programming;Parallelism and concurrency;distributed programming;heterogeneous (hybrid) systems.},
}

@InProceedings{7336309,
  author    = {W. B. Gardner and A. Gumtie and J. D. Carter},
  title     = {Supporting Selective Formalism in CSP++ with Process-Specific Storage},
  booktitle = {2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
  year      = {2015},
  pages     = {1057-1065},
  month     = {Aug},
  abstract  = {Communicating Sequential Processes (CSP) is a formal language whose primary purpose is to model and verify concurrent systems. The CSP++ toolset was created to realize the concept of selective formalism by making machine-readable CSPm specifications both executable (through automatic C++ code generation) and extensible (by allowing integration of C++ user-coded functions, UCFs). However, UCFs were limited by their inability to share data with each other, thus their application was constrained to solving simple problems in isolation. We extend CSP++ by providing UCFs in the same CSP process with safe access to a shared storage area, similar in concept and API to Pthreads' thread-local storage, enabling cooperation between them and granting them the ability to undertake more complex tasks without breaking the formalism of the underlying specification. Process-specific storage is demonstrated with a line-following robot case study, applying CSP++ in a soft real-time system. Also described is the Eclipse plug-in that supports the CSPm design flow.},
  db        = {IEEE},
  doi       = {10.1109/HPCC-CSS-ICESS.2015.265},
  keywords  = {C++ language;application program interfaces;communicating sequential processes;concurrency (computers);control engineering computing;formal languages;formal specification;formal verification;program compilers;real-time systems;robots;storage management;API;C++ user-coded function;CSP++;CSPm design flow;Eclipse plug-in;Pthread thread-local storage;UCF;automatic C++ code generation;concurrent system modelling;concurrent system verification;formal language;line-following robot case study;machine-readable CSPm specification;process-specific storage;selective formalism;soft real-time system;Libraries;Real-time systems;Robot sensing systems;Switches;System recovery;Writing;C++;CSPm;Eclipse;Timed CSP;code generation;embedded systems;formal methods;model-based design;selective formalism;soft real-time;software synthesis},
}

@InProceedings{6962305,
  author    = {E. Gebrewahid and M. Yang and G. Cedersjö and Z. U. Abdin and V. Gaspes and J. W. Janneck and B. Svensson},
  title     = {Realizing Efficient Execution of Dataflow Actors on Manycores},
  booktitle = {2014 12th IEEE International Conference on Embedded and Ubiquitous Computing},
  year      = {2014},
  pages     = {321-328},
  month     = {Aug},
  abstract  = {Embedded DSP computing is currently shifting towards manycore architectures in order to cope with the ever growing computational demands. Actor based dataflow languages are being considered as a programming model. In this paper we present a code generator for CAL, one such dataflow language. We propose to use a compilation tool with two intermediate representations. We start from a machine model of the actors that provides an ordering for testing of conditions and firing of actions. We then generate an Action Execution Intermediate Representation that is closer to a sequential imperative language like C and Java. We describe our two intermediate representations and show the feasibility and portability of our approach by compiling a CAL implementation of the Two-Dimensional Inverse Discrete Cosine Transform on a general purpose processor, on the Epiphany manycore architecture and on the Ambric massively parallel processor array.},
  db        = {IEEE},
  doi       = {10.1109/EUC.2014.55},
  keywords  = {data flow computing;digital signal processing chips;discrete cosine transforms;embedded systems;inverse transforms;multiprocessing systems;parallel processing;program compilers;program processors;Ambric massively parallel processor array;C language;CAL;CAL implementation;Epiphany manycore architecture;Java language;action execution intermediate representation;code generator;computational demands;dataflow actor execution;dataflow languages;embedded DSP computing;general purpose processor;machine model;manycore architectures;programming model;sequential imperative language;two-dimensional inverse discrete cosine transform;Arrays;Availability;Computational modeling;Optimization;Ports (Computers);Programming;Switches;CAL;code generation;compilation framework;dataflow languages;manycore},
}

@InProceedings{6069480,
  author    = {B. Haetzer and M. Radetzki},
  title     = {A case study on message-based discrete event simulation for Transaction Level Modeling},
  booktitle = {FDL 2011 Proceedings},
  year      = {2011},
  pages     = {1-8},
  month     = {Sept},
  abstract  = {Transaction Level Modeling is a system-level design methodology for early design space exploration. The increasing complexity of systems makes it necessary to improve simulation performance. Using parallel discrete event simulation approaches seems promising to speedup the simulation runs of complex transaction level models. One of such approaches is the message-based PDES approach which is applied to TLM in this paper. Two different TLM case studies are used to evaluate and compare the execution times of sequential and message-based simulation. We show that with message-based simulation a speedup over sequential simulation can be achieved even if using only one processor core. This result lies the foundation for further speedup if running on multiple cores as no more significant synchronization overhead is expected.},
  db        = {IEEE},
  issn      = {1636-9874},
  keywords  = {computational complexity;discrete event simulation;electronic engineering computing;system-on-chip;design space exploration;message based PDES;message based discrete event simulation;multiple cores;parallel discrete event simulation;processor core;sequential simulation;system level design methodology;systems complexity;transaction level modeling;Computational modeling;Context;Kernel;Switches;Time domain analysis;Time varying systems},
}

@InProceedings{6513574,
  author    = {R. von Hanxleden and M. Mendler and J. Aguado and B. Duderstadt and I. Fuhrmann and C. Motika and S. Mercer and O. O'Brien},
  title     = {Sequentially constructive concurrency A conservative extension of the synchronous model of computation},
  booktitle = {2013 Design, Automation Test in Europe Conference Exhibition (DATE)},
  year      = {2013},
  pages     = {581-586},
  month     = {March},
  abstract  = {Synchronous languages ensure deterministic concurrency, but at the price of heavy restrictions on what programs are considered valid, or constructive. Meanwhile, sequential languages such as C and Java offer an intuitive, familiar programming paradigm but provide no guarantees with regard to deterministic concurrency. The sequentially constructive model of computation (SC MoC) presented here harnesses the synchronous execution model to achieve deterministic concurrency while addressing concerns that synchronous languages are unnecessarily restrictive and difficult to adopt. In essence, the SC MoC extends the classical synchronous MoC by allowing variables to be read and written in any order as long as sequentiality expressed in the program provides sufficient scheduling information to rule out race conditions. The SC MoC is a conservative extension in that programs considered constructive in the common synchronous MoC are also SC and retain the same semantics. In this paper, we identify classes of variable accesses, define sequential constructiveness based on the concept of SC-admissible scheduling, and present a priority-based scheduling algorithm for analyzing and compiling SC programs.},
  db        = {IEEE},
  doi       = {10.7873/DATE.2013.128},
  issn      = {1530-1591},
  keywords  = {Computational modeling;Concurrent computing;Electronic mail;Instruction sets;Java;Programming;Schedules},
}

@Article{6403642,
  author   = {M. Hosseinabady and J. l. Nunez-Yanez},
  title    = {Fast and low overhead architectural transaction level modelling for large-scale network-on-chip simulation},
  journal  = {IET Computers Digital Techniques},
  year     = {2012},
  volume   = {6},
  number   = {6},
  pages    = {384-395},
  month    = {November},
  issn     = {1751-8601},
  abstract = {Early system modelling is an essential tool to accelerate software development, architectural analysis and hardware verification in complex many-core system-on-chips (SoCs). Transaction level modelling (TLM) offers a higher level of abstraction than register transfer level (RTL) and can be used for early system modelling. Maintaining simulation speed with the right accuracy is a major challenge and this paper proposes SystemC-based architectural modelling techniques that extend TLM to deliver faster simulation models for many-core system. The proposed approach considers a micro-scheduler for large modules (in the sense of SystemC modules) to locally manage all events in the module. Exploiting this micro-scheduler along with function object and coroutine concepts, the authors propose a lightweight thread process that significantly reduces the context switching overhead among the different processes. Additionally the micro-scheduler allows some processes to be run ahead of simulation time. The proposed techniques are applied to the model of a very large networks-on-chip (NoC) formed by thousands of cores stressing the simulation capabilities of the host computer and operating system. The experimental results demonstrate that the model can run successfully and exhibits up to 93% improvement in simulation speed compared to traditional SystemC-based modelling.},
  db       = {IEEE},
  doi      = {10.1049/iet-cdt.2012.0001},
  keywords = {electronic engineering computing;integrated circuit modelling;network-on-chip;operating systems (computers);software engineering;system-on-chip;transaction processing;RTL;SystemC-based architectural modelling techniques;architectural analysis;coroutine concepts;function object concepts;hardware verification;host computer;large-scale NoC;large-scale network-on-chip simulation;low overhead architectural transaction level modelling;many-core SoC;many-core system-on-chips;microscheduler;operating system;register transfer level;software development},
}

@InProceedings{4570792,
  author    = {P. K. Huang and M. Hashemi and S. Ghiasi},
  title     = {System-Level Performance Estimation for Application-Specific MPSoC Interconnect Synthesis},
  booktitle = {2008 Symposium on Application Specific Processors},
  year      = {2008},
  pages     = {95-100},
  month     = {June},
  abstract  = {We present a framework for development of streaming applications as concurrent software modules running on multi-processors system-on-chips (MPSoC). We propose an iterative design space exploration mechanism to customize MPSoC architecture for given applications. Central to the exploration engine is our system-level performance estimation methodology, that both quickly and accurately determine quality of candidate architectures. We implemented a number of streaming applications on candidate architectures that were emulated on an FPGA. Hardware measurements show that our system-level performance estimation method incurs only 15% error in predicting application throughput. More importantly, it always correctly guides design space exploration by achieving 100% fidelity in quality-ranking candidate architectures. Compared to behavioral simulation of compiled code, our system-level estimator runs more than 12 times faster, and requires 7 times less memory.},
  db        = {IEEE},
  doi       = {10.1109/SASP.2008.4570792},
  keywords  = {field programmable gate arrays;multiprocessing systems;parallel architectures;system-on-chip;FPGA;MPSoC interconnect synthesis;design space exploration;system-level performance estimation;Application software;Computational modeling;Computer architecture;Field programmable gate arrays;Hardware;Network synthesis;Software performance;Space exploration;System-on-a-chip;Throughput},
}

@InProceedings{4292873,
  author    = {M. Karpinski and V. Cahill},
  title     = {High-Level Application Development is Realistic for Wireless Sensor Networks},
  booktitle = {2007 4th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks},
  year      = {2007},
  pages     = {610-619},
  month     = {June},
  abstract  = {Programming wireless sensor network (WSN) applications is known to be a difficult task. Part of the problem is that the resource limitations of typical WSN nodes force programmers to use relatively low-level techniques to deal with the logical concurrency and asynchronous event handling inherent in these applications. In addition, existing general-purpose, node-level programming tools only support the networked nature of WSN applications in a limited way and result in application code that is hardly portable across different software platforms. All of this makes programming a single device a tedious and error-prone task. To address these issues we propose a high-level programming model that allows programmers to express applications as hierarchical state machines and to handle events and application concurrency in a way similar to imperative synchronous languages. Our program execution model is based on static scheduling what allows for standalone application analysis and testing. For deployment, the resulting programs are translated into efficient sequential C code. A prototype compiler for TinyOS has been implemented and its evaluation in described in this paper.},
  db        = {IEEE},
  doi       = {10.1109/SAHCN.2007.4292873},
  issn      = {2155-5486},
  keywords  = {C++ language;wireless sensor networks;TinyOS;asynchronous event handling;hierarchical state machines;high-level application development;high-level programming model;logical concurrency;node-level programming tools;nodes force programmers;resource limitations;sequential C code;software platforms;synchronous languages;wireless sensor networks;Application software;Computer languages;Computer science;Concurrent computing;Hardware;Operating systems;Peer to peer computing;Programming profession;Prototypes;Wireless sensor networks},
}

@InProceedings{5775121,
  author    = {R. S. Khaligh and M. Radetzki},
  title     = {A dynamic load balancing method for parallel simulation of accuracy adaptive TLMs},
  booktitle = {2010 Forum on Specification Design Languages (FDL 2010)},
  year      = {2010},
  pages     = {1-6},
  month     = {Sept},
  abstract  = {In this paper we present a load balancing method for parallel simulation of accuracy adaptive transaction level models. In contrast to traditional fixed accuracy TLMs, timing accuracy of adaptive TLMs changes during simulation. This makes the computation and synchronization characteristics of the models variable, and practically prohibits the use of static load balancing. To deal with this issue, we present a light-weight load balancing method which takes advantage of, and can be easily incorporated with the simulation time synchronization scheme used in parallel TLM simulation. We have developed a high performance parallel simulation kernel based on the proposed method, and our experiments using the developed kernel show the effectiveness of the proposed approach in a realistic scenario.},
  db        = {IEEE},
  doi       = {10.1049/ic.2010.0141},
  keywords  = {discrete event simulation;resource allocation;accuracy adaptive TLM;dynamic load balancing method;high performance parallel simulation kernel;parallel simulation;simulation time synchronization scheme},
}

@InProceedings{4725258,
  author    = {M. Khezri and M. A. Sarram and F. Adibniya},
  title     = {Simplifying Concurrent Programming of Networked Embedded Systems},
  booktitle = {2008 IEEE International Symposium on Parallel and Distributed Processing with Applications},
  year      = {2008},
  pages     = {993-998},
  month     = {Dec},
  abstract  = {TinyOS is the current state of the art in operating systems for sensor network research. Event- based programming model of TinyOS presents concept of Task to allow postponing processing. For little processing and memory overhead and to avoid race conditions, tasks are non-preemptive. This causes executing long running task reduce system responsiveness. In general two approaches suggested for solving this problem: cooperative and multithreaded multitasking. In this paper we propose a new TinyOS task scheduler to integrate these approaches with new type of tasks. We argue that this approach improves the overall system responsiveness without concerning about data races or complicate programming for developers.},
  db        = {IEEE},
  doi       = {10.1109/ISPA.2008.138},
  issn      = {2158-9178},
  keywords  = {multi-threading;operating systems (computers);processor scheduling;telecommunication computing;wireless sensor networks;TinyOS;concurrent programming;cooperative multitasking;event-based programming model;memory overhead;multithreaded multitasking;networked embedded systems;operating systems;postponing processing;sensor network;task scheduler;Delay;Embedded system;Job shop scheduling;Multitasking;Operating systems;Programming profession;Sensor systems;Sensor systems and applications;Wireless sensor networks;Yarn;Cooperative;Embedded System;Multitasking;Scheduler;TinyOS;Yield},
}

@InProceedings{4394196,
  author    = {M. Koutsoubelias and S. Lalis},
  title     = {Design and Implementation of an Extensible Architecture for the Efficient Remote Access of Simple RFID-Readers},
  booktitle = {2007 IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications},
  year      = {2007},
  pages     = {1-5},
  month     = {Sept},
  abstract  = {This paper describes a software architecture for the remote monitoring of warehouses equipped with simple RFID reader devices. Its main design objective is to enable a flexible and efficient integration of resource constrained readers that may be implemented as low-cost embedded systems, while allowing higher-level middleware components to access them in a transparent way. The proposed design has been implemented in a Linux-based environment and is currently being tested in conjunction with early prototypes of simple RFID readers that are accessed over low-bandwidth.},
  db        = {IEEE},
  doi       = {10.1109/PIMRC.2007.4394196},
  issn      = {2166-9570},
  keywords  = {embedded systems;radiofrequency identification;software architecture;telecontrol;warehouse automation;efficient remote access;extensible architecture;higher-level middleware components;low-cost embedded systems;simple RFID-readers;software architecture;Computer architecture;Hardware;Information filtering;Information filters;Middleware;Mobile communication;Protocols;Radiofrequency identification;Remote monitoring;Testing},
}

@InProceedings{4400379,
  author    = {A. Krystosik},
  title     = {Model Checking in Concurrent Programming Teaching},
  booktitle = {EUROCON 2007 - The International Conference on "Computer as a Tool"},
  year      = {2007},
  pages     = {2390-2396},
  month     = {Sept},
  abstract  = {The paper presents an application of model checking in teaching a concurrent programming. The success of this approach is based on features of DT-CSM automata and EMLAN modeling language. EMLAN is a C-like, high level language for modeling and model checking of embedded systems. The language is equipped with a lot of synchronization mechanisms (semaphores, mutexes, monitors). This allows easy modeling and verification of concurrent, cooperating tasks, which is very useful for educational purposes. As an example, a typical student exercise: a producer-consumer is given.},
  db        = {IEEE},
  doi       = {10.1109/EURCON.2007.4400379},
  keywords  = {computer science education;distributed programming;embedded systems;finite state machines;program verification;specification languages;synchronisation;DT-CSM automata;EMLAN C-like high level language;EMLAN modeling language;concurrent programming teaching;embedded system model checking;synchronization mechanisms;Automata;Computer science;Concurrent computing;Education;Embedded system;Error correction;Information technology;Logic;Paper technology;Programming profession;Modeling;Software verification and validation;Teaching},
}

@InProceedings{6575497,
  author    = {P. Kugler and P. Nordhus and B. Eskofier},
  title     = {Shimmer, Cooja and Contiki: A new toolset for the simulation of on-node signal processing algorithms},
  booktitle = {2013 IEEE International Conference on Body Sensor Networks},
  year      = {2013},
  pages     = {1-6},
  month     = {May},
  abstract  = {Wearable sensors are widely used for data collection in many applications. Ssensor nodes have also been applied for real-time applications, e.g. for ECG analysis or activity and fall detection. Processing of the sensor data is either done on an external device or on the node itself. While on-node processing reduces data rate and increases battery life, development and testing can be time-consuming. To allow faster implementation of such algorithms, we propose a simulation framework for the Shimmer platform using the Cooja simulator, MSPSim and the Contiki operating system. We provide the simulator and example applications compatible with the ShimmerConnect protocol, allowing streaming of raw and pre-processed sensor data to MATLAB, LabView and Android. Additionally, a simple activity and fall detection algorithm was implemented on the sensor node and evaluated using both the simulator and real hardware. In the future this will allow rapid development and testing of on-node pre-processing algorithms.},
  db        = {IEEE},
  doi       = {10.1109/BSN.2013.6575497},
  issn      = {2376-8886},
  keywords  = {Bluetooth;Hardware;Operating systems;Sensors;Testing;Wireless communication;Wireless sensor networks},
}

@Article{6733370,
  author   = {W. B. Langdon and M. Harman},
  title    = {Optimizing Existing Software With Genetic Programming},
  journal  = {IEEE Transactions on Evolutionary Computation},
  year     = {2015},
  volume   = {19},
  number   = {1},
  pages    = {118-135},
  month    = {Feb},
  issn     = {1089-778X},
  abstract = {We show that the genetic improvement of programs (GIP) can scale by evolving increased performance in a widely-used and highly complex 50000 line system. Genetic improvement of software for multiple objective exploration (GISMOE) found code that is 70 times faster (on average) and yet is at least as good functionally. Indeed, it even gives a small semantic gain.},
  db       = {IEEE},
  doi      = {10.1109/TEVC.2013.2281544},
  keywords = {genetic algorithms;software engineering;GIP;GISMOE;genetic improvement of programs;genetic improvement of software for multiple objective exploration;genetic programming;software optimization;Complexity theory;DNA;Genetic programming;Grammar;Semantics;Software;${\rm Bowtie2}^{GP}$;Automatic software reengineering;genetic programming (GP);multiple objective exploration;search based software engineering (SBSE)},
}

@InProceedings{6651056,
  author    = {J. D. Leidel and J. Bolding and G. Rogers},
  title     = {Toward a Scalable Heterogeneous Runtime System for the Convey MX Architecture},
  booktitle = {2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum},
  year      = {2013},
  pages     = {1597-1606},
  month     = {May},
  abstract  = {Given the recent advent of the multicore era [1], research efforts in the area of high performance, low latency runtime systems have increased significantly. This research has given birth to new techniques in low-overhead scheduling techniques, small-memory footprint parallel execution units and kernel-free contextual environments. This paper presents a framework and runtime system for a truly heterogeneous approach to low-latency, high performance runtime techniques on the Convey MX-100 platform and CHOMP micro-architecture [14]. This framework, deemed the Convey Lightweight Runtime [CLR], is designed to provide high performance, programming-model agnostic parallel library support to the massively parallel CHOMP infrastructure. This work explores the fundamental design requirements and implementation details behind constructing the CLR system as a truly heterogeneous low-level runtime system for a wide array of parallel programming model targets.},
  db        = {IEEE},
  doi       = {10.1109/IPDPSW.2013.18},
  keywords  = {multiprocessing systems;parallel programming;scheduling;CHOMP micro-architecture;CLR system;convey MX architecture;convey lightweight runtime;kernel-free contextual environments;low-overhead scheduling techniques;multicore era;parallel CHOMP infrastructure;parallel programming model;scalable heterogeneous runtime system;small-memory footprint parallel execution units;Computer architecture;Coprocessors;Hardware;Instruction sets;Programming;Registers;Runtime;architecture;heterogeneous;instruction set architecture;multithreading;runtime;scheduling;tasking;work stealing},
}

@InProceedings{6176441,
  author    = {A. Marongiu and P. Burgio and L. Benini},
  title     = {Fast and lightweight support for nested parallelism on cluster-based embedded many-cores},
  booktitle = {2012 Design, Automation Test in Europe Conference Exhibition (DATE)},
  year      = {2012},
  pages     = {105-110},
  month     = {March},
  abstract  = {Several recent many-core accelerators have been architected as fabrics of tightly-coupled shared memory clusters. A hierarchical interconnection system is used - with a crossbar-like medium inside each cluster and a network-on-chip (NoC) at the global level - which make memory operations non-uniform (NUMA). Nested parallelism represents a powerful programming abstraction for these architectures, where a first level of parallelism can be used to distribute coarse-grained tasks to clusters, and additional levels of fine-grained parallelism can be distributed to processors within a cluster. This paper presents a lightweight and highly optimized support for nested parallelism on cluster-based embedded many-cores. We assess the costs to enable multi-level parallelization and demonstrate that our techniques allow to extract high degrees of parallelism.},
  db        = {IEEE},
  doi       = {10.1109/DATE.2012.6176441},
  issn      = {1530-1591},
  keywords  = {embedded systems;network-on-chip;shared memory systems;NUMA;NoC;cluster based embedded manycores;fine grained parallelism;hierarchical interconnection system;manycore accelerators;memory operations nonuniform;nested parallelism;network-on-chip;programming abstraction;shared memory clusters;Arrays;Instruction sets;Parallel processing;Programming;Synchronization},
}

@Article{6242797,
  author   = {C. J. Martin-Arguedas and D. Romero-Laorden and O. Martinez-Graullera and M. Perez-Lopez and L. Gomez-Ullate},
  title    = {An ultrasonic imaging system based on a new SAFT approach and a GPU beamformer},
  journal  = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  year     = {2012},
  volume   = {59},
  number   = {7},
  pages    = {1402-1412},
  month    = {July},
  issn     = {0885-3010},
  abstract = {The design of newer ultrasonic imaging systems attempts to obtain low-cost, small-sized devices with reduced power consumption that are capable of reaching high frame rates with high image quality. In this regard, synthetic aperture techniques have been very useful. They reduce hardware requirements and accelerate information capture. However, the beamforming process is still very slow, limiting the overall speed of the system. Recently, general-purpose computing on graphics processing unit techniques have been proposed as a way to accelerate image composition. They provide excellent computing power with which a very large volume of data can easily and quickly be processed. This paper describes a new system architecture that merges both principles. Thus, using a minimum-redundancy synthetic aperture technique to acquire the signals (2R-SAFT), and a graphics processing unit as a beamformer, we have developed a new scanner with full dynamic focusing, both on emission and reception, that attains real-time imaging with very few resources.},
  db       = {IEEE},
  doi      = {10.1109/TUFFC.2012.2341},
  keywords = {array signal processing;computerised instrumentation;graphics processing units;signal detection;ultrasonic imaging;2R-SAFT approach;GPU beamforming processing;dynamic focusing;general-purpose computing;graphics processing unit technique;hardware requirement;image composition acceleration;image quality;information capture acceleration;minimum-redundancy synthetic aperture technique;power consumption;real-time imaging;signal acquisition;ultrasonic imaging system;Apertures;Array signal processing;Focusing;Graphics processing unit;Hardware;Ultrasonic imaging;0},
}

@InProceedings{6913222,
  author    = {C. Motika and R. von Hanxleden and M. Heinold},
  title     = {Programming deterministic reactive systems with Synchronous Java},
  booktitle = {16th IEEE International Symposium on Object/component/service-oriented Real-time distributed Computing (ISORC 2013)},
  year      = {2013},
  pages     = {1-8},
  month     = {June},
  abstract  = {A key issue in the development of reliable embedded software is the proper handling of reactive control-flow, which typically involves concurrency. Java and its thread concept have only limited provisions for implementing deterministic concurrency. Thus, as has been observed in the past, it is challenging to develop concurrent Java programs without any deadlocks or race conditions. To alleviate this situation, the Synchronous Java (SJ) approach presented here adopts the key concepts that have been established in the world of synchronous programming for handling reactive control-flow. Thus SJ not only provides deterministic concurrency, but also different variants of deterministic preemption. Furthermore SJ allows concurrent threads to communicate with Esterel-style signals. As a case study for an embedded system usage, we also report on how the SJ concepts have been ported to the ARM-based Lego Mindstorms NXT system.},
  db        = {IEEE},
  doi       = {10.1109/ISORC.2013.6913222},
  issn      = {1555-0885},
  keywords  = {Java;concurrency control;embedded systems;object-oriented programming;ARM-based Lego Mindstorms NXT system;Esterel-style signals;Synchronous Java;concurrent Java programs;deterministic concurrency;deterministic preemption;deterministic reactive systems programming;embedded software development;embedded system usage;reactive control-flow handling;synchronous programming;Concurrent computing;Instruction sets;Java;Monitoring;Real-time systems;Switches;Synchronization},
}

@Article{4100760,
  author   = {S. Pasricha and N. D. Dutt},
  title    = {A Framework for Cosynthesis of Memory and Communication Architectures for MPSoC},
  journal  = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year     = {2007},
  volume   = {26},
  number   = {3},
  pages    = {408-420},
  month    = {March},
  issn     = {0278-0070},
  abstract = {Memory and communication architectures have a significant impact on the cost, performance, and time-to-market of complex multiprocessor system-on-chip (MPSoC) designs. The memory architecture dictates most of the data traffic flow in a design, which in turn influences the design of the communication architecture. Thus, there is a need to cosynthesize the memory and communication architectures to avoid making suboptimal design decisions. This is in contrast to traditional platform-based design approaches where memory and communication architectures are synthesized separately. In this paper, the authors propose an automated application-specific cosynthesis framework for memory and communication architecture (COSMECA) in MPSoC designs. The primary objective is to design a communication architecture having the least number of buses, which satisfies performance and memory-area constraints, while the secondary objective is to reduce the memory-area cost. Results of applying COSMECA to several industrial strength MPSoC applications from the networking domain indicate a saving of as much as 40% in number of buses and 29% in memory area compared to the traditional approach},
  db       = {IEEE},
  doi      = {10.1109/TCAD.2006.884487},
  keywords = {high level synthesis;integrated circuit design;integrated memory circuits;memory architecture;multiprocessing systems;system buses;system-on-chip;COSMECA;automated application-specific cosynthesis framework;communication architectures;complex multiprocessor system-on-chip designs;data traffic flow;memory architectures;Bandwidth;Costs;Digital systems;High level synthesis;Libraries;Memory architecture;Multiprocessing systems;Network synthesis;System performance;Time to market;Communication system performance;digital systems;high-level synthesis;memory architecture},
}

@Article{4135373,
  author   = {H. D. Patel and S. K. Shukla and R. A. Bergamaschi},
  title    = {Heterogeneous Behavioral Hierarchy Extensions for SystemC},
  journal  = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year     = {2007},
  volume   = {26},
  number   = {4},
  pages    = {765-780},
  month    = {April},
  issn     = {0278-0070},
  abstract = {System level design methodology and language support for high-level modeling enhances productivity for designing complex embedded systems. For an effective methodology, efficiency of simulation and a sound refinement-based implementation path are also necessary. Although some of the recent system level design languages (SLDLs) such as SystemC, SystemVerilog, or SpecC have features for system level abstractions, several essential ingredients are missing from these. We consider: 1) explicit support for multiple models of computation (MoCs) or heterogeneity so that distributed reactive embedded systems with hardware and software components can be easily modeled; 2) the ability to build complex behaviors by hierarchically composing simpler behaviors and the ability to distinguish between structural and heterogeneous behavioral hierarchy; and 3) hierarchical composition of behaviors that belong to distinct MoCs, as essential for successful SLDLs. One important requirement for such an SLDL should be that the simulation semantics are compositional, and hence no flattening of hierarchically composed behaviors are needed for simulation. In this paper, we show how we designed SystemC extensions to facilitates for heterogeneous behavioral hierarchy, compositional simulation semantics, and a simulation kernel that shows up to 40% more efficient than standard SystemC simulation},
  db       = {IEEE},
  doi      = {10.1109/TCAD.2006.884859},
  keywords = {circuit simulation;embedded systems;high level synthesis;logic design;specification languages;SystemC;behavioral decomposition;compositional simulation semantics;distributed reactive embedded systems;heterogeneous behavioral hierarchy;hierarchical finite state machine;hierarchical synchronous data flow;high-level modeling;language support;multiple models of computation;simulation kernel;structural modeling;system level abstractions;system level design;Circuit simulation;Computational modeling;Design methodology;Embedded computing;Embedded software;Embedded system;Hardware design languages;Productivity;Software systems;System-level design;Behavioral decomposition;SystemC;behavioral modeling;embedded system design;heterogeneous behavioral hierarchy;hierarchical finite state machine (HFSM);hierarchical synchronous data flow (SDF);models of computation (MoCs);simulation efficiency;structural modeling;system level designs},
}

@InProceedings{6164943,
  author    = {S. Roloff and F. Hannig and J. Teich},
  title     = {Approximate time functional simulation of resource-aware programming concepts for heterogeneous MPSoCs},
  booktitle = {17th Asia and South Pacific Design Automation Conference},
  year      = {2012},
  pages     = {187-192},
  month     = {Jan},
  abstract  = {The design and the programming of heterogeneous future MPSoCs including thousands of processor cores is a hard challenge. Means are necessary to program and simulate the dynamic behavior of such systems in order to dimension the hardware design and to verify the software functionality as well as performance goals. Cycle-accurate simulation of multiple parallel applications simultaneously running on different cores of the architecture would be much too slow and is not the desired level of detail. In this paper, we therefore present a novel high-level simulation approach which tackles the complexity and the heterogeneity of such systems and enables the investigation of a new computing paradigm called invasive computing. Here, the workload and its distribution are not known at compile-time but are highly dynamic and have to be adapted to the status (load, temperature, etc.) of the underlying architecture at run-time. We propose an approach for the modeling of tiled MPSoC architectures and the simulation of resource-aware programming concepts on these. This approach delivers important timing information about the parallel execution and also is taking into account the computational properties of possibly different types of cores.},
  db        = {IEEE},
  doi       = {10.1109/ASPDAC.2012.6164943},
  issn      = {2153-6961},
  keywords  = {Computational modeling;Computer architecture;Programming;Reduced instruction set computing;Synchronization;Tiles},
}

@InProceedings{6910501,
  author    = {S. Savas and E. Gebrewahid and Z. Ul-Abdin and T. Nordström and M. Yang},
  title     = {An evaluation of code generation of dataflow languages on manycore architectures},
  booktitle = {2014 IEEE 20th International Conference on Embedded and Real-Time Computing Systems and Applications},
  year      = {2014},
  pages     = {1-9},
  month     = {Aug},
  abstract  = {Today computer architectures are shifting from single core to manycores due to several reasons such as performance demands, power and heat limitations. However, shifting to manycores results in additional complexities, especially with regard to efficient development of applications. Hence there is a need to raise the abstraction level of development techniques for the manycores while exposing the inherent parallelism in the applications. One promising class of programming languages is dataflow languages and in this paper we evaluate and optimize the code generation for one such language, CAL. We have also developed a communication library to support the intercore communication. The code generation can target multiple architectures, but the results presented in this paper is focused on Adapteva's many core architecture Epiphany. We use the two-dimensional inverse discrete cosine transform (2D-IDCT) as our benchmark and compare our code generation from CAL with a hand-written implementation developed in C. Several optimizations in the code generation as well as in the communication library are described, and we have observed that the most critical optimization is reducing the number of external memory accesses. Combining all optimizations we have been able to reduce the difference in execution time between auto-generated and handwritten implementations from a factor of 4.3× down to a factor of only 1.3×.},
  db        = {IEEE},
  doi       = {10.1109/RTCSA.2014.6910501},
  issn      = {2325-1271},
  keywords  = {computer architecture;data flow computing;multiprocessing systems;program compilers;2D inverse discrete cosine transform;2D-IDCT;code generation;communication library;computer architectures;dataflow languages;external memory accesses;heat limitations;intercore communication;manycore architectures;programming languages;Generators;Libraries;Multicore processing;Ports (Computers);Program processors;Programming;2D-IDCT;Actor Machine;Dataflow Languages;Epiphany;Manycore;code generation;evaluation},
}

@InProceedings{7160118,
  author    = {M. Schoeberl and R. B. Sørensen and J. Sparsø},
  title     = {Models of Communication for Multicore Processors},
  booktitle = {2015 IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops},
  year      = {2015},
  pages     = {9-16},
  month     = {April},
  abstract  = {To efficiently use multicore processors we need to ensure that almost all data communication stays on chip, i.e., The bits moved between tasks executing on different processor cores do not leave the chip. Different forms of on-chip communication are supported by different hardware mechanism, e.g., Shared caches with cache coherency protocols, core-to-core networks-on-chip, and shared scratchpad memories. In this paper we explore the different hardware mechanism for on-chip communication and how they support or favor different models of communication. Furthermore, we discuss the usability of the different models of communication for real-time systems.},
  db        = {IEEE},
  doi       = {10.1109/ISORCW.2015.57},
  keywords  = {data communication;multiprocessing systems;core-to-core networks-on-chip;data communication modelling;multicore processors;on-chip communication;processor cores;real-time systems;shared caches;shared scratchpad memories;Computational modeling;Hardware;Multicore processing;Program processors;Real-time systems;System-on-chip;Time division multiplexing;multicore communication;real-time systems;time-predictable systems},
}

@InProceedings{5751508,
  author    = {C. Schumacher and R. Leupers and D. Petras and A. Hoffmann},
  title     = {parSC: Synchronous parallel SystemC simulation on multi-core host architectures},
  booktitle = {2010 IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
  year      = {2010},
  pages     = {241-246},
  month     = {Oct},
  abstract  = {Time-consuming cycle-accurate MPSoC simulation is often needed for debugging and verification. Its practicability is put at risk by the growing MPSoC complexity. This work presents a conservative synchronous parallel simulation approach along with a SystemC framework to accelerate tightly-coupled MPSoC simulations on multi-core hosts. Key contribution is the implementation strategy, which utilizes techniques from the high-performance computing domain. Results show speed-ups of up to 4.4 on four host cores.},
  db        = {IEEE},
  doi       = {10.1145/1878961.1879005},
  keywords  = {computer debugging;multiprocessing systems;network interfaces;parallel architectures;system-on-chip;MPSoC complexity;high performance computing domain;multicore host architectures;multicore host core;synchronous parallel systemC simulation;tightly coupled MPSoC simulation;time consuming cycle accurate MPSoC simulation;Computational modeling;Data models;Kernel;Load modeling;Logic gates;Prefetching;Synchronization;Experimentation;Measurement;Performance},
}

@InProceedings{4577683,
  author    = {H. Schweppe and A. Zimmermann and D. Grilly},
  title     = {Flexible in-vehicle stream processing with distributed automotive control units for engineering and diagnosis},
  booktitle = {2008 International Symposium on Industrial Embedded Systems},
  year      = {2008},
  pages     = {74-81},
  month     = {June},
  abstract  = {This paper introduces a method for selectively pre-processing and recording sensor data for engineering testing purposes in vehicles. In order to condense data, methodologies from the domain of sensor networks and stream processing are applied, which results in a reduction of the quantity of data, while maintaining information quality. A situation-dependent modification of recording parameters allows for a detailed profiling of vehicle-related errors. We developed a data-flow oriented model, in which data streams are connected by processing nodes. These nodes filter and aggregate the data and can be connected in nearly any order, which permits a successive composition of the aggregation and recording strategy. The integration with an event-condition-action model provides adaptability of the processing and recording, depending on the state of the vehicle. In a proof-of-concept system, which we implemented on top of the automotive diagnostic protocols KWP and UDS, the feasibility of the approach was shown. The target platform was an embedded on-board computer that is connected to the OBD-II interface of the vehicle. As the scope of recording can be adjusted flexibly, the recording system can not only be used for diagnostic purposes, but also serves objectives in development, quality assurance, and even marketing.},
  db        = {IEEE},
  doi       = {10.1109/SIES.2008.4577683},
  issn      = {2150-3109},
  keywords  = {automobiles;automotive electronics;fault diagnosis;OBD-II interface;automotive diagnostic protocol;data-flow oriented model;diagnostic software interface;distributed automotive control unit;embedded onboard computer;engineering testing purpose;event-condition-action model;flexible in-vehicle stream processing;proof-of-concept system;sensor network;Aggregates;Automotive engineering;Computer interfaces;Data engineering;Distributed control;Filters;Maintenance engineering;Protocols;Testing;Vehicles},
}

@InProceedings{6416747,
  author    = {R. R. Sharma and Y. Rajasekhar and R. Sass},
  title     = {Exploring hardware work queue support for lightweight threads in MPSoCs},
  booktitle = {2012 International Conference on Reconfigurable Computing and FPGAs},
  year      = {2012},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Fine-grain thread parallelism using task based programming models are a new trend in achieving massively parallel computations. Often, software pre-fetching and queuing mechanisms for managing these dynamic environments are inadequate, failing to keep the processor cores busy with computation. At the same time, the CPU-memory performance gap is getting worse and this puts a strain on memory subsystem to keep cores in a busy state. We describe a hardware based pre-fetching and queuing mechanism aimed at assisting the over-subscription of very lightweight threads per core. Experiments with a soft processor and a reconfigurable accelerator core are reported. The hardware demonstrates the ability to block on out-of-order memory transactions and alleviates the software bottleneck.},
  db        = {IEEE},
  doi       = {10.1109/ReConFig.2012.6416747},
  issn      = {2325-6532},
  keywords  = {multiprocessing systems;parallel programming;queueing theory;storage management;system-on-chip;CPU-memory performance gap;MPSoC;fine-grain thread parallelism;hardware based pre-fetching;hardware work queue support;lightweight threads;memory subsystem;multiprocessing system on chip;out-of-order memory transactions;parallel computations;queuing mechanisms;reconfigurable accelerator core;soft processor;software pre-fetching;task based programming models;Hardware;Instruction sets;Memory management;Random access memory;Registers;Switches;Throughput},
}

@InProceedings{4224663,
  author    = {C. Suh and J. E. Joung and Y. B. Ko},
  title     = {New RF Models of the TinyOS Simulator for IEEE 802.15.4 Standard},
  booktitle = {2007 IEEE Wireless Communications and Networking Conference},
  year      = {2007},
  pages     = {2236-2240},
  month     = {March},
  abstract  = {Recently, wireless sensor networks have gained increasing attention from the industry as well as academia. Various research issues related with sensor networks are intensively proposed, and they are evaluated by some network simulators or real sensor platforms. One of the well-known simulators for wireless sensor networks is called TOSSIM. It can simulate with TinyOS source codes on the real testbed without any significant modifications. Although TOSSIM's architecture and interfaces are well designed for wireless sensor networks based on IEEE 802.15.4 standards, its current RF model is too simple to support main features of the PHY stack of the IEEE 802.15.4. In order to enhance the accuracy of wireless simulation results and implement IEEE 802.15.4 standard, we design a new wireless propagation model and RF physical stack based on the two-ray ground path loss model and CC2420 RF transceiver. Our work contributes on the performance evaluation areas of wireless sensor networks and IEEE 802.15.4 WPAN standard using simulations.},
  db        = {IEEE},
  doi       = {10.1109/WCNC.2007.418},
  issn      = {1525-3511},
  keywords  = {IEEE standards;personal area networks;source coding;telecommunication standards;transceivers;wireless sensor networks;CC2420 RF transceiver;IEEE 802.15.4 standard;RF models;RF physical stack;TOSSIM;TinyOS simulator;WPAN standard;source codes;wireless propagation model;wireless sensor networks;Hardware;Intelligent sensors;Operating systems;Propagation losses;Radio frequency;Sensor systems;Transceivers;Wireless application protocol;Wireless communication;Wireless sensor networks},
}

@InProceedings{7001431,
  author    = {M. Tan and B. Liu and S. Dai and Z. Zhang},
  title     = {Multithreaded pipeline synthesis for data-parallel kernels},
  booktitle = {2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  year      = {2014},
  pages     = {718-725},
  month     = {Nov},
  abstract  = {Pipelining is an important technique in high-level synthesis, which overlaps the execution of successive loop iterations or threads to achieve high throughput for loop/function kernels. Since existing pipelining techniques typically enforce in-order thread execution, a variable-latency operation in one thread would block all subsequent threads, resulting in considerable performance degradation. In this paper, we propose a multithreaded pipelining approach that enables context switching to allow out-of-order thread execution for data-parallel kernels. To ensure that the synthesized pipeline is complexity effective, we further propose efficient scheduling algorithms for minimizing the hardware overhead associated with context management. Experimental results show that our proposed techniques can significantly improve the effective pipeline throughput over conventional approaches while conserving hardware resources.},
  db        = {IEEE},
  doi       = {10.1109/ICCAD.2014.7001431},
  issn      = {1092-3152},
  keywords  = {multi-threading;pipeline processing;processor scheduling;context management;context switching;data-parallel kernels;hardware overhead;hardware resources;high-level synthesis;in-order thread execution;loop iterations;loop/function kernels;multithreaded pipeline synthesis;out-of-order thread execution;pipeline throughput;pipelining techniques;scheduling algorithms;variable-latency operation;Context;Instruction sets;Kernel;Pipeline processing;Schedules;Switches;Throughput},
}

@InProceedings{5090916,
  author    = {E. Vecchie and J. P. Talpin and K. Schneider},
  title     = {Separate compilation and execution of imperative synchronous modules},
  booktitle = {2009 Design, Automation Test in Europe Conference Exhibition},
  year      = {2009},
  pages     = {1580-1583},
  month     = {April},
  abstract  = {The compilation of imperative synchronous languages like Esterel has been widely studied, the separate compilation of synchronous modules has not, and remains a challenge. We propose a new compilation method inspired by traditional sequential code generation techniques to produce coroutines whose hierarchical structure reflects the control flow of the original source code. A minimalistic runtime system executes separately compiled modules.},
  db        = {IEEE},
  doi       = {10.1109/DATE.2009.5090916},
  issn      = {1530-1591},
  keywords  = {data flow analysis;program compilers;Esterel language;imperative synchronous language;imperative synchronous modules;minimalistic runtime system;module compilation;module execution;sequential code generation;source code control flow;Automata;Computational modeling;Computer languages;Domain specific languages;Embedded system;Flow graphs;Intellectual property;Real time systems;Virtual prototyping;Yarn},
}

@InProceedings{5302678,
  author    = {W. h. Wang and Y. l. Cui and T. m. Chen},
  title     = {Identity-Based Authentication Protocol with Paring of Tate on WSN},
  booktitle = {2009 5th International Conference on Wireless Communications, Networking and Mobile Computing},
  year      = {2009},
  pages     = {1-4},
  month     = {Sept},
  abstract  = {Identity cryptography is widely used in security authentication. This paper proposes a standard identity-based authentication scheme, and also proves the security of the scheme under passive attack. On the basis of ECC and bilinear maps, paper implements the authentication protocol in the platform of TinyOs. Finally, paper analyzes its result and proves its feasibility.},
  db        = {IEEE},
  doi       = {10.1109/WICOM.2009.5302678},
  issn      = {2161-9646},
  keywords  = {cryptographic protocols;message authentication;telecommunication security;wireless sensor networks;TinyOs platform;WSN;cryptography;identity-based authentication protocols;security authentication;wireless sensor networks;Application software;Authentication;Cryptographic protocols;Electronic mail;Elliptic curve cryptography;Hardware;Identity-based encryption;Public key cryptography;Security;Wireless sensor networks},
}

@InProceedings{4536359,
  author    = {K. B. Wheeler and R. C. Murphy and D. Thain},
  title     = {Qthreads: An API for programming with millions of lightweight threads},
  booktitle = {2008 IEEE International Symposium on Parallel and Distributed Processing},
  year      = {2008},
  pages     = {1-8},
  month     = {April},
  abstract  = {Large scale hardware-supported multithreading, an attractive means of increasing computational power, benefits significantly from low per-thread costs. Hardware support for lightweight threads is a developing area of research. Each architecture with such support provides a unique interface, hindering development for them and comparisons between them. A portable abstraction that provides basic lightweight thread control and synchronization primitives is needed. Such an abstraction would assist in exploring both the architectural needs of large scale threading and the semantic power of existing languages. Managing thread resources is a problem that must be addressed if massive parallelism is to be popularized. The qthread abstraction enables development of large-scale multithreading applications on commodity architectures. This paper introduces the qthread API and its Unix implementation, discusses resource management, and presents performance results from the HPCCG benchmark.},
  db        = {IEEE},
  doi       = {10.1109/IPDPS.2008.4536359},
  issn      = {1530-2075},
  keywords  = {Unix;application program interfaces;multi-threading;resource allocation;API;HPCCG benchmark;Qthreads;Unix implementation;application program interfaces;large scale hardware supported multithreading;lightweight threads;qthread API;qthread abstraction;resource management;Computer architecture;Costs;Hardware;Laboratories;Large-scale systems;Multithreading;Parallel processing;Programming profession;Resource management;Yarn},
}

@Article{4397184,
  author   = {P. Wilson and A. Frey and T. Mihm and D. Kershaw and T. Alves},
  title    = {Implementing Embedded Security on Dual-Virtual-CPU Systems},
  journal  = {IEEE Design Test of Computers},
  year     = {2007},
  volume   = {24},
  number   = {6},
  pages    = {582-591},
  month    = {Nov},
  issn     = {0740-7475},
  abstract = {In this article, we describe a low-cost, dual-virtual-CPU hardware technology for embedded-systems security. We also present a case study of a programmable software design to exploit such hardware. This design integrates a rich operating system without requiring significant changes to it, while maintaining preemptive and real-time properties, exception handling, and power management.},
  db       = {IEEE},
  doi      = {10.1109/MDT.2007.196},
  keywords = {cryptography;embedded systems;flash memories;logic partitioning;operating systems (computers);virtual storage;dual-virtual-CPU system;embedded-systems security;flash memory;logical partitioning;off-chip storage device;operating system;programmable software design;Application software;Central Processing Unit;Circuit testing;Hardware;Information security;Isolation technology;Kernel;Operating systems;Packaging;Space technology;TrustZone technology;embedded security;programmable;security software framework},
}

@InProceedings{6270634,
  author    = {Q. Wu and C. Yang and F. Wang and J. Xue},
  title     = {A Fast Parallel Implementation of Molecular Dynamics with the Morse Potential on a Heterogeneous Petascale Supercomputer},
  booktitle = {2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum},
  year      = {2012},
  pages     = {140-149},
  month     = {May},
  abstract  = {Molecular Dynamics (MD) simulations have been widely used in the study of macromolecules. To ensure an acceptable level of statistical accuracy relatively large number of particles are needed, which calls for high performance implementations of MD. These days heterogeneous systems, with their high performance potential, low power consumption, and high price-performance ratio, offer a viable alternative for running MD simulations. In this paper we introduce a fast parallel implementation of MD simulation with the Morse potential on Tianhe-1A, a petascale heterogeneous supercomputer. Our code achieves a speedup of 3.6× on one NVIDIA Tesla M2050 GPU (containing 14 Streaming Multiprocessors) compared to a 2.93GHz six-core Intel Xeon X5670 CPU. In addition, our code runs faster on 1024 compute nodes (with two CPUs and one GPU inside a node) than on 4096 GPU-excluded nodes, effectively rendering one GPU more efficient than six six-core CPUs. Our work shows that large-scale MD simulations can benefit enormously from GPU acceleration in petascale supercomputing platforms. Our performance results are achieved by using (1) a patch-cell design to exploit parallelism across the simulation domain, (2) a new GPU kernel developed by taking advantage of Newton's Third Law to reduce redundant force computation on GPUs, (3) two optimization methods including a dynamic load balancing strategy that adjusts the workload, and a communication overlapping method to overlap the communications between CPUs and GPUs.},
  db        = {IEEE},
  doi       = {10.1109/IPDPSW.2012.13},
  keywords  = {Morse potential;chemistry computing;graphics processing units;macromolecules;molecular dynamics method;parallel machines;resource allocation;statistical analysis;GPU acceleration;GPU kernel;GPU-excluded node;MD simulation;Morse potential;NVIDIA Tesla M2050 GPU;Newton Third Law;Tianhe-1A;communication overlapping method;dynamic load balancing strategy;fast parallel implementation;heterogeneous petascale supercomputer;heterogeneous systems;macromolecules;molecular dynamics simulation;optimization method;parallelism;patch-cell design;petascale supercomputing platform;redundant force computation;six-core Intel Xeon X5670 CPU;statistical accuracy;streaming multiprocessors;workload adjustment;Computational modeling;Force;Graphics processing unit;Indexes;Instruction sets;Kernel;Mathematical model;GPU computing;Molecular Dynamics;heterogeneous computing;petascale supercomputer},
}

@InProceedings{4340471,
  author    = {M. Yao and X. Zhu},
  title     = {Study and Transplant of Operating System for Wireless Sensor Network Node},
  booktitle = {2007 International Conference on Wireless Communications, Networking and Mobile Computing},
  year      = {2007},
  pages     = {2803-2807},
  month     = {Sept},
  abstract  = {This paper studies the embedded operating system TinyOS, including its event-driven mechanism, schedule strategy mechanics, power management mechanism, and etc. Then, the component-based architecture of TinyOS is analyzed. Based on these, the transplant scheme, the design principle of the layer of hardware description and the selection principle of processor are provided. Finally, this paper gives some conclusions and foresight.},
  db        = {IEEE},
  doi       = {10.1109/WICOM.2007.696},
  issn      = {2161-9646},
  keywords  = {embedded systems;network operating systems;power aware computing;scheduling;wireless sensor networks;TinyOS embedded operating system transplant scheme;component-based architecture;event-driven mechanism;hardware description layer design principle;power management mechanism;processor selection principle;schedule strategy mechanics;wireless sensor network node;Application software;Batteries;Communication system control;Data processing;Energy management;Hardware;Operating systems;Power supplies;Power system management;Wireless sensor networks},
}

@InProceedings{4678868,
  author    = {M. Yu and S. Xiahou and X. Li},
  title     = {A Survey of Studying on Task Scheduling Mechanism for TinyOS},
  booktitle = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing},
  year      = {2008},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {Although TinyOS has been regarded as the defacto standard for WSN (Wireless Sensor Network) applications, its simple task scheduling mechanism became a great obstacle to WSN applications. This paper, from two directions (one based on cooperative, the other based on preemptive), presented a variety of scheduling algorithms and their application in TinyOS. And their characters and advantage were discussed as well in terms of energy consuming, tasks executing efficiency. Then a new, integrated and adaptive task scheduling mechanism was pointed out for the future TinyOS task scheduling. This new scheduling mechanism was characterized with features of dynamical adaptability and context-awareness.},
  db        = {IEEE},
  doi       = {10.1109/WiCom.2008.960},
  issn      = {2161-9646},
  keywords  = {scheduling;wireless sensor networks;TinyOS;task scheduling mechanism;wireless sensor network;Adaptive scheduling;Computer languages;Context;Dynamic scheduling;Mobile communication;Mobile computing;Processor scheduling;Scheduling algorithm;Sensor systems and applications;Wireless sensor networks},
}

@Article{6393004,
  title   = {CEDA Currents [IEEE News]},
  journal = {IEEE Solid-State Circuits Magazine},
  year    = {2012},
  volume  = {4},
  number  = {4},
  pages   = {62-63},
  month   = {Dec},
  issn    = {1943-0582},
  db      = {IEEE},
  doi     = {10.1109/MSSC.2012.2214295},
}

@InProceedings{6820867,
  author    = {O. Baldellon and J. C. Fabre and M. Roy},
  title     = {Minotor: Monitoring Timing and Behavioral Properties for Dependable Distributed Systems},
  booktitle = {2013 IEEE 19th Pacific Rim International Symposium on Dependable Computing},
  year      = {2013},
  pages     = {206-215},
  month     = {Dec},
  abstract  = {Assessing the correct behavior of a given system at run-time can be achieved by monitoring its execution, and is complementary to off-line analysis such as static verification. In this work, we focus on run-time monitoring of system properties that include both causality and timing constraints, in distributed and time-constrained systems. Based on a description of a property that includes events and temporal constraints, expressed as a timed-arc Petri net, we show how to automatically transform it into a an executable and distributed monitoring engine. To that aim, we introduce a modification of the semantics of Petri nets to be able to execute it online on partial executions and distributed observation environments. We show how to use this formal framework to provide MINOTOR, a model-driven distributed monitoring system, describe its implementation and show its applicability on a transportation use-case.},
  db        = {IEEE},
  doi       = {10.1109/PRDC.2013.41},
  keywords  = {Petri nets;formal specification;program diagnostics;program verification;rail traffic;system monitoring;transportation;MINOTOR;Petri net semantics modification;behavioral property monitoring;causality;dependable distributed systems;distributed monitoring engine;distributed observation environment;executable monitoring engine;execution monitoring;model-driven distributed monitoring system;off-line analysis;partial execution;railway transportation;run-time monitoring;run-time system behavior assessment;static verification;temporal constraint;time-constrained system;timed-arc Petri net;timing constraint;timing property monitoring;Delays;Message systems;Monitoring;Petri nets;Real-time systems;Semantics;Ditributed Systems;Fault-tolerant Systems;Online Monitoring;Petri nets;Time-constrained Systems},
}

@InProceedings{4724900,
  author    = {J. Chase and B. Nelson and J. Bodily and Z. Wei and D. J. Lee},
  title     = {Real-Time Optical Flow Calculations on FPGA and GPU Architectures: A Comparison Study},
  booktitle = {2008 16th International Symposium on Field-Programmable Custom Computing Machines},
  year      = {2008},
  pages     = {173-182},
  month     = {April},
  abstract  = {FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing a variety of computations. Applications successfully implemented on FPGAs have typically contained high levels of parallelism and have often used simple statically-scheduled control and modest arithmetic. Recently introduced computing devices such as coarse grain reconfigurable arrays, multi-core processors, and graphical processing units (GPUs) promise to significantly change the computational landscape for the implementation of high-speed real-time computing tasks. One reason for this is that these architectures take advantage of many of the same application characteristics that fit well on FPGAs. One real-time computing task, optical flow, is difficult to apply in robotic vision applications in practice because of its high computational and data rate requirements, and so is a good candidate for implementation on FPGAs and other custom computing architectures. In this paper, a tensor-based optical flow algorithm is implemented on both an FPGA and a GPU and the two implementations discussed. The two implementations had similar performance, but with the FPGA implementation requiring 12Ã more development time. Other comparison data for these two technologies is then given for three additional applications taken from a MIMO digital communication system design, providing additional examples of the relative capabilities of these two technologies.},
  db        = {IEEE},
  doi       = {10.1109/FCCM.2008.24},
  keywords  = {MIMO systems;field programmable gate arrays;optical computing;program processors;FPGA devices;MIMO digital communication system design;computing architectures;graphical processing units;multicore processors;statically-scheduled control;tensor-based optical flow algorithm;Arithmetic;Computer architecture;Computer vision;Data flow computing;Field programmable gate arrays;High speed optical techniques;Image motion analysis;Multicore processing;Optical computing;Parallel processing},
}

@InProceedings{5751491,
  author    = {M. Geilen and S. Stuijk},
  title     = {Worst-case performance analysis of Synchronous Dataflow scenarios},
  booktitle = {2010 IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
  year      = {2010},
  pages     = {125-134},
  month     = {Oct},
  abstract  = {Synchronous Dataflow (SDF) is a powerful analysis tool for regular, cyclic, parallel task graphs. The behaviour of SDF graphs however is static and therefore not always able to accurately capture the behaviour of modern, dynamic dataflow applications, such as embedded multimedia codecs. An approach to tackle this limitation is by means of scenarios. In this paper we introduce a technique and a tool to automatically analyse a scenario-aware dataflow model for its worst-case performance. A system is specified as a collection of SDF graphs representing individual scenarios of behaviour and a finite state machine that specifies the possible orders of scenario occurrences. This combination accurately captures more dynamic applications and this way provides tighter results than an existing analysis based on a conservative static dataflow model, which is too pessimistic, while looking only at the `worst-case' individual scenario, without considering scenario transitions, can be too optimistic. We introduce a formal semantics of the model, in terms of (max; +) linear system-theory and in particular (max; +) automata. Leveraging existing results and algorithms from this domain, we give throughput analysis and state space generation algorithms for worst-case performance analysis. The method is implemented in a tool and the effectiveness of the approach is experimentally evaluated.},
  db        = {IEEE},
  doi       = {10.1145/1878961.1878985},
  keywords  = {computational complexity;data analysis;data flow analysis;data flow graphs;finite state machines;SDF graph;embedded multimedia codecs;finite state machine;formal semantics;linear system theory;parallel task graph;scenario-aware dataflow model;state space generation algorithm;synchronous dataflow scenario;worst case performance analysis;(max; +) algebra;Synchronous Data Flow;worst-case performance analysis},
}

@InProceedings{4686688,
  author    = {M. C. Jadud and C. L. Jacobsen and C. G. Ritson and J. Simpson},
  title     = {Safe parallelism for robotic control},
  booktitle = {2008 IEEE International Conference on Technologies for Practical Robot Applications},
  year      = {2008},
  pages     = {137-142},
  month     = {Nov},
  abstract  = {During the Spring 2008 semester at Olin College, we introduced the programming language occam-pi to undergraduates as part of their first course in robotics. Students were able to explore image processing and autonomous behavioral control in a parallel programming language on a small mobile robotics platform with just two weeks of tutorial instruction. Our experiences to date suggest that the language and tools we have developed allow the concise expression of complex robotic control systems, and enable the integration of events from the environment in a consistent and safe model for parallel control that is directly expressed in software.},
  db        = {IEEE},
  doi       = {10.1109/TEPRA.2008.4686688},
  issn      = {2325-0526},
  keywords  = {control engineering education;educational courses;image processing;mobile robots;parallel languages;parallel programming;robot programming;autonomous behavioral control;complex robotic control system;image processing;mobile robotics;occam-pi programming language;parallel programming language;robotic course;safe parallelism;tutorial instruction;Computer languages;Control system synthesis;Educational institutions;Educational robots;Image processing;Mobile robots;Parallel programming;Parallel robots;Robot control;Springs},
}

@InProceedings{6047305,
  author    = {H. Nguyen and D. Abramson and B. Bethwaite and M. N. Dinh and C. Enticott and S. Garic and A. B. M. Russel and S. Firth and I. Harper and M. Lackmann and M. Vail and S. Schek},
  title     = {Integrating Scientific Workflows and Large Tiled Display Walls: Bridging the Visualization Divide},
  booktitle = {2011 40th International Conference on Parallel Processing Workshops},
  year      = {2011},
  pages     = {308-316},
  month     = {Sept},
  abstract  = {Modern in-silico science (or e-Science) is a complex process, often involving multiple steps conducted across different computing environments. Scientific workflow tools help scientists automate, manage and execute these steps, providing a robust and repeatable research environment. Increasingly workflows generate data sets that require scientific visualization, using a range of display devices such as local workstations, immersive 3D caves and large display walls. Traditionally, this display step handled outside the workflow, and output files are manually copied to a suitable visualization engine for display. This inhibits the scientific discovery process disconnecting the workflow that generated the data from the display and interpretation processes. In this paper we present a solution that links scientific workflows with a variety of display devises, including large tiled display walls. We demonstrate the feasibility of the system by a prototype implementation that leverages the Kepler workflow engine and the SAGE display software. We illustrate the use of the system with a case study in workflow driven microscopy.},
  db        = {IEEE},
  doi       = {10.1109/ICPPW.2011.30},
  issn      = {0190-3918},
  keywords  = {data visualisation;display devices;file organisation;scientific information systems;workflow management software;Kepler workflow engine;SAGE display software;data sets;display device;e-science;file copying;in-silico science;interpretation processes;large tiled display walls;scientific discovery process;scientific visualization;scientific workflow tool;visualization divide;visualization engine;Data visualization;Educational institutions;Engines;Middleware;Rendering (computer graphics);Streaming media;E-Science;Kepler;Optiportal;Scientific Workflows;Tiled Display Wall;Visualization},
}

@InProceedings{6842273,
  author    = {U. Cekmez and M. Ozsiginan and O. K. Sahingoz},
  title     = {A UAV path planning with parallel ACO algorithm on CUDA platform},
  booktitle = {2014 International Conference on Unmanned Aircraft Systems (ICUAS)},
  year      = {2014},
  pages     = {347-354},
  month     = {May},
  abstract  = {Solving the path planning problem of a UAV is a challenging issue especially if there are too many checkpoints to visit. Mainly, the brute force approach is used to find the shortest path in the mission area, which requires too many times to find a solution. Therefore, evolutionary algorithms and swarm intelligence techniques are used to find a feasible solution in an acceptable time. In this study, path planning problem of a UAV is solved by using a highly parallelized Ant Colony Optimization (ACO) algorithm on CUDA platform. The UAV path is constructed for disseminating keys and collecting data from a Wireless Sensor Network, which is previously defined. Due to its simplicity and effectiveness, ACO is selected as a path planning algorithm. However, ACO is not satisfactory if the mission area becomes large and there are an excessive number of checkpoints and/or additional constraints. In order to increase the performance, some parallelization techniques must be used in high performance computing platforms. GPU architecture has emerged as a powerful and low cost architecture for enabling impressive speedups for scientific calculations. Therefore, the parallel structure is constructed on CUDA architecture. The experimental results are compared with the CPU performance of the serial algorithm, and they clearly show that the proposed approach have a great potential for acceleration of ACO and allow to solve many complex tasks such as UAV path planning problem. We also present the execution results with different parameter values to expose the results for the researchers.},
  db        = {IEEE},
  doi       = {10.1109/ICUAS.2014.6842273},
  keywords  = {ant colony optimisation;autonomous aerial vehicles;graphics processing units;mobile robots;parallel architectures;path planning;CUDA platform;GPU architecture;UAV path planning;ant colony optimization;brute force approach;compute unified device architecture;evolutionary algorithms;graphics processing units;parallel ACO algorithm;swarm intelligence techniques;unmanned aerial vehicle;wireless sensor network;Central Processing Unit;Cities and towns;Computer architecture;Graphics processing units;Instruction sets;Path planning;Planning},
}

@InProceedings{7371363,
  author    = {G. Ferro and R. Silva and L. Lopes},
  title     = {Towards Out-of-the-Box Programming of Wireless Sensor-Actuator Networks},
  booktitle = {2015 IEEE 18th International Conference on Computational Science and Engineering},
  year      = {2015},
  pages     = {110-119},
  month     = {Oct},
  abstract  = {We address the problem of providing users, namely non specialists, with out-of-the-box, programmable, Wireless Sensor-Actuator Networks (WSN). The idea is that users get a package containing a gateway and an undetermined number of nodes, pre-configured to work as a self-organized wireless mesh. Each node comes with two pre-installed components: a small operating system and a virtual machine. The user can then use a simple, domain-specific, programming language to implement periodic tasks that are compiled into byte-code, and can be sent to the nodes for execution. At the nodes, the operating system manages a task table and schedules non-preemptive tasks for execution using the virtual machine. No subtle hardware or software configuration is required from the user as these details are abstracted away by the virtual machine. We developed a full specification for a data-layer that follows the aforementioned guidelines and implemented a complete prototype, integrated in our own Publish/Subscribe middleware called SONAR. In this paper we report the first results of using the prototype as compared to using the low level programming tools provided with the hardware. We measure a small increase in both resource consumption and processing overhead suggesting that this data-layer can be used effectively in WSN, even in cases where nodes have very limited hardware resources.},
  db        = {IEEE},
  doi       = {10.1109/CSE.2015.20},
  keywords  = {actuators;programming languages;virtual machines;wireless sensor networks;Publish/Subscribe middleware;SONAR;WSN;byte-code;gateway;level programming tools;operating system;out-of-the-box programming;programming language;virtual machine;wireless sensor-actuator networks;Logic gates;Operating systems;Programming;Sensors;Sonar;Virtual machining;Wireless sensor networks;Programming Language;Virtual Machine;Wireless Sensor Network},
}

@InProceedings{5663821,
  author    = {M. Strube and M. Daum and R. Kapitza and F. Villanueva and F. Dressler},
  title     = {Dynamic operator replacement in sensor networks},
  booktitle = {The 7th IEEE International Conference on Mobile Ad-hoc and Sensor Systems (IEEE MASS 2010)},
  year      = {2010},
  pages     = {748-750},
  month     = {Nov},
  abstract  = {We present an integrated approach for supporting in-network sensor data processing in dynamic and heterogeneous sensor networks. The concept relies on data stream processing techniques that define and optimize the distribution of queries and their operators. We anticipate a high degree of dynamics in the network, which can for example be expected in the case of wildlife monitoring applications. The distribution of operators to individual nodes demands system level capabilities not available in current sensor node operating systems. In particular, we present a system for seamless and on demand operator migration between sensor nodes. Our framework, which we implemented for Contiki running on TelosB nodes, supports stateful module migration including selected parts of the code and data sections.},
  db        = {IEEE},
  doi       = {10.1109/MASS.2010.5663821},
  issn      = {2155-6806},
  keywords  = {optimisation;wireless sensor networks;Contiki running;TelosB nodes;data stream processing;dynamic operator replacement;dynamic sensor networks;heterogeneous sensor networks;in-network sensor data processing;optimization;Data processing;Distributed databases;Operating systems;Programming;Runtime;Servers;Wireless sensor networks},
}

@InProceedings{4797078,
  author    = {Y. XiangMin and W. WenYong and X. Yu},
  title     = {An IPv6 Wireless Sensor Network Node-TaraxNode},
  booktitle = {2009 WRI International Conference on Communications and Mobile Computing},
  year      = {2009},
  volume    = {2},
  pages     = {9-14},
  month     = {Jan},
  abstract  = {This paper presents the IPv6 wireless sensor network platform system TaraxNode. It is designed and implemented with independent intellectual property rights. For effectively monitoring, we developed TaraxOS, which includes a cluster-based self-organized multi-hop routing protocol. It is a multi-task, high-powered, low energy-consumed operating system for sensor nodes. TaraxNode containing heterogeneous elements provides numerous benefits at the traditional intelligent monitor system. It is mainly composed of a low power consumed MCU TaraxCore and WSN data acquired unit etc. When applied at the central air-condition intelligent monitor and control system, it gives excellent result.},
  db        = {IEEE},
  doi       = {10.1109/CMC.2009.111},
  keywords  = {computerised monitoring;industrial property;intelligent sensors;operating systems (computers);routing protocols;wireless sensor networks;IPv6;TaraxNode;TaraxOS;central air-condition intelligent monitor;cluster-based self-organized multihop routing protocol;control system;intellectual property rights;intelligent monitor system;operating system;sensor nodes;wireless sensor network node;Centralized control;Intellectual property;Intelligent control;Intelligent sensors;Intelligent systems;Monitoring;Operating systems;Routing protocols;Sensor systems;Wireless sensor networks},
}

@InProceedings{4694538,
  author    = {D. Singh and U. S. Tiwary and Wan-Young Chung},
  title     = {IP-based ubiquitous healthcare system},
  booktitle = {2008 International Conference on Control, Automation and Systems},
  year      = {2008},
  pages     = {131-136},
  month     = {Oct},
  abstract  = {This paper presents a new concept of MAC and LOAD protocols for IP based ubiquitous healthcare system. The system used IEEE 802.15.4 standard lowpan with integrated IPv6. For healthcare system we added LOAD (6lowpan Ad-hoc on Demand Distance Vector) and MAC (Medium Access Control) protocols in Harvanpsilas 6lowpan stack. 6lowpan stack has ability to connect the physical environment in real-world applications such as healthcare, wireless sensor network, network technology etc. IP-enable motes set on the patient body for retrieving biomedical from body in PAN. PAN network connected PC via gateway or base station for further analysis or to the doctorpsilas PDA (personal digital assistant). The doctor can recognize or analysis patient data from anywhere on globe by internet service provider equipments (PDA). Result shows the performance biomedical data packets in multi-hope routing as well as represents the topology of the networks. TelosB motes were tested on octopus simulator in tinyOS2.02 for performance of biomedical data communication and network topology.},
  db        = {IEEE},
  doi       = {10.1109/ICCAS.2008.4694538},
  keywords  = {IP networks;Internet;access protocols;ad hoc networks;biomedical communication;data communication;health care;personal area networks;telecommunication network routing;telecommunication network topology;ubiquitous computing;wireless sensor networks;6lowpan ad-hoc on demand distance vector;IEEE 802.15.4 standard lowpan;IP-based ubiquitous healthcare system;IPv6;LOAD protocols;MAC protocols;biomedical data communication;multihope routing;network technology;network topology;personal digital assistant;tinyOS2.02;wireless sensor network;Access protocols;Base stations;Bioinformatics;Data analysis;Media Access Protocol;Medical services;Network topology;Personal digital assistants;Wireless application protocol;Wireless sensor networks;6lowpan;Body Area Networks;Healthcare;IP-based;PAN;TelosB;TinyOS},
}

@InProceedings{4239012,
  author    = {X. Wang and X. Zhao and Z. Liang and M. Tan},
  title     = {Deploying a Wireless Sensor Network on the Coal Mines},
  booktitle = {2007 IEEE International Conference on Networking, Sensing and Control},
  year      = {2007},
  pages     = {324-328},
  month     = {April},
  abstract  = {In this paper we describe our experiences using a wireless sensor network to monitor the coal mines with sensors. Wireless networks that link sensors have the potential to greatly benefit for monitoring of coal mine conditions and localization of miners. Substituting light, small wireless sensor network nodes for traditional wired, heavy and fixed monitoring equipment leads to faster, easier and larger-scale deployments in complicated environment. Network consisted of many wireless sensor nodes is now feasible in practice. The primary problem of designing a wireless sensor network for coal mine monitoring and localization of miners is the high system reliability and robustness demanded by its application and safety. The topology of sensor networks is also a main factor we have to take into account. The sensor network application we designed for coal mine monitoring bases on Berkeley TinyOS and Motes platform. We evaluate this design and gain some useful experiences which will benefit our subsequent work.},
  db        = {IEEE},
  doi       = {10.1109/ICNSC.2007.372799},
  keywords  = {coal;mining industry;wireless sensor networks;coal mine monitoring;high system reliability;larger-scale deployments;miners localization;wireless sensor network;Accidents;Automation;Energy management;Large-scale systems;Monitoring;Network topology;Reliability;Robustness;Safety;Wireless sensor networks},
}

@Comment{jabref-meta: databaseType:bibtex;}
